<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Machine Learning Notes | mushetty.me</title>
<meta name="keywords" content="">
<meta name="description" content="Compilation of notes for Machine Learning class for Spring 2023.
 Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier     Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9  Administrative Stuff  All homework is submitted via Gradescope.  Written parts must be PDFs and be typeset.  Plots and data analysis from programming part must be included in written part   Programming parts are mostly turned in as a separate assignment on Gradescope   40 % HW, 30% Midterm, 30% Final  Topic 1  Inputs encoded as vectors ($x \in X$):  $\vec{x} = &lt;x_{1},x_{2},&hellip;x_{n}&gt;$ Can think of each component as a measurement that you make   Output is another vector ($y \in Y$):  $\vec{y} = &lt;y_{1},y_{2},&hellip;y_{m}&gt;$   Goal is to figure out the function that maps x to y  You only have a limited number of samples from X  You need to make predictions from this limited sample size      Workflow of Classification Problems (Supervised Learning)  $(\vec{x_{1}},y_{1}),(\vec{x_{2}},y_{2})&hellip; \in X \times Y$  Example: $X = \mathbb{R^{d}}$ and $Y = {0,1}$ for a binary classification from a image with d pixels Assumption: there exists a &ldquo;relatively simple&rdquo; function $f^{*}: X \rightarrow Y$ such that $f^{*}(\vec{x_{i}}) = y_{i}$ for most i  $*$ in ML implies some optimum value (in this case, an optimal function) Most includes that possibility that there is some fundamental noise in your data Say someone wrote a number that kind of looks like 7 and kinda looks like 1.">
<meta name="author" content="">
<link rel="canonical" href="https://mushetty.me/notes/machinelearning/machine/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.c5e8fb7cf778d88f94a46c567e365d250dace54446141434726dce09e8036474.css" integrity="sha256-xej7fPd42I&#43;UpGxWfjZdJQ2s5URGFBQ0cm3OCegDZHQ=" rel="preload stylesheet" as="style">
<link rel="stylesheet" href="" />
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://mushetty.me/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mushetty.me/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mushetty.me/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mushetty.me/apple-touch-icon.png">
<link rel="mask-icon" href="https://mushetty.me/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


<meta property="og:title" content="Machine Learning Notes" />
<meta property="og:description" content="Compilation of notes for Machine Learning class for Spring 2023.
 Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier     Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9  Administrative Stuff  All homework is submitted via Gradescope.  Written parts must be PDFs and be typeset.  Plots and data analysis from programming part must be included in written part   Programming parts are mostly turned in as a separate assignment on Gradescope   40 % HW, 30% Midterm, 30% Final  Topic 1  Inputs encoded as vectors ($x \in X$):  $\vec{x} = &lt;x_{1},x_{2},&hellip;x_{n}&gt;$ Can think of each component as a measurement that you make   Output is another vector ($y \in Y$):  $\vec{y} = &lt;y_{1},y_{2},&hellip;y_{m}&gt;$   Goal is to figure out the function that maps x to y  You only have a limited number of samples from X  You need to make predictions from this limited sample size      Workflow of Classification Problems (Supervised Learning)  $(\vec{x_{1}},y_{1}),(\vec{x_{2}},y_{2})&hellip; \in X \times Y$  Example: $X = \mathbb{R^{d}}$ and $Y = {0,1}$ for a binary classification from a image with d pixels Assumption: there exists a &ldquo;relatively simple&rdquo; function $f^{*}: X \rightarrow Y$ such that $f^{*}(\vec{x_{i}}) = y_{i}$ for most i  $*$ in ML implies some optimum value (in this case, an optimal function) Most includes that possibility that there is some fundamental noise in your data Say someone wrote a number that kind of looks like 7 and kinda looks like 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mushetty.me/notes/machinelearning/machine/" /><meta property="article:section" content="notes" />
<meta property="article:published_time" content="2023-01-17T22:19:16-05:00" />
<meta property="article:modified_time" content="2023-01-19T23:55:12-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning Notes"/>
<meta name="twitter:description" content="Compilation of notes for Machine Learning class for Spring 2023.
 Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier     Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9  Administrative Stuff  All homework is submitted via Gradescope.  Written parts must be PDFs and be typeset.  Plots and data analysis from programming part must be included in written part   Programming parts are mostly turned in as a separate assignment on Gradescope   40 % HW, 30% Midterm, 30% Final  Topic 1  Inputs encoded as vectors ($x \in X$):  $\vec{x} = &lt;x_{1},x_{2},&hellip;x_{n}&gt;$ Can think of each component as a measurement that you make   Output is another vector ($y \in Y$):  $\vec{y} = &lt;y_{1},y_{2},&hellip;y_{m}&gt;$   Goal is to figure out the function that maps x to y  You only have a limited number of samples from X  You need to make predictions from this limited sample size      Workflow of Classification Problems (Supervised Learning)  $(\vec{x_{1}},y_{1}),(\vec{x_{2}},y_{2})&hellip; \in X \times Y$  Example: $X = \mathbb{R^{d}}$ and $Y = {0,1}$ for a binary classification from a image with d pixels Assumption: there exists a &ldquo;relatively simple&rdquo; function $f^{*}: X \rightarrow Y$ such that $f^{*}(\vec{x_{i}}) = y_{i}$ for most i  $*$ in ML implies some optimum value (in this case, an optimal function) Most includes that possibility that there is some fundamental noise in your data Say someone wrote a number that kind of looks like 7 and kinda looks like 1."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Machine Learning Notes",
      "item": "https://mushetty.me/notes/machinelearning/machine/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Machine Learning Notes",
  "name": "Machine Learning Notes",
  "description": "Compilation of notes for Machine Learning class for Spring 2023.\n Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier     Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9  Administrative Stuff  All homework is submitted via Gradescope.  Written parts must be PDFs and be typeset.  Plots and data analysis from programming part must be included in written part   Programming parts are mostly turned in as a separate assignment on Gradescope   40 % HW, 30% Midterm, 30% Final  Topic 1  Inputs encoded as vectors ($x \\in X$):  $\\vec{x} = \u0026lt;x_{1},x_{2},\u0026hellip;x_{n}\u0026gt;$ Can think of each component as a measurement that you make   Output is another vector ($y \\in Y$):  $\\vec{y} = \u0026lt;y_{1},y_{2},\u0026hellip;y_{m}\u0026gt;$   Goal is to figure out the function that maps x to y  You only have a limited number of samples from X  You need to make predictions from this limited sample size      Workflow of Classification Problems (Supervised Learning)  $(\\vec{x_{1}},y_{1}),(\\vec{x_{2}},y_{2})\u0026hellip; \\in X \\times Y$  Example: $X = \\mathbb{R^{d}}$ and $Y = {0,1}$ for a binary classification from a image with d pixels Assumption: there exists a \u0026ldquo;relatively simple\u0026rdquo; function $f^{*}: X \\rightarrow Y$ such that $f^{*}(\\vec{x_{i}}) = y_{i}$ for most i  $*$ in ML implies some optimum value (in this case, an optimal function) Most includes that possibility that there is some fundamental noise in your data Say someone wrote a number that kind of looks like 7 and kinda looks like 1.",
  "keywords": [
    
  ],
  "articleBody": "Compilation of notes for Machine Learning class for Spring 2023.\n Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier     Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9  Administrative Stuff  All homework is submitted via Gradescope.  Written parts must be PDFs and be typeset.  Plots and data analysis from programming part must be included in written part   Programming parts are mostly turned in as a separate assignment on Gradescope   40 % HW, 30% Midterm, 30% Final  Topic 1  Inputs encoded as vectors ($x \\in X$):  $\\vec{x} = $ Can think of each component as a measurement that you make   Output is another vector ($y \\in Y$):  $\\vec{y} = $   Goal is to figure out the function that maps x to y  You only have a limited number of samples from X  You need to make predictions from this limited sample size      Workflow of Classification Problems (Supervised Learning)  $(\\vec{x_{1}},y_{1}),(\\vec{x_{2}},y_{2})… \\in X \\times Y$  Example: $X = \\mathbb{R^{d}}$ and $Y = {0,1}$ for a binary classification from a image with d pixels Assumption: there exists a “relatively simple” function $f^{*}: X \\rightarrow Y$ such that $f^{*}(\\vec{x_{i}}) = y_{i}$ for most i  $*$ in ML implies some optimum value (in this case, an optimal function) Most includes that possibility that there is some fundamental noise in your data Say someone wrote a number that kind of looks like 7 and kinda looks like 1. For the same image, the true $y_{i}$ could be either 7 or 1 (depending on the person’s intentions). Hence, to force $f^{*}$ be a correct for all inputs misses this uncertainty in the true output   Learning task: given $n$ examples from the data, from $\\hat{f} \\approx f^{*}$  $\\hat{f}$ denotes a function that tries its best to be optimal   Goal: $\\hat{f}$ gives mostly correct prediction on unseen examples    Statistical Approach  $(\\vec{x_{1}},y_{1}),(\\vec{x_{2}},y_{2})…(\\vec{x_{n}},y_{n})$ samples are drawn from the underlying distribution  For simplicity, we assume each sample is drawn independently from the same underlying distribution is (called i.i.d. assumption)   The learning algorithm draws $\\hat{f}$ from a pool of models $\\mathbb{F}$ that maximizes the label agreement with the training data  How do we select $f \\in \\mathbb{F}$? Maximum likelihood (best fits the data) Maximum a posteriori (best fits the data but incorporates prior assumptions) Optimization of ‘loss’ criterion (best discriminates the labels)    Classifier  We are given a joint input/output space ($X \\times Y$)  The data is distributed like $D(X \\times Y)$  You have a region where your inputs can reside in ($X \\times Y$) and D tells you the density of inputs on the space You can visualize this space by either:  Taking slices at a fixed variable value ($P[Y,X=x]$) or the conditional distribution Adding up all the slices along a variable ($P[Y]$) or the marginal distribution       A classifier is a measurable function of the type $f: x \\in Y$  Ex1 (Constant): $f_{1}(\\vec{x}) = y$ for some fixed $y \\in Y$. Ex2(Threshold): $f_{2}(x) = \\begin{cases} 0\\ if \\ x \\geq 5 \\\\ 1 \\ otherwise \\end{cases}$ Ex3 (Majority class): $f_{3}(x) = arg\\ max_{y\\in Y} P[Y=y]$  This asks: What argument in Y yields the maximum probability? Implicitly, this probability is the marginalized distribution on X More explicitly, you can write probability as: $P[Y=y] = P_{y \\approx D}|_{Y}[Y=y]$  In words, you draw a (x,y) pair from D, and you only pay attention to the y part     None of these are particularly good/generally applicable classifiers    Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9 ",
  "wordCount" : "585",
  "inLanguage": "en",
  "datePublished": "2023-01-17T22:19:16-05:00",
  "dateModified": "2023-01-19T23:55:12-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mushetty.me/notes/machinelearning/machine/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "mushetty.me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mushetty.me/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mushetty.me" accesskey="h" title="mushetty.me (Alt + H)">mushetty.me</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mushetty.me/simulations/" title="Simulations">
                    <span>Simulations</span>
                </a>
            </li>
            <li>
                <a href="https://mushetty.me/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://mushetty.me/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Machine Learning Notes
    </h1>
    <div class="post-meta"><span title='2023-01-17 22:19:16 -0500 EST'>Date Created: January 17, 2023</span>&nbsp;·&nbsp;<span title='2023-01-19 23:55:12 -0500 EST'>Last Modified: January 19, 2023</span>

</div>
  </header> 
  <div class="post-content"><p>Compilation of notes for Machine Learning class for Spring 2023.</p>
<ul>
<li><a href="#administrative-stuff">Administrative Stuff</a></li>
<li><a href="#topic-1">Topic 1</a>
<ul>
<li><a href="#workflow-of-classification-problems-supervised-learning">Workflow of Classification Problems (Supervised Learning)</a></li>
<li><a href="#statistical-approach">Statistical Approach</a>
<ul>
<li><a href="#classifier">Classifier</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#topic-2">Topic 2</a></li>
<li><a href="#topic-3">Topic 3</a></li>
<li><a href="#topic-4">Topic 4</a></li>
<li><a href="#topic-5">Topic 5</a></li>
<li><a href="#topic-6">Topic 6</a></li>
<li><a href="#topic-7">Topic 7</a></li>
<li><a href="#topic-8">Topic 8</a></li>
<li><a href="#topic-9">Topic 9</a></li>
</ul>
<h2 id="administrative-stuff">Administrative Stuff<a hidden class="anchor" aria-hidden="true" href="#administrative-stuff">#</a></h2>
<ul>
<li>All homework is submitted via Gradescope.
<ul>
<li>Written parts must be PDFs and be typeset.
<ul>
<li>Plots and data analysis from programming part must be included in written part</li>
</ul>
</li>
<li>Programming parts are mostly turned in as a separate assignment on Gradescope</li>
</ul>
</li>
<li>40 % HW, 30% Midterm, 30% Final</li>
</ul>
<h2 id="topic-1">Topic 1<a hidden class="anchor" aria-hidden="true" href="#topic-1">#</a></h2>
<ul>
<li>Inputs encoded as vectors ($x \in X$):
<ul>
<li>$\vec{x} = &lt;x_{1},x_{2},&hellip;x_{n}&gt;$</li>
<li>Can think of each component as a measurement that you make</li>
</ul>
</li>
<li>Output is another vector ($y \in Y$):
<ul>
<li>$\vec{y} = &lt;y_{1},y_{2},&hellip;y_{m}&gt;$</li>
</ul>
</li>
<li>Goal is to figure out the function that maps x to y
<ul>
<li>You only have a limited number of samples from X
<ul>
<li>You need to make predictions from this limited sample size</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="workflow-of-classification-problems-supervised-learning">Workflow of Classification Problems (Supervised Learning)<a hidden class="anchor" aria-hidden="true" href="#workflow-of-classification-problems-supervised-learning">#</a></h3>
<ul>
<li>$(\vec{x_{1}},y_{1}),(\vec{x_{2}},y_{2})&hellip; \in X \times Y$
<ul>
<li>Example: $X = \mathbb{R^{d}}$ and $Y = {0,1}$ for a binary classification from a image with d pixels</li>
<li>Assumption: there exists a &ldquo;relatively simple&rdquo; function $f^{*}: X \rightarrow Y$ such that $f^{*}(\vec{x_{i}}) = y_{i}$ for most i
<ul>
<li>$*$ in ML implies some optimum value (in this case, an optimal function)</li>
<li><strong>Most</strong> includes that possibility that there is some fundamental noise in your data</li>
<li>Say someone wrote a number that kind of looks like 7 and kinda looks like 1. For the same image, the true $y_{i}$ could be either 7 or 1  (depending on the person&rsquo;s intentions). Hence, to force $f^{*}$ be a correct for all inputs misses this uncertainty in the true output</li>
</ul>
</li>
<li>Learning task: given $n$ examples from the data, from $\hat{f} \approx f^{*}$
<ul>
<li>$\hat{f}$ denotes a function that tries its best to be optimal</li>
</ul>
</li>
<li>Goal: $\hat{f}$ gives mostly correct prediction on unseen examples</li>
</ul>
</li>
</ul>
<h3 id="statistical-approach">Statistical Approach<a hidden class="anchor" aria-hidden="true" href="#statistical-approach">#</a></h3>
<ul>
<li>$(\vec{x_{1}},y_{1}),(\vec{x_{2}},y_{2})&hellip;(\vec{x_{n}},y_{n})$ samples are drawn from the underlying distribution
<ul>
<li>For simplicity, we assume each sample is drawn independently from the <strong>same</strong> underlying distribution is (called i.i.d. assumption)</li>
</ul>
</li>
<li>The learning algorithm draws $\hat{f}$ from a pool of models $\mathbb{F}$ that maximizes the label agreement with the training data
<ul>
<li>How do we select $f \in \mathbb{F}$?</li>
<li>Maximum likelihood (best fits the data)</li>
<li>Maximum a posteriori (best fits the data but incorporates prior assumptions)</li>
<li>Optimization of &lsquo;loss&rsquo; criterion (best discriminates the labels)</li>
</ul>
</li>
</ul>
<h4 id="classifier">Classifier<a hidden class="anchor" aria-hidden="true" href="#classifier">#</a></h4>
<ul>
<li>We are given a joint input/output space ($X \times Y$)
<ul>
<li>The data is distributed like $D(X \times Y)$
<ul>
<li>You have a region where your inputs can reside in ($X \times Y$) and D tells you the density of inputs on the space</li>
<li>You can visualize this space by either:
<ul>
<li>Taking slices at a fixed variable value ($P[Y,X=x]$) or the conditional distribution</li>
<li>Adding up all the slices along a variable ($P[Y]$) or the marginal distribution</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>A classifier is a measurable function of the type $f: x \in Y$
<ul>
<li>Ex1 (Constant): $f_{1}(\vec{x}) = y$ for some fixed $y \in Y$.</li>
<li>Ex2(Threshold): $f_{2}(x) = \begin{cases} 0\ if \ x \geq 5 \\ 1 \ otherwise  \end{cases}$</li>
<li>Ex3 (Majority class): $f_{3}(x) = arg\ max_{y\in Y} P[Y=y]$
<ul>
<li>This asks: What argument in Y yields the maximum probability?</li>
<li>Implicitly, this probability is the marginalized distribution on X</li>
<li>More explicitly, you can write probability as:</li>
<li>$P[Y=y] = P_{y \approx D}|_{Y}[Y=y]$
<ul>
<li>In words, you draw a (x,y) pair from D, and you only pay attention to the y part</li>
</ul>
</li>
</ul>
</li>
<li>None of these are particularly good/generally applicable classifiers</li>
</ul>
</li>
</ul>
<h2 id="topic-2">Topic 2<a hidden class="anchor" aria-hidden="true" href="#topic-2">#</a></h2>
<h2 id="topic-3">Topic 3<a hidden class="anchor" aria-hidden="true" href="#topic-3">#</a></h2>
<h2 id="topic-4">Topic 4<a hidden class="anchor" aria-hidden="true" href="#topic-4">#</a></h2>
<h2 id="topic-5">Topic 5<a hidden class="anchor" aria-hidden="true" href="#topic-5">#</a></h2>
<h2 id="topic-6">Topic 6<a hidden class="anchor" aria-hidden="true" href="#topic-6">#</a></h2>
<h2 id="topic-7">Topic 7<a hidden class="anchor" aria-hidden="true" href="#topic-7">#</a></h2>
<h2 id="topic-8">Topic 8<a hidden class="anchor" aria-hidden="true" href="#topic-8">#</a></h2>
<h2 id="topic-9">Topic 9<a hidden class="anchor" aria-hidden="true" href="#topic-9">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://mushetty.me">mushetty.me</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
