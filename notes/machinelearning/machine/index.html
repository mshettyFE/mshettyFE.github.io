<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Machine Learning Notes | mushetty.me</title>
<meta name="keywords" content="">
<meta name="description" content="Compilation of notes for Machine Learning class for Spring 2023.
 Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier   Maximum Likelihood Estimation  Example   Convergence Naive Bayes Classifier How do you quantify the quality of a classifier? Approaches to Classification  Generative Approach  Advantages Disadvantages   Discriminative Approach  Advantages Disadvantages       Topic 2  Nearest K-Neighbor  Importance of Closeness for k-Nearest Neighbors  Distances Similarities   Issues with the nearest neighbor k-NN Optimality  Proof Practical Considerations of k-nearest neighbor  Finding the k-th nearest neighbors takes time   Metric of Closeness is Sometimes Unclear  Derivative w.">
<meta name="author" content="">
<link rel="canonical" href="https://mushetty.me/notes/machinelearning/machine/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a5456a2fb6485e28d3190c101c9f7fea21d7eddeb7cf1b8dddf20aeb314df0cf.css" integrity="sha256-pUVqL7ZIXijTGQwQHJ9/6iHX7d63zxuN3fIK6zFN8M8=" rel="preload stylesheet" as="style">
<link rel="stylesheet" href="" />
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://mushetty.me/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mushetty.me/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mushetty.me/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mushetty.me/apple-touch-icon.png">
<link rel="mask-icon" href="https://mushetty.me/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


<meta property="og:title" content="Machine Learning Notes" />
<meta property="og:description" content="Compilation of notes for Machine Learning class for Spring 2023.
 Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier   Maximum Likelihood Estimation  Example   Convergence Naive Bayes Classifier How do you quantify the quality of a classifier? Approaches to Classification  Generative Approach  Advantages Disadvantages   Discriminative Approach  Advantages Disadvantages       Topic 2  Nearest K-Neighbor  Importance of Closeness for k-Nearest Neighbors  Distances Similarities   Issues with the nearest neighbor k-NN Optimality  Proof Practical Considerations of k-nearest neighbor  Finding the k-th nearest neighbors takes time   Metric of Closeness is Sometimes Unclear  Derivative w." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mushetty.me/notes/machinelearning/machine/" /><meta property="article:section" content="notes" />
<meta property="article:published_time" content="2023-01-17T22:19:16-05:00" />
<meta property="article:modified_time" content="2023-02-21T21:27:18-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning Notes"/>
<meta name="twitter:description" content="Compilation of notes for Machine Learning class for Spring 2023.
 Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier   Maximum Likelihood Estimation  Example   Convergence Naive Bayes Classifier How do you quantify the quality of a classifier? Approaches to Classification  Generative Approach  Advantages Disadvantages   Discriminative Approach  Advantages Disadvantages       Topic 2  Nearest K-Neighbor  Importance of Closeness for k-Nearest Neighbors  Distances Similarities   Issues with the nearest neighbor k-NN Optimality  Proof Practical Considerations of k-nearest neighbor  Finding the k-th nearest neighbors takes time   Metric of Closeness is Sometimes Unclear  Derivative w."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Machine Learning Notes",
      "item": "https://mushetty.me/notes/machinelearning/machine/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Machine Learning Notes",
  "name": "Machine Learning Notes",
  "description": "Compilation of notes for Machine Learning class for Spring 2023.\n Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier   Maximum Likelihood Estimation  Example   Convergence Naive Bayes Classifier How do you quantify the quality of a classifier? Approaches to Classification  Generative Approach  Advantages Disadvantages   Discriminative Approach  Advantages Disadvantages       Topic 2  Nearest K-Neighbor  Importance of Closeness for k-Nearest Neighbors  Distances Similarities   Issues with the nearest neighbor k-NN Optimality  Proof Practical Considerations of k-nearest neighbor  Finding the k-th nearest neighbors takes time   Metric of Closeness is Sometimes Unclear  Derivative w.",
  "keywords": [
    
  ],
  "articleBody": "Compilation of notes for Machine Learning class for Spring 2023.\n Administrative Stuff Topic 1  Workflow of Classification Problems (Supervised Learning) Statistical Approach  Classifier   Maximum Likelihood Estimation  Example   Convergence Naive Bayes Classifier How do you quantify the quality of a classifier? Approaches to Classification  Generative Approach  Advantages Disadvantages   Discriminative Approach  Advantages Disadvantages       Topic 2  Nearest K-Neighbor  Importance of Closeness for k-Nearest Neighbors  Distances Similarities   Issues with the nearest neighbor k-NN Optimality  Proof Practical Considerations of k-nearest neighbor  Finding the k-th nearest neighbors takes time   Metric of Closeness is Sometimes Unclear  Derivative w.r.t. Matrix   Not Saving All the Training Data       Topic 3  Overfitting the Training Data Decision Boundaries  Linear Classifier  Finding The Weights     Perceptron  Perceptron Mistake Bound  Proof     Nonlinear Regression  Quadratic Boundaries  Proof that All Data is Linear Separable in some Space Kernel Trick (to Deal with Computation)       Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9  Administrative Stuff  All homework is submitted via Gradescope.  Written parts must be PDFs and be typeset.  Plots and data analysis from programming part must be included in written part   Programming parts are mostly turned in as a separate assignment on Gradescope   40 % HW, 30% Midterm, 30% Final  Topic 1  Inputs encoded as vectors ($x \\in X$):  $\\vec{x} = $ Can think of each component as a measurement that you make   Output is another vector ($y \\in Y$):  $\\vec{y} = $   Goal is to figure out the function that maps x to y  You only have a limited number of samples from X  You need to make predictions from this limited sample size      Workflow of Classification Problems (Supervised Learning)  $(\\vec{x_{1}},y_{1}),(\\vec{x_{2}},y_{2})… \\in X \\times Y$  Example: $X = \\mathbb{R^{d}}$ and $Y = {0,1}$ for a binary classification from a image with d pixels Assumption: there exists a “relatively simple” function $f^{*}: X \\rightarrow Y$ such that $f^{*}(\\vec{x_{i}}) = y_{i}$ for most i  $*$ in ML implies some optimum value (in this case, an optimal function) Most includes that possibility that there is some fundamental noise in your data Say someone wrote a number that kind of looks like 7 and kinda looks like 1. For the same image, the true $y_{i}$ could be either 7 or 1 (depending on the person’s intentions). Hence, to force $f^{*}$ be a correct for all inputs misses this uncertainty in the true output   Learning task: given $n$ examples from the data, from $\\hat{f} \\approx f^{*}$  $\\hat{f}$ denotes a function that tries its best to be optimal   Goal: $\\hat{f}$ gives mostly correct prediction on unseen examples    Statistical Approach  $(\\vec{x_{1}},y_{1}),(\\vec{x_{2}},y_{2})…(\\vec{x_{n}},y_{n})$ samples are drawn from the underlying distribution  For simplicity, we assume each sample is drawn independently from the same underlying distribution is (called i.i.d. assumption)   The learning algorithm draws $\\hat{f}$ from a pool of models $\\mathbb{F}$ that maximizes the label agreement with the training data  How do we select $f \\in \\mathbb{F}$? Maximum likelihood (best fits the data) Maximum a posteriori (best fits the data but incorporates prior assumptions) Optimization of ‘loss’ criterion (best discriminates the labels)    Classifier  We are given a joint input/output space ($X \\times Y$)  The data is distributed like $D(X \\times Y)$  You have a region where your inputs can reside in ($X \\times Y$) and D tells you the density of inputs on the space You can visualize this space by either:  Taking slices at a fixed variable value ($P[Y,X=x]$) or the conditional distribution Adding up all the slices along a variable ($P[Y]$) or the marginal distribution       A classifier is a measurable function of the type $f: x \\in Y$  Ex1 (Constant): $f_{1}(\\vec{x}) = y$ for some fixed $y \\in Y$. Ex2(Threshold): $f_{2}(x) = \\begin{cases} 0\\ if \\ x \\geq 5 \\\\ 1 \\ otherwise \\end{cases}$ Ex3 (Majority class): $f_{3}(x) = arg\\ max_{y\\in Y} P[Y=y]$  This asks: What argument in Y yields the maximum probability? Implicitly, this probability is the marginalized distribution on X More explicitly, you can write probability as: $P[Y=y] = P_{y \\approx D}|_{Y}[Y=y]$  In words, you draw a (x,y) pair from D, and you only pay attention to the y part     None of these are particularly good/generally applicable classifiers Ex4 (Sample Average Class):  $\\frac{1}{n}\\Sigma \\mathbb{I}[f(x_{i}) = y_{i}]$  $\\mathbb{I}$ is the indicator function that maps a boolean expression (like $f(x_{i}) = y_{i}$) to either 0 (false) or 1 (true) This is from a sample     Ex5 (Population Average):  $\\mathbb{E}_{(x,y)\\in D}[\\mathbb{I}[f(x)=y]]= P[f(x)=y]$  This is on the entire population. Essentially a probability that states how often this classifier f gets the output correct called the accuracy function acc(f)       Let’s look at the following classifier:  $f(\\vec{x})= arg\\ max_{y\\in Y}\\ P[y=Y|X=\\vec{x}]$  In words, find y that maximizes the probability given a particular x (called Bayes classifier) This is the best classifier. But most of the time, you can’t actually compute $P[y=Y|X=\\vec{x}]$ for all inputs. You try your best to estimate these probabilities Proof:  Assume binary classification for output space (for simplicity)  Let $f(\\vec{x})= arg\\ max_{y\\in Y}\\ P[y=Y|X=\\vec{x}]$. Let $g(\\vec{x})\\rightarrow \\{0,1\\}$. Then $P_{(\\vec{x},y)}[g(\\vec{x})=y] \\leq P_{(\\vec{x},y)}[f(\\vec{x})=y]$   Let’s look at a single example (Fix $\\vec{x}\\in X$). Then for any classifier h  $P[h(\\vec{x})=y|X=\\vec{x}]=P[h(\\vec{x})=1|X=\\vec{x}]+P[h(\\vec{x})=0|X=\\vec{x}]$ Since $h(\\vec{x})=1$ is a constant, we can use $\\mathbb{I}$ to rewrite above  $\\mathbb{I}[h(\\vec{x})=1]P[Y=1|X=\\vec{x}]+\\mathbb{I}[h(\\vec{x})=0]P[Y=0|X=\\vec{x}] =\\mathbb{I}[h(\\vec{x})=1]\\eta(\\vec{x})+\\mathbb{I}[h(\\vec{x})=0](1- \\eta(\\vec{x}))$  Where $\\eta(\\vec{x}) := P[Y=1|X=\\vec{x}]$     Hence we need to show that $P[f(\\vec{x})=y|X=\\vec{x})]-P[g(\\vec{x})=y|X=\\vec{x})] \\geq 0$  This equals $\\eta(\\vec{x})[\\mathbb{I}[f(\\vec{x})=1]-\\mathbb{I}[g(\\vec{x})=1]]+(1-\\eta(\\vec{x}))[\\mathbb{I}[f(\\vec{x})=0]-\\mathbb{I}[g(\\vec{x})=0]] = (2\\eta(\\vec{x})-1)[\\mathbb{I}[f(\\vec{x})=1]-\\mathbb{I}[g(\\vec{x})=1]]$  You use $\\mathbb{I}[f(\\vec{x})=0] = 1 -\\mathbb{I}[f(\\vec{x})=1]$   If both classifiers guess the same output, there is no degradation in the performance in either classifier What happens if the outputs are different?  In either case$[\\mathbb{I}[f(\\vec{x})=1]-\\mathbb{I}[g(\\vec{x})=1]]=\\pm 1$ depending on if $f(x)$ is correct or $g(x)$ is correct, Say $f(x)$ returns 1:  This means that probability of $f(x)$ selecting 1 is greater than 0.5, hence $P[Y=1|X-\\vec{x}] =\\eta(\\vec{x}) \\geq 0.5$ so $(2\\eta(\\vec{x})-1)[\\mathbb{I}[f(\\vec{x})=1]-\\mathbb{I}[g(\\vec{x})=1]] \\geq 0$   A similar argument holds for $f(x)$ returning 0 (ie. the bayes classifier fails and the alternate classifier works)         Since we know that this works for a fixed example, we can integrate across all $x\\in X$ to prove the original theorem $P_{(\\vec{x},y)}[g(\\vec{x})=y] \\leq P_{(\\vec{x},y)}[f(\\vec{x})=y]$  Want to show $\\mathbb{E}[\\mathbb{I}[P_{(\\vec{x},y)}[g(\\vec{x})=y]]] - \\mathbb{E}[\\mathbb{I}[P_{(\\vec{x},y)}[f(\\vec{x})=y]]] \\geq 0$  $\\mathbb{E}[\\alpha(\\vec{x},y)] = \\int_{(\\vec({x},y)\\in X\\times Y)} \\alpha(\\vec{x},y)P((\\vec{x},y))d((\\vec{x},y))$ So we can rewrite the expectation values using linearity: $\\alpha(x,y)=\\mathbb{I}[P_{(\\vec{x},y)}[f(\\vec{x})=y]]$ and $\\beta(x,y)=\\mathbb{I}[P_{(\\vec{x},y)}[g(\\vec{x})=y]]$:  $\\mathbb{E}[\\mathbb{I}[P_{(\\vec{x},y)}[g(\\vec{x})=y]]] - \\mathbb{E}[\\mathbb{I}[P_{(\\vec{x},y)}[f(\\vec{x})=y]]] = \\int (\\alpha(\\vec{x},y)-\\beta(\\vec{x},y)) p(\\vec{x},y)d(x,y)=\\int_{x}[\\int_{y} (\\alpha-\\beta) p(y|x)dy]dx$  The inner integral is the probability we calculated earlier (the one that was always greater than zero). Hence the double integral is always greater than or equal to 0             Since we can’t normally acheive this classifier, how can we estimate it?  $f(\\vec{x})= argmax_{y\\in Y} P[Y=y|X=\\vec{x}]= argmax_{y\\in Y}\\frac{P[X=\\vec{x}|Y=y]\\cdot P[Y=y]}{P[X=\\vec{x}]}$  denominator independent of Y. This means that  $argmax_{y\\in Y}\\frac{P[X=\\vec{x}|Y=y]\\cdot P[Y=y]}{P[X=\\vec{x}]} = argmax_{y\\in Y}P[X=\\vec{x}|Y=y]\\cdot P[Y=y]$  The value of the function on the LHS is not the same as the RHS, but the y that maximizes the LHS and RHS is the same Instead of figuring out an infinite number of estimates (1 for each $\\vec{x}$ in $\\R^{d}$ space) you instead make 1 estimate for each $y\\in Y$, which is a much smaller space  Instead of an infinite number of Bernoulli distributions, you have a finite number of multidimensional distributions   $P[Y=y]$ is called the Class Prior or the prior $P[X=\\vec{x}|Y=y]$ is called the Class Conditional or the Conditional $P[Y=y|X=\\vec{x}]$ is called the Class Posterior or the Posterior          Maximum Likelihood Estimation  We are given some data $\\vec{x_{1}}… \\vec{x_{n}} \\in X$ independently draw Say we have a model class $P = \\{p_{\\theta}|\\theta\\in\\Theta\\}$ where $\\Theta$ is the parameter space  For Gaussians, $\\in \\Theta$ and $p_{\\theta} = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp(\\frac{-1}{2}\\frac{(x-\\mu)^{2}}{\\sigma^{2}})$   Find the model that best fits the data Define the likelihood function $\\mathbb{L}(\\theta|X)= P(X|\\theta)=P(x_{1},…,x_{n}|\\theta)=\\Pi_{i=1}^{n}P(\\vec{x_{i}}|\\theta) = \\Pi_{i=1}^{n}p_{\\theta}(\\vec{x_{i}})$  The likelihood function defines how probable (or how likely) is the data given the model $p_{\\theta}$   we want to find $\\theta$ that maximizes the likelihood function: $argmax_{\\theta\\in\\Theta}\\mathbb{L} = argmax_{\\theta\\in\\Theta}\\Pi_{i=1}^{n}p_{\\theta}(\\vec{x_{i}})$ Say for instance, that we have some data and we want to fit a Gaussian to it:  Find $argmax_{\\mu,\\sigma^{2}}\\Pi_{i=1}^{n}p_{\\mu,\\sigma^{2}}(\\vec{x_{i}})$ A trick to avoid using the product rule too much:  $argmax_{\\theta}\\mathbb{L}(\\theta|X)= argmax_{\\theta} log(\\mathbb{L}(\\theta|X))$ This turns the product into sums and eliminates the exp of the Guassian. After doing this manipulation, we get: find $argmax_{\\mu,\\sigma^{2}} \\Sigma_{i=1}^{n}[\\frac{-1}{2}log(2\\pi\\sigma^{2})-\\frac{(x_{i}-\\mu)^{2}}{2\\sigma^{2}}]$ Maximize w.r.t $\\mu$ to and solve for the stationary points yield $\\mu_{ML} = \\frac{1}{n}\\Sigma_{i=1}^{n}x_{i}$  This is an unbiased estimator that converges to the true $\\mu$ as the number of samples go to infinity   Can also take w.r.t. $\\sigma^{2}$: $\\sigma_{ML}^{2}=\\frac{1}{n}\\Sigma_{i=1}^{n}(x_{i}-\\mu)^{2}$  We can find the sample variance by plugging in $\\mu_{ML}$ for the true $\\mu$ (This is where the $\\frac{1}{n-1}$ thing comes from)        Example  Say that we want to estimate biological gender (male or female) based on weight and height The distribution is spread like $D[X\\times Y]$ The Bayes classifier is thus:  $f(\\vec{x}) = argmax_{y\\in\\{male,female\\}} P[X=\\vec{x}|Y=y]\\cdot P[Y=y]$  $\\hat{P}[Y=male]=$ fraction of training data labelled as male $\\hat{P}[Y=female]=$ fraction of training data labelled as female $\\hat{P}[X|Y=male]=p_{\\theta(male)}(X)$  Use MLE with only male data   $\\hat{P}[X|Y=female]=p_{\\theta(female)}(X)$  Use MLE with only female data   Say that the conditional follows a 2D Gaussian distribution     Another way to put this:  We have a bunch of labelled points in $\\mathbb{R^{2}}$  Look only at the male labels. Fit a 2D Gaussian to this data (Find some mean center and the covariance matrix) Do the same with the female labels     Once you have gotten your distributions, say that you get a new data point. The distribution (male or female) that yields the higher probability is your Bayes classifier’s output  Convergence  How fast does our sample converges to the population?  Define an error function $E = \\int(P(x)-\\hat{P}(x))^{2}dx \\leq n^{\\frac{-2}{2+d}} \\approx n^{\\frac{-1}{d}}$  If the dimensionality of the problem gets larger (d goes up), then the convergence becomes much slower (you need many many more samples in a higher dimensional space to get the same threshold as a lower dimensional space)  Say that you have a tolerance of $\\epsilon$ on the error. Then you need $n \\geq \\epsilon^{d}$ samples        Naive Bayes Classifier  $f(\\vec{x}) = argmax_{y\\in Y} P[X=\\vec{x}|Y=y]\\cdot p[Y=y] = argmax_{y\\in Y}\\Sigma_{y=1}^{d}P[X^{j}=x^{j}|Y=y]\\cdot P[Y=y]$  In words: each feature of the input (the $x_{i}$) are independent from each other given marginalization on the output You break a d-dimensional problem into d 1-dimensional problem, where each 1-d problem requires $\\frac{1}{\\sqrt{n}}$    How do you quantify the quality of a classifier?  If you know the population distribution, then just run the classifier with that data If you have a sample distribution, then you have a biased qualifier since you chose your classifying function from the training data Split the training data into training and testing data  The testing data provides an unbiased means of testing your model    Approaches to Classification Generative Approach  You assume a model of the underlying distribution. When you get a new point, you find which model is the best  Advantages  The model gives you an interpretation of how data gets generated from a population  Since you have a interpretation of the underlying distribution, you can generate simulated data    Disadvantages  You need to pick a model This is overkill if you just need a classification result, which costs time and money You also are more prone to errors since the sole focus is not classification  Discriminative Approach  you don’t assume an underlying model. Given a new point, you directly figure out the classification  Advantages  Typically you get better classification accuracy  Disadvantages  You gain no understanding of the population  Topic 2 Nearest K-Neighbor Importance of Closeness for k-Nearest Neighbors  If you have improper units (say that you measure human height in lightyears and human weight in micrograms), this can stretch and compress the axes and exaggerates the importance of a particular feature Choosing the correct closeness metric allows to to scale the axes to avoid this problem  Distances  If $X\\in \\R^{d}$, then an example of a distance metric is the $L^{p}$ norm ($(\\Sigma_{i=1}^{d} (\\vec{x_{i}}-\\vec{\\hat{x_{j}}})^{p})^\\frac{1}{p}$):  $p =2$ is Euclidean distance $p =1$ Manhattan distance or cityblock distance $p=\\infty$ max distance $p=0$ count ‘non-zero’ distance (ignore $\\frac{1}{p}$ power in norm definition)   Edit distance (genomics):  Domain specific: the number of mutations needed to convert one sequence to another   Kendell-Tau distance (to compare rankings)  What is the minimum number of adjacent pairwise swaps needed to convert 1 ranking to another?    Similarities  take the distance and construct the function $\\frac{1}{1+||\\vec{x_{1}}-\\vec{x_{2}}||_{p}}$ Cosine similarity: Find the cosine between the two vector  Issues with the nearest neighbor  Sensitive to noise in data, so labelling is unstable  Solution: look at the kth nearest labels to make this stable    k-NN Optimality  Stone’s Theorem:  Theorem 1: for a fixed k, as $n \\rightarrow \\infty$, k-NN classification error converges to no more than twice Bayes classifier error Theorem 2: if $k\\rightarrow\\infty$, $n\\rightarrow \\infty$, but $\\frac{k}{n}\\rightarrow0$ (the rate of growth goes to zero ie. n grows faster than k), then the k-NN classifier converges to Bayes classifier   Why $\\frac{k}{n}$? Say that n is fixed an k increases. Eventually, you are averaging over the entire dataset. Hence, if you increase n as well at a rate faster than k, then you are still sampling a small local region of the dataset  Proof  Let k=1 Notation:  P[e] = NN error rate = $1-P_{x,y\\in D_{n}}[[f(x)_{n}=y]]$  ie. error = 1-accuracy   $D_{n} = $ = labeled data (size n)  The sample data that you have drawn from the underlying distribution   $x_{n}$ = nearest neighbor of $x_{t}$ in $D_{n}$ $y_{n}$ = label of nearest neighbor $x_{t}$ and $y_{t}$ are a fixed test point of $x_{t}$   What is the error at a fixed test point $x_{t}$?  $lim_{n\\rightarrow\\infty} P_{y_{t},D_{n}}[e|x_{t}]$  There is some randomness in $y_{t}$ and $D_{n}$     Recall that $P[A] = \\Sigma_{b\\in B} P[A,B=b] = \\Sigma_{b\\in B} P[A|B=b]P[B=b]$ If you condition on C, you get:  $P[A|C] = \\Sigma_{b\\in B} P[A,B=b|C] = \\Sigma_{b\\in B} P[A|B=b,C]P[B=b|C]$   Using the appropriate substitutions, can write: $lim_{n\\rightarrow\\infty} P_{y_{t},D_{n}}[e|x_{t}] = \\int_{n\\rightarrow\\infty} P_{yt}[e|x_{t}X_{n}]P[X_{n}|x_{t}]dX_{n} = lim_{n\\rightarrow\\infty}\\int P_{yt}[e|x_{t}n_{n}]P[n_{n}|x_{t}] $  In words, the second equality states that you fix your dataset   You then write out what nearest neighbor means:  $lim_{n\\rightarrow\\infty}P_{y_{t},D_{n}}[e|x_{t}] = lim_{n\\infty}[1-\\Sigma_{y\\in Y} P(y_{t}=y,y_{n}=y|x_{t},x_{n})]P[x_{n}x_{t}] dx = lim_{n\\infty}[1-\\Sigma_{y\\in Y} P(y_{t}=y|x_{t})P(y_{n}=y|x_{t})]P[x_{n}|x_{t}] dx$  The first is the definition of k-NN (the output of the nearest neighbor is the same as the test sample) The second equality uses independence (ie. truth level labels of the test point and neighbor points are conditionally independent)     You take the limit w.r.t. $n\\rightarrow\\infty$. $P[y_{t}=y|x_{t}]=P[y_{n}=y|x_{n}]$. When you integrate $P[x_{n}|x_{t}] dx$ you get 1, hence  $lim_{n\\rightarrow\\infty}P_{y_{t},D_{n}}[e|x_{t}] = 1-\\Sigma_{y\\in Y}P^{2}(y_{t}=y|x_{t})$   If Bayes classifier return $y*$ at point $x_{t}$, then  $1-\\Sigma P^{2}(y_{t}=y|x_{t}) \\leq 1- P^{2}(y_{t}=y*|x_{t})$ or $\\leq 2(1-P(y_{t}-y*|x_{t})) = 2P^{*}[e|x_{t}]$    Practical Considerations of k-nearest neighbor Finding the k-th nearest neighbors takes time  $O(nd)$ where n is the number of samples and d is the dimensionality of each sample Simplify to $\\R$. Then you can sort your data in a binary search tree  Look up in $O(log\\ n)$ Preprocessing is $O(n\\ log\\ n)$   Instead of thinking in terms of midpoints, think in terms of regions After you divide up $\\R$, you can put your data sample into one of the bins  Problem: the hard threshold can misclassify the nearest neighbor. Instead, it gives you a near neighbor (in practice ,this distinction doesn’t really matter if you have enough data)  Solution: Soften the boundary, where if your data fall in the overlap region, then you traverse both branches of the tree     To extend to higher dimensions, take the projection of your data onto one of your dimensions, then do the median. Recursively repeat this process, select which coordinate you project onto each time  You can use the same coordinate, although if you overuse a particular coordinate, than your fills (bins) can be elongated along 1 direction  You can mitigate this problem by selecting your coordinate based on the one that has the smaller variance     Problem/Caveat: What if a different direction is better to subdivide the data? Perform PCA, which calculates the direction of maximum variance, and then project onto this direction  Cons: need to do an eigenvalue decomposition, which has a big online overhead   This data structure is called a k-d tree  Used in databases (boolean selections select regions of the space to reduce the search problem   This problem can also be solved by a locality-sensitive hash map  Metric of Closeness is Sometimes Unclear  Say that you have a particular task and you gather data with some sort of features  Some features are not relevant for the task at hand (ie. these features are noisy). However, for other tasks, these noisy features might be of great importance You could also have highly correlated features that can distort the Euclidean distance   Solution: we could re-weight the contribution of each feature:  $\\rho(\\vec{x_{1}},\\vec{x_{2}};\\vec{w}) = [(\\vec{x_{1}}-\\vec{x_{2}})^{T}W(\\vec{x_{1}}-\\vec{x_{2}})]^{\\frac{1}{2}}$ where $W$ is a diagonal matrix How do we learn what $W$ is?  Create two sets, one similar and one dissimilar  $S = [(\\vec{x_{i}},\\vec{x_{j}})|y_{i}=y_{j}]$ $D = [(\\vec{x_{i}},\\vec{x_{j}})|y_{i}\\neq y_{j}]$   Define a cost function $\\Psi(\\vec{w}) = \\lambda\\Sigma_{(\\vec{x_{i}},\\vec{x_{j}})\\in S} \\rho(\\vec{x_{1}},\\vec{x_{2}};\\vec{w})-(1-\\lambda)\\Sigma_{(\\vec{x_{i}},\\vec{x_{j}})\\in D} \\rho(\\vec{x_{1}},\\vec{x_{2}};\\vec{w})$ $\\lambda$ controls which force dominates In words, we want to find the weights that minimize the distance between similar points and maximize the distance between dissimilar points We can simplify this by squaring $\\rho$ in our cost function to avoid annoying square root functions      Derivative w.r.t. Matrix  What is $\\frac{d}{dW}(\\lambda\\Sigma_{i,j}(x_{i}-x_{j})^{T}W(x_{i}-x_{j}))$ where W is a matrix We can think of the function as a map from $\\R^{d\\times d}\\rightarrow \\R$  The derivative must have the same dimension as the domain (ie $R^{d\\times d}$)   Hence, we can intuit that the derivative is $\\Sigma_{i,j}(x_{i}-x_{j})(x_{i}-x_{j})^{T}$ (this is correct)  Not Saving All the Training Data  Instead of saving all the training data, you save the classification of the region where the data lies  You also try to make the classification regions as large as possible. Namely, you decide your selection cuts to maximally reduce label uncertainty (ie. as you decide your cut to maximize the purity of each region at each step (up to a certain point to avoid overfitting)) You can quantify label uncertainty as follows  Classification Error:  $u(C) = 1- max_{y}p_{y}$ $p_{y}$ is the fraction of training data labelled as y in cell C   Entropy:  $u(C) = \\Sigma_{(y\\in Y)} p_{y} \\log \\frac{1}{p_{y}}$ Units of bits This is linearly additive for independent events   Gini index:  a measure of how “mixed” a population is $u(C) = \\Sigma_{(y\\in Y)}p_{y}^{2}$ For homogeneity, we take $1-u(C)$     Whatever metric you use, you want to  $argmax_{F,T}[u(C)-(p_{L}u(C_{L})+p_{R}u(C_{R}))]$ Namely, choose a feature F and threshold T that maximizes the fraction of the same labels in a region You keep your features and thresholds parallel to axes to maintain interpretability This is a greedy algorithm Finding the optimal decision tree is NP-hard Uncertainty estimates are unstable Tree complexity is dependent of data geometry      Topic 3 Overfitting the Training Data  As you increase the complexity of a model, you can drive the training error down to zero in the limit  This does not mean that this generalizes to the population You can take a cut of your training data (call this your validation data) to tune how complicated you make your model to avoid this overfitting    Decision Boundaries Linear Classifier  A linear Classifier is defined as a classifier whose decision boundary is a line For simplicity, say that we have a binary classification of -1,1. Recall definition of linearity: $g(a+b) = g(a)+g(b)$ and $g(ca) = cg(a)$  you can drop $g(ca) = cg(a)$ assumption and examine affine decision boundaries (As an abuse of notation, people call these affine decision boundaries linear boundaries)   Let g be a decision boundary such that:  In 1D, we get $g(x) = w_{1}x+w_{0} = 0$ where x is from your data that satisfied these conditions In general $g(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+w_{0}=0$   Define a linear classifier such that $f(x) = 1$ if $g(x) \\geq 0$ and $f(x) = -1$ if $g(x) \\leq 0$ or $f(\\vec(x) = sign(g(\\vec{x}))$ Let $g(x) = (\\vec{w},w_{0})\\cdot(\\vec{x},1)$ where you call the extra 1 the bias. This is called “lifting” the data because all of your data gets assigned a value of 1 in a new dimension. This extra degree of freedom allows an affine classifier to truely become a linear classifier  Finding The Weights  We want to minimize the training error:  $argmin_{\\vec{w}} = \\frac{1}{n}\\Sigma_{i}^{N}\\bold{1}[sign(\\vec{w}\\cdot\\vec{x_i})\\neq y_i]$ The above equals $argmax_{\\vec{w}} \\Sigma_{x_{i},y_{i}=1}\\bold{1}[\\vec{x_i}\\cdot \\vec{w} \\le 0] + \\Sigma_{x_{i},y_{i}=-1}\\bold{1}[\\vec{x_i}\\cdot \\vec{w} \\ge 0]$  This is NP-hard to solve (NP-hard for the approximation as well)   This is not an absolute statement: not every instance of your data is hard to solve  For data where it is NP-hard, you probably don’t want to use a linear classifier in the first place For now, we assume that we have linear separable data (ie. the problem is not NP-hard)     Say that our data has a linear dicision boundary that perfectly separates the training data  We can define the margin $\\gamma$ as the shortest distance between the boundary and the closest data point    Perceptron  Given: labelled training data  Initialize $\\vec{w}^{(0)} = 0$ for t = 1,2,3…  If $\\exist (\\vec{x},y) \\in S$ such that $sign(\\vec{w}^{(t-1)}\\cdot \\vec{x}) \\neq y$  Update $\\vec{w}^{(t)} = \\vec{w}^{(t-1)}+y\\vec{x}$  Remember that $y=\\pm 1$     terminate when there is no such training sample that you misclassify     This algorithm might not terminate because it is too greedy  Perceptron Mistake Bound  Assume that there is a unit length $\\vec{w}^{*}$ Let $R = max_{\\vec{x}\\in S} |\\vec{x}|$ be the radius and $\\gamma$ be the margin The number of iterations T is bounded by $|\\frac{R}{\\gamma}|^2$  Proof  How far is $\\vec{w}$ from $\\vec{w}^{*}$  We want $\\vec{w}$ to converge to $\\vec{w}^{*}$  We can use the dot product as a measure of this. We want to decrease the dot product by reducing the angle and not increasing the lengths   Let’s say that the algorithm made a mistake at time t. Then:  $\\vec{w}^{(t)}\\cdot \\vec{w}^{*} = (\\vec{w}^{(t-1)}+y\\vec{x})\\cdot \\vec{w}^{*} \\geq \\vec{w}^{(t)}\\cdot \\vec{w}^{*}+\\gamma$ $|\\vec{w}^{t}|^2=|\\vec{w}^{t-1}yx|^2 = |\\vec{w}^{t-1}|^2+2y(\\vec{w}^{t-1}\\cdot x)+|y\\vec{x}|^2 \\leq |\\vec{w}^{t-1}|^2+R^{2}$   The above inequalities hold after every iteration. After iteration T:  $T\\gamma \\leq \\vec{w}^{T}\\cdot \\vec{w}^{*} \\leq |\\vec{w}^{T}||\\vec{w}^{*}| \\leq R\\sqrt{T}$ Rearrange to get that $T\\leq (\\frac{R}{\\gamma})^{2}$     This means that the perceptron is an online algorithm: it only looks at a small subset of the data (1 at a time).  Nonlinear Regression  Say that we have some data that is not conducive to linear boundaries  Quadratic Boundaries  For a quadratic term, our quadratic boundary can be defined as $g(\\vec{x}) = w_1x^1_2+w_2x^2_2+w_3x_1x_2+w_4x_1+w_5x_2+w_0$ Suppose that we only look at boundaries of the form $g(\\vec{x}) = w_1x_1^2+w_2x_2^2+w_0 = \\Sigma_{p+q\\leq 2} w^{p,q}x_1^p x_2^q$ Make the change of coordinates $\\chi_1 = x_1^{2}$ and $\\chi_2 = x_2^{2}$ to make this boundary a linear boundary In function notation: $\\phi(x_1,x_2) \\rightarrow (x_1^{2},x_2^{2})$  This is called a feature transformation   For the general case we have $\\phi(x_1,x_2) \\rightarrow (x_1^2,x_2^2,x_1x_2,x_1,x_2,1)$ For d dimensions, we have that  $g(\\vec{x}) \\Sigma_{i,j=1}^{d} \\Sigma_{p+q\\leq 2} w_{i,j}^{p,q} x_{i}^{p}x_{j}^{q}$   Transforming the data into higher dimensions takes effort. We don’t want to do this unless we know that the problem will become linear seperable  Proof that All Data is Linear Separable in some Space  No label information is given to use ($Y$ is unknown) What we are given is n distinct points S = $\\vec{x_1},\\vec{x_2},… \\vec{x_n}$ There exists a feature transformation (kernel) such that for any labelling of S is linearly separable in the transformed space Consider the mapping into $\\mathbb{R}^n$  $\\phi(x_i) \\rightarrow $ or in words, you map the ith data point to the ith component of the $\\mathbb{R}^n$ vector   Then, the decision boundary induced by linear weighting $\\vec{w^{*}} = $ perfectly seperates the data because $\\forall \\vec{x}_i \\rightarrow sign(\\phi(\\vec{x}_i)\\cdot \\vec{w}^{*}) = sign(y_i) = y_i$ This doesn’t work in practice since we don’t know a priori how large out data will be  Kernel Trick (to Deal with Computation)  Explicitly working with generic Kernel space takes time $\\Omega(n) Suppose that taking the dot product $\\dot(\\vec{x_1}\\cdot \\vec{x_2})$ is somehow easy to do  For example, the generic quadratic boundary  make the transformation like the standard transformation, but scale the cross terms up by $\\sqrt$ This goes as $O(d^2)$ The dot product calculation is of $O(d)$: $(1+\\vec{x_i}\\vec{x_j})^2$ This also extend to polynomial transformations $\\phi(\\vec{x_i})\\cdot \\phi(\\vec{x_i}) = (1+\\vec{x_i}\\vec{x_j})^2$ with is also linear in d   If you have a machine learning algorithm that only works with dot products, then you can run it quickly    Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9 ",
  "wordCount" : "3923",
  "inLanguage": "en",
  "datePublished": "2023-01-17T22:19:16-05:00",
  "dateModified": "2023-02-21T21:27:18-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mushetty.me/notes/machinelearning/machine/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "mushetty.me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mushetty.me/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mushetty.me" accesskey="h" title="mushetty.me (Alt + H)">mushetty.me</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mushetty.me/simulations/" title="Simulations">
                    <span>Simulations</span>
                </a>
            </li>
            <li>
                <a href="https://mushetty.me/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://mushetty.me/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Machine Learning Notes
    </h1>
    <div class="post-meta"><span title='2023-01-17 22:19:16 -0500 EST'>Date Created: January 17, 2023</span>&nbsp;·&nbsp;<span title='2023-02-21 21:27:18 -0500 EST'>Last Modified: February 21, 2023</span>

</div>
  </header> 
  <div class="post-content"><p>Compilation of notes for Machine Learning class for Spring 2023.</p>
<ul>
<li><a href="#administrative-stuff">Administrative Stuff</a></li>
<li><a href="#topic-1">Topic 1</a>
<ul>
<li><a href="#workflow-of-classification-problems-supervised-learning">Workflow of Classification Problems (Supervised Learning)</a></li>
<li><a href="#statistical-approach">Statistical Approach</a>
<ul>
<li><a href="#classifier">Classifier</a></li>
</ul>
</li>
<li><a href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a>
<ul>
<li><a href="#example">Example</a></li>
</ul>
</li>
<li><a href="#convergence">Convergence</a></li>
<li><a href="#naive-bayes-classifier">Naive Bayes Classifier</a></li>
<li><a href="#how-do-you-quantify-the-quality-of-a-classifier">How do you quantify the quality of a classifier?</a></li>
<li><a href="#approaches-to-classification">Approaches to Classification</a>
<ul>
<li><a href="#generative-approach">Generative Approach</a>
<ul>
<li><a href="#advantages">Advantages</a></li>
<li><a href="#disadvantages">Disadvantages</a></li>
</ul>
</li>
<li><a href="#discriminative-approach">Discriminative Approach</a>
<ul>
<li><a href="#advantages-1">Advantages</a></li>
<li><a href="#disadvantages-1">Disadvantages</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#topic-2">Topic 2</a>
<ul>
<li><a href="#nearest-k-neighbor">Nearest K-Neighbor</a>
<ul>
<li><a href="#importance-of-closeness-for-k-nearest-neighbors">Importance of Closeness for k-Nearest Neighbors</a>
<ul>
<li><a href="#distances">Distances</a></li>
<li><a href="#similarities">Similarities</a></li>
</ul>
</li>
<li><a href="#issues-with-the-nearest-neighbor">Issues with the nearest neighbor</a></li>
<li><a href="#k-nn-optimality">k-NN Optimality</a>
<ul>
<li><a href="#proof">Proof</a></li>
<li><a href="#practical-considerations-of-k-nearest-neighbor">Practical Considerations of k-nearest neighbor</a>
<ul>
<li><a href="#finding-the-k-th-nearest-neighbors-takes-time">Finding the k-th nearest neighbors takes time</a></li>
</ul>
</li>
<li><a href="#metric-of-closeness-is-sometimes-unclear">Metric of Closeness is Sometimes Unclear</a>
<ul>
<li><a href="#derivative-wrt-matrix">Derivative w.r.t. Matrix</a></li>
</ul>
</li>
<li><a href="#not-saving-all-the-training-data">Not Saving All the Training Data</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#topic-3">Topic 3</a>
<ul>
<li><a href="#overfitting-the-training-data">Overfitting the Training Data</a></li>
<li><a href="#decision-boundaries">Decision Boundaries</a>
<ul>
<li><a href="#linear-classifier">Linear Classifier</a>
<ul>
<li><a href="#finding-the-weights">Finding The Weights</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#perceptron">Perceptron</a>
<ul>
<li><a href="#perceptron-mistake-bound">Perceptron Mistake Bound</a>
<ul>
<li><a href="#proof-1">Proof</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#nonlinear-regression">Nonlinear Regression</a>
<ul>
<li><a href="#quadratic-boundaries">Quadratic Boundaries</a>
<ul>
<li><a href="#proof-that-all-data-is-linear-separable-in-some-space">Proof that All Data is Linear Separable in some Space</a></li>
<li><a href="#kernel-trick-to-deal-with-computation">Kernel Trick (to Deal with Computation)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#topic-4">Topic 4</a></li>
<li><a href="#topic-5">Topic 5</a></li>
<li><a href="#topic-6">Topic 6</a></li>
<li><a href="#topic-7">Topic 7</a></li>
<li><a href="#topic-8">Topic 8</a></li>
<li><a href="#topic-9">Topic 9</a></li>
</ul>
<h2 id="administrative-stuff">Administrative Stuff<a hidden class="anchor" aria-hidden="true" href="#administrative-stuff">#</a></h2>
<ul>
<li>All homework is submitted via Gradescope.
<ul>
<li>Written parts must be PDFs and be typeset.
<ul>
<li>Plots and data analysis from programming part must be included in written part</li>
</ul>
</li>
<li>Programming parts are mostly turned in as a separate assignment on Gradescope</li>
</ul>
</li>
<li>40 % HW, 30% Midterm, 30% Final</li>
</ul>
<h2 id="topic-1">Topic 1<a hidden class="anchor" aria-hidden="true" href="#topic-1">#</a></h2>
<ul>
<li>Inputs encoded as vectors ($x \in X$):
<ul>
<li>$\vec{x} = &lt;x_{1},x_{2},&hellip;x_{n}&gt;$</li>
<li>Can think of each component as a measurement that you make</li>
</ul>
</li>
<li>Output is another vector ($y \in Y$):
<ul>
<li>$\vec{y} = &lt;y_{1},y_{2},&hellip;y_{m}&gt;$</li>
</ul>
</li>
<li>Goal is to figure out the function that maps x to y
<ul>
<li>You only have a limited number of samples from X
<ul>
<li>You need to make predictions from this limited sample size</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="workflow-of-classification-problems-supervised-learning">Workflow of Classification Problems (Supervised Learning)<a hidden class="anchor" aria-hidden="true" href="#workflow-of-classification-problems-supervised-learning">#</a></h3>
<ul>
<li>$(\vec{x_{1}},y_{1}),(\vec{x_{2}},y_{2})&hellip; \in X \times Y$
<ul>
<li>Example: $X = \mathbb{R^{d}}$ and $Y = {0,1}$ for a binary classification from a image with d pixels</li>
<li>Assumption: there exists a &ldquo;relatively simple&rdquo; function $f^{*}: X \rightarrow Y$ such that $f^{*}(\vec{x_{i}}) = y_{i}$ for most i
<ul>
<li>$*$ in ML implies some optimum value (in this case, an optimal function)</li>
<li><strong>Most</strong> includes that possibility that there is some fundamental noise in your data</li>
<li>Say someone wrote a number that kind of looks like 7 and kinda looks like 1. For the same image, the true $y_{i}$ could be either 7 or 1  (depending on the person&rsquo;s intentions). Hence, to force $f^{*}$ be a correct for all inputs misses this uncertainty in the true output</li>
</ul>
</li>
<li>Learning task: given $n$ examples from the data, from $\hat{f} \approx f^{*}$
<ul>
<li>$\hat{f}$ denotes a function that tries its best to be optimal</li>
</ul>
</li>
<li>Goal: $\hat{f}$ gives mostly correct prediction on unseen examples</li>
</ul>
</li>
</ul>
<h3 id="statistical-approach">Statistical Approach<a hidden class="anchor" aria-hidden="true" href="#statistical-approach">#</a></h3>
<ul>
<li>$(\vec{x_{1}},y_{1}),(\vec{x_{2}},y_{2})&hellip;(\vec{x_{n}},y_{n})$ samples are drawn from the underlying distribution
<ul>
<li>For simplicity, we assume each sample is drawn independently from the <strong>same</strong> underlying distribution is (called i.i.d. assumption)</li>
</ul>
</li>
<li>The learning algorithm draws $\hat{f}$ from a pool of models $\mathbb{F}$ that maximizes the label agreement with the training data
<ul>
<li>How do we select $f \in \mathbb{F}$?</li>
<li>Maximum likelihood (best fits the data)</li>
<li>Maximum a posteriori (best fits the data but incorporates prior assumptions)</li>
<li>Optimization of &lsquo;loss&rsquo; criterion (best discriminates the labels)</li>
</ul>
</li>
</ul>
<h4 id="classifier">Classifier<a hidden class="anchor" aria-hidden="true" href="#classifier">#</a></h4>
<ul>
<li>We are given a joint input/output space ($X \times Y$)
<ul>
<li>The data is distributed like $D(X \times Y)$
<ul>
<li>You have a region where your inputs can reside in ($X \times Y$) and D tells you the density of inputs on the space</li>
<li>You can visualize this space by either:
<ul>
<li>Taking slices at a fixed variable value ($P[Y,X=x]$) or the conditional distribution</li>
<li>Adding up all the slices along a variable ($P[Y]$) or the marginal distribution</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>A classifier is a measurable function of the type $f: x \in Y$
<ul>
<li>Ex1 (Constant): $f_{1}(\vec{x}) = y$ for some fixed $y \in Y$.</li>
<li>Ex2(Threshold): $f_{2}(x) = \begin{cases} 0\ if \ x \geq 5 \\ 1 \ otherwise  \end{cases}$</li>
<li>Ex3 (Majority class): $f_{3}(x) = arg\ max_{y\in Y} P[Y=y]$
<ul>
<li>This asks: What argument in Y yields the maximum probability?</li>
<li>Implicitly, this probability is the marginalized distribution on X</li>
<li>More explicitly, you can write probability as:</li>
<li>$P[Y=y] = P_{y \approx D}|_{Y}[Y=y]$
<ul>
<li>In words, you draw a (x,y) pair from D, and you only pay attention to the y part</li>
</ul>
</li>
</ul>
</li>
<li>None of these are particularly good/generally applicable classifiers</li>
<li>Ex4 (Sample Average Class):
<ul>
<li>$\frac{1}{n}\Sigma \mathbb{I}[f(x_{i}) = y_{i}]$
<ul>
<li>$\mathbb{I}$ is the indicator function that maps a boolean expression (like $f(x_{i}) = y_{i}$) to either 0 (false) or 1 (true)</li>
<li>This is from a sample</li>
</ul>
</li>
</ul>
</li>
<li>Ex5 (Population Average):
<ul>
<li>$\mathbb{E}_{(x,y)\in D}[\mathbb{I}[f(x)=y]]= P[f(x)=y]$
<ul>
<li>This is on the entire population. Essentially a probability that states how often this classifier f gets the output correct</li>
<li>called the <strong>accuracy function</strong> acc(f)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Let&rsquo;s look at the following classifier:
<ul>
<li>$f(\vec{x})= arg\ max_{y\in Y}\ P[y=Y|X=\vec{x}]$
<ul>
<li>In words, find y that maximizes the probability given a particular x (called <strong>Bayes</strong> classifier)</li>
<li>This is the best classifier. But most of the time, you can&rsquo;t actually compute $P[y=Y|X=\vec{x}]$ for all inputs. You try your best to estimate these probabilities</li>
<li>Proof:
<ul>
<li>Assume binary classification for output space (for simplicity)
<ul>
<li>Let $f(\vec{x})= arg\ max_{y\in Y}\ P[y=Y|X=\vec{x}]$. Let $g(\vec{x})\rightarrow \{0,1\}$. Then $P_{(\vec{x},y)}[g(\vec{x})=y] \leq P_{(\vec{x},y)}[f(\vec{x})=y]$</li>
</ul>
</li>
<li>Let&rsquo;s look at a single example (Fix $\vec{x}\in X$). Then for any classifier h
<ul>
<li>$P[h(\vec{x})=y|X=\vec{x}]=P[h(\vec{x})=1|X=\vec{x}]+P[h(\vec{x})=0|X=\vec{x}]$</li>
<li>Since $h(\vec{x})=1$ is a constant, we can use $\mathbb{I}$ to rewrite above
<ul>
<li>$\mathbb{I}[h(\vec{x})=1]P[Y=1|X=\vec{x}]+\mathbb{I}[h(\vec{x})=0]P[Y=0|X=\vec{x}] =\mathbb{I}[h(\vec{x})=1]\eta(\vec{x})+\mathbb{I}[h(\vec{x})=0](1- \eta(\vec{x}))$
<ul>
<li>Where $\eta(\vec{x}) := P[Y=1|X=\vec{x}]$</li>
</ul>
</li>
</ul>
</li>
<li>Hence we need to show that $P[f(\vec{x})=y|X=\vec{x})]-P[g(\vec{x})=y|X=\vec{x})] \geq 0$
<ul>
<li>This equals $\eta(\vec{x})[\mathbb{I}[f(\vec{x})=1]-\mathbb{I}[g(\vec{x})=1]]+(1-\eta(\vec{x}))[\mathbb{I}[f(\vec{x})=0]-\mathbb{I}[g(\vec{x})=0]] = (2\eta(\vec{x})-1)[\mathbb{I}[f(\vec{x})=1]-\mathbb{I}[g(\vec{x})=1]]$
<ul>
<li>You use $\mathbb{I}[f(\vec{x})=0] = 1 -\mathbb{I}[f(\vec{x})=1]$</li>
</ul>
</li>
<li>If both classifiers guess the same output, there is no degradation in the performance in either classifier</li>
<li>What happens if the outputs are different?
<ul>
<li>In either case$[\mathbb{I}[f(\vec{x})=1]-\mathbb{I}[g(\vec{x})=1]]=\pm 1$ depending on if $f(x)$ is correct or $g(x)$ is correct,</li>
<li>Say $f(x)$ returns 1:
<ul>
<li>This means that probability of $f(x)$ selecting 1 is greater than 0.5, hence $P[Y=1|X-\vec{x}] =\eta(\vec{x}) \geq 0.5$ so $(2\eta(\vec{x})-1)[\mathbb{I}[f(\vec{x})=1]-\mathbb{I}[g(\vec{x})=1]] \geq 0$</li>
</ul>
</li>
<li>A similar argument holds for $f(x)$ returning 0 (ie. the bayes classifier fails and the alternate classifier works)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Since we know that this works for a fixed example, we can integrate across all $x\in X$ to prove the original theorem $P_{(\vec{x},y)}[g(\vec{x})=y] \leq P_{(\vec{x},y)}[f(\vec{x})=y]$
<ul>
<li>Want to show $\mathbb{E}[\mathbb{I}[P_{(\vec{x},y)}[g(\vec{x})=y]]] - \mathbb{E}[\mathbb{I}[P_{(\vec{x},y)}[f(\vec{x})=y]]] \geq 0$
<ul>
<li>$\mathbb{E}[\alpha(\vec{x},y)] = \int_{(\vec({x},y)\in X\times Y)} \alpha(\vec{x},y)P((\vec{x},y))d((\vec{x},y))$</li>
<li>So we can rewrite the expectation values using linearity: $\alpha(x,y)=\mathbb{I}[P_{(\vec{x},y)}[f(\vec{x})=y]]$ and $\beta(x,y)=\mathbb{I}[P_{(\vec{x},y)}[g(\vec{x})=y]]$:
<ul>
<li>$\mathbb{E}[\mathbb{I}[P_{(\vec{x},y)}[g(\vec{x})=y]]] - \mathbb{E}[\mathbb{I}[P_{(\vec{x},y)}[f(\vec{x})=y]]] = \int (\alpha(\vec{x},y)-\beta(\vec{x},y)) p(\vec{x},y)d(x,y)=\int_{x}[\int_{y} (\alpha-\beta) p(y|x)dy]dx$
<ul>
<li>The inner integral is the probability we calculated earlier (the one that was always greater than zero). Hence the double integral is always greater than or equal to 0</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Since we can&rsquo;t normally acheive this classifier, how can we estimate it?
<ul>
<li>$f(\vec{x})= argmax_{y\in Y} P[Y=y|X=\vec{x}]= argmax_{y\in Y}\frac{P[X=\vec{x}|Y=y]\cdot P[Y=y]}{P[X=\vec{x}]}$
<ul>
<li>denominator independent of Y. This means that
<ul>
<li>$argmax_{y\in Y}\frac{P[X=\vec{x}|Y=y]\cdot P[Y=y]}{P[X=\vec{x}]} = argmax_{y\in Y}P[X=\vec{x}|Y=y]\cdot P[Y=y]$
<ul>
<li>The value of the function on the LHS is not the same as the RHS, but the y that maximizes the LHS and RHS is the same</li>
<li>Instead of figuring out an infinite number of estimates (1 for each $\vec{x}$ in $\R^{d}$ space) you instead make 1 estimate for each $y\in Y$, which is a much smaller space
<ul>
<li>Instead of an infinite number of Bernoulli distributions, you have a finite number of multidimensional distributions</li>
</ul>
</li>
<li>$P[Y=y]$ is called the <strong>Class Prior</strong> or the <strong>prior</strong></li>
<li>$P[X=\vec{x}|Y=y]$ is called the <strong>Class Conditional</strong> or the <strong>Conditional</strong></li>
<li>$P[Y=y|X=\vec{x}]$ is called the <strong>Class Posterior</strong> or the <strong>Posterior</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation<a hidden class="anchor" aria-hidden="true" href="#maximum-likelihood-estimation">#</a></h3>
<ul>
<li>We are given some data $\vec{x_{1}}&hellip; \vec{x_{n}} \in X$ independently draw</li>
<li>Say we have a model class $P = \{p_{\theta}|\theta\in\Theta\}$ where $\Theta$ is the parameter space
<ul>
<li>For Gaussians, $&lt;\mu,\sigma&gt;\in \Theta$ and $p_{\theta} = \frac{1}{\sqrt{2\pi\sigma^{2}}}exp(\frac{-1}{2}\frac{(x-\mu)^{2}}{\sigma^{2}})$</li>
</ul>
</li>
<li>Find the model that best fits the data</li>
<li>Define the likelihood function $\mathbb{L}(\theta|X)= P(X|\theta)=P(x_{1},&hellip;,x_{n}|\theta)=\Pi_{i=1}^{n}P(\vec{x_{i}}|\theta) = \Pi_{i=1}^{n}p_{\theta}(\vec{x_{i}})$
<ul>
<li>The likelihood function defines how <strong>probable</strong> (or how likely) is the data given the model $p_{\theta}$</li>
</ul>
</li>
<li>we want to find $\theta$ that maximizes the likelihood function: $argmax_{\theta\in\Theta}\mathbb{L} = argmax_{\theta\in\Theta}\Pi_{i=1}^{n}p_{\theta}(\vec{x_{i}})$</li>
<li>Say for instance, that we have some data and we want to fit a Gaussian to it:
<ul>
<li>Find $argmax_{\mu,\sigma^{2}}\Pi_{i=1}^{n}p_{\mu,\sigma^{2}}(\vec{x_{i}})$</li>
<li>A trick to avoid using the product rule too much:
<ul>
<li>$argmax_{\theta}\mathbb{L}(\theta|X)= argmax_{\theta} log(\mathbb{L}(\theta|X))$</li>
<li>This turns the product into sums and eliminates the exp of the Guassian. After doing this manipulation, we get:</li>
<li>find $argmax_{\mu,\sigma^{2}} \Sigma_{i=1}^{n}[\frac{-1}{2}log(2\pi\sigma^{2})-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}]$</li>
<li>Maximize w.r.t $\mu$ to and solve for the stationary points yield $\mu_{ML} = \frac{1}{n}\Sigma_{i=1}^{n}x_{i}$
<ul>
<li>This is an unbiased estimator that converges to the true $\mu$ as the number of samples go to infinity</li>
</ul>
</li>
<li>Can also take w.r.t. $\sigma^{2}$: $\sigma_{ML}^{2}=\frac{1}{n}\Sigma_{i=1}^{n}(x_{i}-\mu)^{2}$
<ul>
<li>We can find the sample variance by plugging in $\mu_{ML}$ for the true $\mu$ (This is where the $\frac{1}{n-1}$ thing comes from)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="example">Example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h4>
<ul>
<li>Say that we want to estimate biological gender (male or female) based on weight and height</li>
<li>The distribution is spread like $D[X\times Y]$</li>
<li>The Bayes classifier is thus:
<ul>
<li>$f(\vec{x}) = argmax_{y\in\{male,female\}} P[X=\vec{x}|Y=y]\cdot P[Y=y]$
<ul>
<li>$\hat{P}[Y=male]=$ fraction of training data labelled as male</li>
<li>$\hat{P}[Y=female]=$ fraction of training data labelled as female</li>
<li>$\hat{P}[X|Y=male]=p_{\theta(male)}(X)$
<ul>
<li>Use MLE with only male data</li>
</ul>
</li>
<li>$\hat{P}[X|Y=female]=p_{\theta(female)}(X)$
<ul>
<li>Use MLE with only female data</li>
</ul>
</li>
<li>Say that the conditional follows a 2D Gaussian distribution</li>
</ul>
</li>
</ul>
</li>
<li>Another way to put this:
<ul>
<li>We have a bunch of labelled points in $\mathbb{R^{2}}$
<ul>
<li>Look only at the male labels. Fit a 2D Gaussian to this data (Find some mean center and the covariance matrix)</li>
<li>Do the same with the female labels</li>
</ul>
</li>
</ul>
</li>
<li>Once you have gotten your distributions, say that you get a new data point. The distribution (male or female) that yields the higher probability is your Bayes classifier&rsquo;s output</li>
</ul>
<h3 id="convergence">Convergence<a hidden class="anchor" aria-hidden="true" href="#convergence">#</a></h3>
<ul>
<li>How fast does our sample converges to the population?
<ul>
<li>Define an error function $E = \int(P(x)-\hat{P}(x))^{2}dx \leq n^{\frac{-2}{2+d}} \approx n^{\frac{-1}{d}}$
<ul>
<li>If the dimensionality of the problem gets larger (d goes up), then the convergence becomes much slower (you need many many more samples in a higher dimensional space to get the same threshold as a lower dimensional space)
<ul>
<li>Say that you have a tolerance of $\epsilon$ on the error. Then you need $n \geq \epsilon^{d}$ samples</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="naive-bayes-classifier">Naive Bayes Classifier<a hidden class="anchor" aria-hidden="true" href="#naive-bayes-classifier">#</a></h3>
<ul>
<li>$f(\vec{x}) = argmax_{y\in Y} P[X=\vec{x}|Y=y]\cdot p[Y=y] = argmax_{y\in Y}\Sigma_{y=1}^{d}P[X^{j}=x^{j}|Y=y]\cdot P[Y=y]$
<ul>
<li>In words: each feature of the input (the $x_{i}$) are independent from each other given marginalization on the output</li>
<li>You break a d-dimensional problem into d 1-dimensional problem, where each 1-d problem requires $\frac{1}{\sqrt{n}}$</li>
</ul>
</li>
</ul>
<h3 id="how-do-you-quantify-the-quality-of-a-classifier">How do you quantify the quality of a classifier?<a hidden class="anchor" aria-hidden="true" href="#how-do-you-quantify-the-quality-of-a-classifier">#</a></h3>
<ul>
<li>If you know the population distribution, then just run the classifier with that data</li>
<li>If you have a sample distribution, then you have a biased qualifier since you chose your classifying function from the training data</li>
<li>Split the training data into training and testing data
<ul>
<li>The testing data provides an unbiased means of testing your model</li>
</ul>
</li>
</ul>
<h3 id="approaches-to-classification">Approaches to Classification<a hidden class="anchor" aria-hidden="true" href="#approaches-to-classification">#</a></h3>
<h4 id="generative-approach">Generative Approach<a hidden class="anchor" aria-hidden="true" href="#generative-approach">#</a></h4>
<ul>
<li>You assume a model of the underlying distribution. When you get a new point, you find which model is the best</li>
</ul>
<h5 id="advantages">Advantages<a hidden class="anchor" aria-hidden="true" href="#advantages">#</a></h5>
<ul>
<li>The model gives you an interpretation of how data gets generated from a population
<ul>
<li>Since you have a interpretation of the underlying  distribution, you can generate simulated data</li>
</ul>
</li>
</ul>
<h5 id="disadvantages">Disadvantages<a hidden class="anchor" aria-hidden="true" href="#disadvantages">#</a></h5>
<ul>
<li>You need to pick a model</li>
<li>This is overkill if you just need a classification result, which costs time and money</li>
<li>You also are more prone to errors since the sole focus is not classification</li>
</ul>
<h4 id="discriminative-approach">Discriminative Approach<a hidden class="anchor" aria-hidden="true" href="#discriminative-approach">#</a></h4>
<ul>
<li>you don&rsquo;t assume an underlying model. Given a new point, you directly figure out the classification</li>
</ul>
<h5 id="advantages-1">Advantages<a hidden class="anchor" aria-hidden="true" href="#advantages-1">#</a></h5>
<ul>
<li>Typically you get better classification accuracy</li>
</ul>
<h5 id="disadvantages-1">Disadvantages<a hidden class="anchor" aria-hidden="true" href="#disadvantages-1">#</a></h5>
<ul>
<li>You gain no understanding of the population</li>
</ul>
<h2 id="topic-2">Topic 2<a hidden class="anchor" aria-hidden="true" href="#topic-2">#</a></h2>
<h3 id="nearest-k-neighbor">Nearest K-Neighbor<a hidden class="anchor" aria-hidden="true" href="#nearest-k-neighbor">#</a></h3>
<h4 id="importance-of-closeness-for-k-nearest-neighbors">Importance of Closeness for k-Nearest Neighbors<a hidden class="anchor" aria-hidden="true" href="#importance-of-closeness-for-k-nearest-neighbors">#</a></h4>
<ul>
<li>If you have improper units (say that you measure human height in lightyears and human weight in micrograms), this can stretch and compress the axes and exaggerates the importance of a particular feature</li>
<li>Choosing the correct closeness metric allows to to scale the axes to avoid this problem</li>
</ul>
<h5 id="distances">Distances<a hidden class="anchor" aria-hidden="true" href="#distances">#</a></h5>
<ul>
<li>If $X\in \R^{d}$, then an example of a distance metric is the $L^{p}$ norm ($(\Sigma_{i=1}^{d} (\vec{x_{i}}-\vec{\hat{x_{j}}})^{p})^\frac{1}{p}$):
<ul>
<li>$p =2$ is Euclidean distance</li>
<li>$p =1$ Manhattan distance or cityblock distance</li>
<li>$p=\infty$ max distance</li>
<li>$p=0$ count &lsquo;non-zero&rsquo; distance (ignore $\frac{1}{p}$ power in norm definition)</li>
</ul>
</li>
<li>Edit distance (genomics):
<ul>
<li>Domain specific: the number of mutations needed to convert one sequence to another</li>
</ul>
</li>
<li>Kendell-Tau distance (to compare rankings)
<ul>
<li>What is the minimum number of adjacent pairwise swaps needed to convert 1 ranking to another?</li>
</ul>
</li>
</ul>
<h5 id="similarities">Similarities<a hidden class="anchor" aria-hidden="true" href="#similarities">#</a></h5>
<ul>
<li>take the distance and construct the function $\frac{1}{1+||\vec{x_{1}}-\vec{x_{2}}||_{p}}$</li>
<li>Cosine similarity: Find the cosine between the two vector</li>
</ul>
<h4 id="issues-with-the-nearest-neighbor">Issues with the nearest neighbor<a hidden class="anchor" aria-hidden="true" href="#issues-with-the-nearest-neighbor">#</a></h4>
<ul>
<li>Sensitive to noise in data, so labelling is unstable
<ul>
<li>Solution: look at the kth nearest labels to make this stable</li>
</ul>
</li>
</ul>
<h4 id="k-nn-optimality">k-NN Optimality<a hidden class="anchor" aria-hidden="true" href="#k-nn-optimality">#</a></h4>
<ul>
<li>Stone&rsquo;s Theorem:
<ul>
<li>Theorem 1: for a fixed k, as $n \rightarrow \infty$, k-NN classification error converges to no more than twice Bayes classifier error</li>
<li>Theorem 2: if $k\rightarrow\infty$, $n\rightarrow \infty$, but $\frac{k}{n}\rightarrow0$ (the rate of growth goes to zero ie. n grows faster than k), then the k-NN classifier converges to Bayes classifier</li>
</ul>
</li>
<li>Why $\frac{k}{n}$?</li>
<li>Say that n is fixed an k increases. Eventually, you are averaging over the entire dataset. Hence, if you increase n as well at a rate faster than k, then you are still sampling a small local region of the dataset</li>
</ul>
<h5 id="proof">Proof<a hidden class="anchor" aria-hidden="true" href="#proof">#</a></h5>
<ul>
<li>Let k=1</li>
<li>Notation:
<ul>
<li>P[e] = NN error rate = $1-P_{x,y\in D_{n}}[[f(x)_{n}=y]]$
<ul>
<li>ie. error  = 1-accuracy</li>
</ul>
</li>
<li>$D_{n} = &lt;X_{n},Y_{n}&gt;$ = labeled data (size n)
<ul>
<li>The sample data that you have drawn from the underlying distribution</li>
</ul>
</li>
<li>$x_{n}$ = nearest neighbor of $x_{t}$ in $D_{n}$</li>
<li>$y_{n}$ = label of nearest neighbor</li>
<li>$x_{t}$ and $y_{t}$ are a fixed test point of $x_{t}$</li>
</ul>
</li>
<li>What is the error at a fixed test point $x_{t}$?
<ul>
<li>$lim_{n\rightarrow\infty} P_{y_{t},D_{n}}[e|x_{t}]$
<ul>
<li>There is some randomness in $y_{t}$ and $D_{n}$</li>
</ul>
</li>
</ul>
</li>
<li>Recall that $P[A] = \Sigma_{b\in B} P[A,B=b] = \Sigma_{b\in B} P[A|B=b]P[B=b]$</li>
<li>If you condition on C, you get:
<ul>
<li>$P[A|C] = \Sigma_{b\in B} P[A,B=b|C] = \Sigma_{b\in B} P[A|B=b,C]P[B=b|C]$</li>
</ul>
</li>
<li>Using the appropriate substitutions, can write:</li>
<li>$lim_{n\rightarrow\infty}  P_{y_{t},D_{n}}[e|x_{t}] = \int_{n\rightarrow\infty} P_{yt}[e|x_{t}X_{n}]P[X_{n}|x_{t}]dX_{n} = lim_{n\rightarrow\infty}\int P_{yt}[e|x_{t}n_{n}]P[n_{n}|x_{t}] $
<ul>
<li>In words, the second equality states that you fix your dataset</li>
</ul>
</li>
<li>You then write out what nearest neighbor means:
<ul>
<li>$lim_{n\rightarrow\infty}P_{y_{t},D_{n}}[e|x_{t}] = lim_{n\infty}[1-\Sigma_{y\in Y} P(y_{t}=y,y_{n}=y|x_{t},x_{n})]P[x_{n}x_{t}] dx =  lim_{n\infty}[1-\Sigma_{y\in Y} P(y_{t}=y|x_{t})P(y_{n}=y|x_{t})]P[x_{n}|x_{t}] dx$
<ul>
<li>The first is the definition of k-NN (the output of the nearest neighbor is the same as the test sample)</li>
<li>The second equality uses independence (ie. truth level labels of the test point and neighbor points are conditionally independent)</li>
</ul>
</li>
</ul>
</li>
<li>You take the limit w.r.t. $n\rightarrow\infty$. $P[y_{t}=y|x_{t}]=P[y_{n}=y|x_{n}]$. When you integrate $P[x_{n}|x_{t}] dx$ you get 1, hence
<ul>
<li>$lim_{n\rightarrow\infty}P_{y_{t},D_{n}}[e|x_{t}] = 1-\Sigma_{y\in Y}P^{2}(y_{t}=y|x_{t})$</li>
</ul>
</li>
<li>If Bayes classifier return $y*$ at point $x_{t}$, then
<ul>
<li>$1-\Sigma P^{2}(y_{t}=y|x_{t}) \leq 1- P^{2}(y_{t}=y*|x_{t})$ or $\leq 2(1-P(y_{t}-y*|x_{t})) = 2P^{*}[e|x_{t}]$</li>
</ul>
</li>
</ul>
<h5 id="practical-considerations-of-k-nearest-neighbor">Practical Considerations of k-nearest neighbor<a hidden class="anchor" aria-hidden="true" href="#practical-considerations-of-k-nearest-neighbor">#</a></h5>
<h6 id="finding-the-k-th-nearest-neighbors-takes-time">Finding the k-th nearest neighbors takes time<a hidden class="anchor" aria-hidden="true" href="#finding-the-k-th-nearest-neighbors-takes-time">#</a></h6>
<ul>
<li>$O(nd)$ where n is the number of samples and d is the dimensionality of each sample</li>
<li>Simplify to $\R$. Then you can sort your data in a binary search tree
<ul>
<li>Look up in $O(log\ n)$</li>
<li>Preprocessing is $O(n\ log\ n)$</li>
</ul>
</li>
<li>Instead of thinking in terms of midpoints, think in terms of regions</li>
<li>After you divide up $\R$, you can put your data sample into one of the bins
<ul>
<li>Problem: the hard threshold can misclassify the nearest neighbor. Instead, it gives you a near neighbor (in practice ,this distinction doesn&rsquo;t really matter if you have enough data)
<ul>
<li>Solution: Soften the boundary, where if your data fall in the overlap region, then you traverse both branches of the tree</li>
</ul>
</li>
</ul>
</li>
<li>To extend to higher dimensions, take the projection of your data onto one of your dimensions, then do the median. Recursively repeat this process, select which coordinate you project onto each time
<ul>
<li>You can use the same coordinate, although if you overuse a particular coordinate, than your fills (bins) can be elongated along 1 direction
<ul>
<li>You can mitigate this problem by selecting your coordinate based on the one that has the smaller variance</li>
</ul>
</li>
</ul>
</li>
<li>Problem/Caveat: What if a different direction is better to subdivide the data?</li>
<li>Perform PCA, which calculates the direction of maximum variance, and then project onto this direction
<ul>
<li>Cons: need to do an eigenvalue decomposition, which has a big online overhead</li>
</ul>
</li>
<li>This data structure is called a k-d tree
<ul>
<li>Used in databases (boolean selections select regions of the space to reduce the search problem</li>
</ul>
</li>
<li>This problem can also be solved by a locality-sensitive hash map</li>
</ul>
<h5 id="metric-of-closeness-is-sometimes-unclear">Metric of Closeness is Sometimes Unclear<a hidden class="anchor" aria-hidden="true" href="#metric-of-closeness-is-sometimes-unclear">#</a></h5>
<ul>
<li>Say that you have a particular task and you gather data with some sort of features
<ul>
<li>Some features are not relevant for the task at hand (ie. these features are noisy). However, for other tasks, these noisy features might be of great importance</li>
<li>You could also have highly correlated features that can distort the Euclidean distance</li>
</ul>
</li>
<li>Solution: we could re-weight the contribution of each feature:
<ul>
<li>$\rho(\vec{x_{1}},\vec{x_{2}};\vec{w}) = [(\vec{x_{1}}-\vec{x_{2}})^{T}W(\vec{x_{1}}-\vec{x_{2}})]^{\frac{1}{2}}$ where $W$ is a diagonal matrix</li>
<li>How do we learn what $W$ is?
<ul>
<li>Create two sets, one similar and one dissimilar
<ul>
<li>$S = [(\vec{x_{i}},\vec{x_{j}})|y_{i}=y_{j}]$</li>
<li>$D = [(\vec{x_{i}},\vec{x_{j}})|y_{i}\neq y_{j}]$</li>
</ul>
</li>
<li>Define a cost function $\Psi(\vec{w}) = \lambda\Sigma_{(\vec{x_{i}},\vec{x_{j}})\in S} \rho(\vec{x_{1}},\vec{x_{2}};\vec{w})-(1-\lambda)\Sigma_{(\vec{x_{i}},\vec{x_{j}})\in D} \rho(\vec{x_{1}},\vec{x_{2}};\vec{w})$</li>
<li>$\lambda$ controls which force dominates</li>
<li>In words, we want to find the weights that minimize the distance between similar points and maximize the distance between dissimilar points</li>
<li>We can simplify this by squaring $\rho$ in our cost function to avoid  annoying square root functions</li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="derivative-wrt-matrix">Derivative w.r.t. Matrix<a hidden class="anchor" aria-hidden="true" href="#derivative-wrt-matrix">#</a></h6>
<ul>
<li>What is $\frac{d}{dW}(\lambda\Sigma_{i,j}(x_{i}-x_{j})^{T}W(x_{i}-x_{j}))$ where W is a matrix</li>
<li>We can think of the function as a map from $\R^{d\times d}\rightarrow \R$
<ul>
<li>The derivative must have the same dimension as the domain (ie $R^{d\times d}$)</li>
</ul>
</li>
<li>Hence, we can intuit that the derivative is $\Sigma_{i,j}(x_{i}-x_{j})(x_{i}-x_{j})^{T}$ (this is correct)</li>
</ul>
<h5 id="not-saving-all-the-training-data">Not Saving All the Training Data<a hidden class="anchor" aria-hidden="true" href="#not-saving-all-the-training-data">#</a></h5>
<ul>
<li>Instead of saving all the training data, you save the classification of the region where the data lies
<ul>
<li>You also try to make the classification regions as large as possible. Namely, you decide your selection cuts to maximally reduce label uncertainty (ie. as you decide your cut to maximize the purity of each region at each step (up to a certain point to avoid overfitting))</li>
<li>You can quantify label uncertainty as follows
<ul>
<li>Classification Error:
<ul>
<li>$u(C) = 1- max_{y}p_{y}$</li>
<li>$p_{y}$ is the fraction of training data labelled as y in cell C</li>
</ul>
</li>
<li>Entropy:
<ul>
<li>$u(C) = \Sigma_{(y\in Y)} p_{y} \log \frac{1}{p_{y}}$</li>
<li>Units of bits</li>
<li>This is linearly additive for independent events</li>
</ul>
</li>
<li>Gini index:
<ul>
<li>a measure of how &ldquo;mixed&rdquo; a population is</li>
<li>$u(C) = \Sigma_{(y\in Y)}p_{y}^{2}$</li>
<li>For homogeneity, we take $1-u(C)$</li>
</ul>
</li>
</ul>
</li>
<li>Whatever metric you use, you want to
<ul>
<li>$argmax_{F,T}[u(C)-(p_{L}u(C_{L})+p_{R}u(C_{R}))]$</li>
<li>Namely, choose a feature F and threshold T that maximizes the fraction of the same labels in a region</li>
<li>You keep your features and thresholds parallel to axes to maintain interpretability</li>
<li>This is a greedy algorithm</li>
<li>Finding the optimal decision tree is NP-hard</li>
<li>Uncertainty estimates are unstable</li>
<li>Tree complexity is dependent of data geometry</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="topic-3">Topic 3<a hidden class="anchor" aria-hidden="true" href="#topic-3">#</a></h2>
<h3 id="overfitting-the-training-data">Overfitting the Training Data<a hidden class="anchor" aria-hidden="true" href="#overfitting-the-training-data">#</a></h3>
<ul>
<li>As you increase the complexity of a model, you can drive the training error down to zero in the limit
<ul>
<li>This does not mean that this generalizes to the population</li>
<li>You can take a cut of your training data (call this your validation data) to tune how complicated you make your model to avoid this overfitting</li>
</ul>
</li>
</ul>
<h3 id="decision-boundaries">Decision Boundaries<a hidden class="anchor" aria-hidden="true" href="#decision-boundaries">#</a></h3>
<h4 id="linear-classifier">Linear Classifier<a hidden class="anchor" aria-hidden="true" href="#linear-classifier">#</a></h4>
<ul>
<li>A linear Classifier is defined as a classifier whose decision boundary is a line</li>
<li>For simplicity, say that we have a binary classification of -1,1.</li>
<li>Recall definition of linearity: $g(a+b) = g(a)+g(b)$ and $g(ca) = cg(a)$
<ul>
<li>you can drop $g(ca) = cg(a)$ assumption and examine affine decision boundaries (As an abuse of notation, people call these affine decision boundaries linear boundaries)</li>
</ul>
</li>
<li>Let g be a decision boundary such that:
<ul>
<li>In 1D, we get $g(x) = w_{1}x+w_{0} = 0$ where x is from your data that satisfied these conditions</li>
<li>In general $g(\vec{x}) = \vec{w}\cdot\vec{x}+w_{0}=0$</li>
</ul>
</li>
<li>Define a linear classifier such that $f(x) = 1$ if $g(x) \geq 0$ and $f(x) = -1$ if $g(x) \leq 0$ or $f(\vec(x) = sign(g(\vec{x}))$</li>
<li>Let $g(x) = (\vec{w},w_{0})\cdot(\vec{x},1)$ where you call the extra 1 the bias. This is called &ldquo;lifting&rdquo; the data because all of your data gets assigned a value of 1 in a new dimension. This extra degree of freedom allows an affine classifier to truely become a linear classifier</li>
</ul>
<h5 id="finding-the-weights">Finding The Weights<a hidden class="anchor" aria-hidden="true" href="#finding-the-weights">#</a></h5>
<ul>
<li>We want to minimize the training error:
<ul>
<li>$argmin_{\vec{w}} = \frac{1}{n}\Sigma_{i}^{N}\bold{1}[sign(\vec{w}\cdot\vec{x_i})\neq y_i]$</li>
<li>The above equals $argmax_{\vec{w}} \Sigma_{x_{i},y_{i}=1}\bold{1}[\vec{x_i}\cdot \vec{w} \le 0] + \Sigma_{x_{i},y_{i}=-1}\bold{1}[\vec{x_i}\cdot \vec{w} \ge 0]$
<ul>
<li>This is NP-hard to solve (NP-hard for the approximation as well)</li>
</ul>
</li>
<li>This is not an absolute statement: not every instance of your data is hard to solve
<ul>
<li>For data where it is NP-hard, you probably don&rsquo;t want to use a linear classifier in the first place</li>
<li>For now, we assume that we have linear separable data (ie. the problem is not NP-hard)</li>
</ul>
</li>
</ul>
</li>
<li>Say that our data has a linear dicision boundary that perfectly separates the training data
<ul>
<li>We can define the margin $\gamma$ as the shortest distance between the boundary and the closest data point</li>
</ul>
</li>
</ul>
<h3 id="perceptron">Perceptron<a hidden class="anchor" aria-hidden="true" href="#perceptron">#</a></h3>
<ul>
<li>Given: labelled training data
<ul>
<li>Initialize $\vec{w}^{(0)} = 0$</li>
<li>for t = 1,2,3&hellip;
<ul>
<li>If $\exist (\vec{x},y) \in S$ such that $sign(\vec{w}^{(t-1)}\cdot \vec{x}) \neq y$
<ul>
<li>Update $\vec{w}^{(t)} = \vec{w}^{(t-1)}+y\vec{x}$
<ul>
<li>Remember that $y=\pm 1$</li>
</ul>
</li>
</ul>
</li>
<li>terminate when there is no such training sample that you misclassify</li>
</ul>
</li>
</ul>
</li>
<li>This algorithm might not terminate because it is too greedy</li>
</ul>
<h4 id="perceptron-mistake-bound">Perceptron Mistake Bound<a hidden class="anchor" aria-hidden="true" href="#perceptron-mistake-bound">#</a></h4>
<ul>
<li>Assume that there is a unit length $\vec{w}^{*}$</li>
<li>Let $R = max_{\vec{x}\in S} |\vec{x}|$ be the radius and $\gamma$ be the margin</li>
<li>The number of iterations T is bounded by $|\frac{R}{\gamma}|^2$</li>
</ul>
<h5 id="proof-1">Proof<a hidden class="anchor" aria-hidden="true" href="#proof-1">#</a></h5>
<ul>
<li>How far is $\vec{w}$ from $\vec{w}^{*}$
<ul>
<li>We want $\vec{w}$ to converge to $\vec{w}^{*}$
<ul>
<li>We can use the dot product as a measure of this. We want to decrease the dot product by reducing the angle and not increasing the lengths</li>
</ul>
</li>
<li>Let&rsquo;s say that the algorithm made a mistake at time t. Then:
<ul>
<li>$\vec{w}^{(t)}\cdot \vec{w}^{*} = (\vec{w}^{(t-1)}+y\vec{x})\cdot \vec{w}^{*} \geq \vec{w}^{(t)}\cdot \vec{w}^{*}+\gamma$</li>
<li>$|\vec{w}^{t}|^2=|\vec{w}^{t-1}yx|^2 = |\vec{w}^{t-1}|^2+2y(\vec{w}^{t-1}\cdot x)+|y\vec{x}|^2 \leq |\vec{w}^{t-1}|^2+R^{2}$</li>
</ul>
</li>
<li>The above inequalities hold after every iteration. After iteration T:
<ul>
<li>$T\gamma \leq \vec{w}^{T}\cdot \vec{w}^{*} \leq |\vec{w}^{T}||\vec{w}^{*}| \leq R\sqrt{T}$</li>
<li>Rearrange to get that $T\leq (\frac{R}{\gamma})^{2}$</li>
</ul>
</li>
</ul>
</li>
<li>This means that the perceptron is an online algorithm: it only looks at a small subset of the data (1 at a time).</li>
</ul>
<h3 id="nonlinear-regression">Nonlinear Regression<a hidden class="anchor" aria-hidden="true" href="#nonlinear-regression">#</a></h3>
<ul>
<li>Say that we have some data that is not conducive to linear boundaries</li>
</ul>
<h4 id="quadratic-boundaries">Quadratic Boundaries<a hidden class="anchor" aria-hidden="true" href="#quadratic-boundaries">#</a></h4>
<ul>
<li>For a quadratic term, our quadratic boundary can be defined as $g(\vec{x}) = w_1x^1_2+w_2x^2_2+w_3x_1x_2+w_4x_1+w_5x_2+w_0$</li>
<li>Suppose that we only look at boundaries of the form $g(\vec{x}) = w_1x_1^2+w_2x_2^2+w_0 = \Sigma_{p+q\leq 2} w^{p,q}x_1^p x_2^q$</li>
<li>Make the change of coordinates $\chi_1 = x_1^{2}$ and $\chi_2 = x_2^{2}$ to make this boundary a linear boundary</li>
<li>In function notation: $\phi(x_1,x_2) \rightarrow (x_1^{2},x_2^{2})$
<ul>
<li>This is called a feature transformation</li>
</ul>
</li>
<li>For the general case  we have $\phi(x_1,x_2) \rightarrow (x_1^2,x_2^2,x_1x_2,x_1,x_2,1)$</li>
<li>For d dimensions, we have that
<ul>
<li>$g(\vec{x}) \Sigma_{i,j=1}^{d} \Sigma_{p+q\leq 2} w_{i,j}^{p,q} x_{i}^{p}x_{j}^{q}$</li>
</ul>
</li>
<li>Transforming the data into higher dimensions takes effort. We don&rsquo;t want to do this unless we know that the problem will become linear seperable</li>
</ul>
<h5 id="proof-that-all-data-is-linear-separable-in-some-space">Proof that All Data is Linear Separable in some Space<a hidden class="anchor" aria-hidden="true" href="#proof-that-all-data-is-linear-separable-in-some-space">#</a></h5>
<ul>
<li>No label information is given to use ($Y$ is unknown)</li>
<li>What we are given is n distinct points S = $\vec{x_1},\vec{x_2},&hellip; \vec{x_n}$</li>
<li>There exists a feature transformation (kernel) such that for any labelling of S is linearly separable in the transformed space</li>
<li>Consider the mapping into $\mathbb{R}^n$
<ul>
<li>$\phi(x_i) \rightarrow &lt;0,0,&hellip; ,1,0,0,&hellip;.0&gt;$ or in words, you map the ith data point to the ith component of the $\mathbb{R}^n$ vector</li>
</ul>
</li>
<li>Then, the decision boundary induced by linear weighting $\vec{w^{*}} =  &lt;y_1,y_2&hellip; y_n&gt;$ perfectly seperates the data because</li>
<li>$\forall \vec{x}_i \rightarrow sign(\phi(\vec{x}_i)\cdot \vec{w}^{*}) = sign(y_i) =  y_i$</li>
<li>This doesn&rsquo;t work in practice since we don&rsquo;t know a priori how large out data will be</li>
</ul>
<h5 id="kernel-trick-to-deal-with-computation">Kernel Trick (to Deal with Computation)<a hidden class="anchor" aria-hidden="true" href="#kernel-trick-to-deal-with-computation">#</a></h5>
<ul>
<li>Explicitly working with generic Kernel space takes time $\Omega(n)</li>
<li>Suppose that taking the dot product $\dot(\vec{x_1}\cdot \vec{x_2})$ is somehow easy to do
<ul>
<li>For example, the generic quadratic boundary
<ul>
<li>make the transformation like the standard transformation, but scale the cross terms up by $\sqrt$</li>
<li>This goes as $O(d^2)$</li>
<li>The dot product calculation is of $O(d)$: $(1+\vec{x_i}\vec{x_j})^2$</li>
<li>This also extend to polynomial transformations $\phi(\vec{x_i})\cdot \phi(\vec{x_i}) = (1+\vec{x_i}\vec{x_j})^2$ with is also linear in d</li>
</ul>
</li>
<li>If you have a machine learning algorithm that only works with dot products, then you can run it quickly</li>
</ul>
</li>
</ul>
<h2 id="topic-4">Topic 4<a hidden class="anchor" aria-hidden="true" href="#topic-4">#</a></h2>
<h2 id="topic-5">Topic 5<a hidden class="anchor" aria-hidden="true" href="#topic-5">#</a></h2>
<h2 id="topic-6">Topic 6<a hidden class="anchor" aria-hidden="true" href="#topic-6">#</a></h2>
<h2 id="topic-7">Topic 7<a hidden class="anchor" aria-hidden="true" href="#topic-7">#</a></h2>
<h2 id="topic-8">Topic 8<a hidden class="anchor" aria-hidden="true" href="#topic-8">#</a></h2>
<h2 id="topic-9">Topic 9<a hidden class="anchor" aria-hidden="true" href="#topic-9">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://mushetty.me">mushetty.me</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
