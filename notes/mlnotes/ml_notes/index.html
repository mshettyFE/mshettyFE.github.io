<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>ML Refresher Notes | mushetty.me</title>
<meta name="keywords" content="">
<meta name="description" content="Here is a compendium of my notes that I took to review for Machine Learning. This was meant to be a quick refresher on concepts and to put everything in one place. For a less cursory version, see the background section under Resources at the course website.
 Probability and Statistics  Basic Concepts  Axioms of Probability Definitions Baye&rsquo;s Rule Expectation   Common Probability Distributions  Bernoulli Distribution Binomial Distribution Poisson Distribution Categorical Distribution Multinomial Distribution Guassian(Normal) Distribution Multivariate Gaussian Distribution Laplace distribution Dirac Delta Distribution Mixtures of Distributions   Common Functions  Logistic sigmoid $\sigma(x)$ Softplus Function $\Zeta(x)$   Information Theory  Self-Information Shannon Entropy Kullback-Leibler (KL) divergence Cross-entropy Exponential Distribution   Chi-squared Distribution  Basics Goodness of Fit (GOF) Independence   T-test  One Sample Two Sample  Paired Unpaired       Linear Algebra  Types of Objects in Linear Algebra Matrix Operations Central Problem of Linear Algebra  Gaussian Elimination Reduced Row Echelon Form (RREF)  Reading of Solutions of Ax=b from RREF     LU Decomposition  LDU Decomposition   Identity and Inverses Vector Spaces  Subspaces  Orthogonal Complements     Linear Transformations  Diagonalization  Change of Basis     Linear Dependence and Span Basis and Dimension Norms Orthogonal Bases Gram-Schmidt and Orthogonal Complements Eigendecomposition Singular Value Decomposition (SVD) Moore-Penrose Pseudoinverse QR Decomposition  Gram-Schmidt   Trace Determinant Kernel, Range, Nullity,Rank  Mapping definitions Kernel Rank and Nullity   Least Squares   Calculus  Probability and Statistics Basic Concepts Axioms of Probability  Probability Measure $P: \mathbb{F} \rightarrow \mathbb{{R}}$ such that  $P(A) \geq 0$ for all $A \in \mathbb{F}$ $P(\Omega) = 1$ If $A_{1},A_{2}&hellip;$ are disjoint events, then  $P(\cup A_{i}) = \Sigma_{i} P(A_{i})$      Definitions  Sample space $\Omega$: The set of all possible outcomes Event space $\mathbb{F}:A$ is a subset of $\Omega$ a random variable quantity that has an uncertain value.">
<meta name="author" content="">
<link rel="canonical" href="https://mushetty.me/notes/mlnotes/ml_notes/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a5456a2fb6485e28d3190c101c9f7fea21d7eddeb7cf1b8dddf20aeb314df0cf.css" integrity="sha256-pUVqL7ZIXijTGQwQHJ9/6iHX7d63zxuN3fIK6zFN8M8=" rel="preload stylesheet" as="style">
<link rel="stylesheet" href="" />
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://mushetty.me/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mushetty.me/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mushetty.me/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mushetty.me/apple-touch-icon.png">
<link rel="mask-icon" href="https://mushetty.me/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


<meta property="og:title" content="ML Refresher Notes" />
<meta property="og:description" content="Here is a compendium of my notes that I took to review for Machine Learning. This was meant to be a quick refresher on concepts and to put everything in one place. For a less cursory version, see the background section under Resources at the course website.
 Probability and Statistics  Basic Concepts  Axioms of Probability Definitions Baye&rsquo;s Rule Expectation   Common Probability Distributions  Bernoulli Distribution Binomial Distribution Poisson Distribution Categorical Distribution Multinomial Distribution Guassian(Normal) Distribution Multivariate Gaussian Distribution Laplace distribution Dirac Delta Distribution Mixtures of Distributions   Common Functions  Logistic sigmoid $\sigma(x)$ Softplus Function $\Zeta(x)$   Information Theory  Self-Information Shannon Entropy Kullback-Leibler (KL) divergence Cross-entropy Exponential Distribution   Chi-squared Distribution  Basics Goodness of Fit (GOF) Independence   T-test  One Sample Two Sample  Paired Unpaired       Linear Algebra  Types of Objects in Linear Algebra Matrix Operations Central Problem of Linear Algebra  Gaussian Elimination Reduced Row Echelon Form (RREF)  Reading of Solutions of Ax=b from RREF     LU Decomposition  LDU Decomposition   Identity and Inverses Vector Spaces  Subspaces  Orthogonal Complements     Linear Transformations  Diagonalization  Change of Basis     Linear Dependence and Span Basis and Dimension Norms Orthogonal Bases Gram-Schmidt and Orthogonal Complements Eigendecomposition Singular Value Decomposition (SVD) Moore-Penrose Pseudoinverse QR Decomposition  Gram-Schmidt   Trace Determinant Kernel, Range, Nullity,Rank  Mapping definitions Kernel Rank and Nullity   Least Squares   Calculus  Probability and Statistics Basic Concepts Axioms of Probability  Probability Measure $P: \mathbb{F} \rightarrow \mathbb{{R}}$ such that  $P(A) \geq 0$ for all $A \in \mathbb{F}$ $P(\Omega) = 1$ If $A_{1},A_{2}&hellip;$ are disjoint events, then  $P(\cup A_{i}) = \Sigma_{i} P(A_{i})$      Definitions  Sample space $\Omega$: The set of all possible outcomes Event space $\mathbb{F}:A$ is a subset of $\Omega$ a random variable quantity that has an uncertain value." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mushetty.me/notes/mlnotes/ml_notes/" /><meta property="article:section" content="notes" />
<meta property="article:published_time" content="2023-01-07T17:29:54-05:00" />
<meta property="article:modified_time" content="2023-01-23T11:26:25-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ML Refresher Notes"/>
<meta name="twitter:description" content="Here is a compendium of my notes that I took to review for Machine Learning. This was meant to be a quick refresher on concepts and to put everything in one place. For a less cursory version, see the background section under Resources at the course website.
 Probability and Statistics  Basic Concepts  Axioms of Probability Definitions Baye&rsquo;s Rule Expectation   Common Probability Distributions  Bernoulli Distribution Binomial Distribution Poisson Distribution Categorical Distribution Multinomial Distribution Guassian(Normal) Distribution Multivariate Gaussian Distribution Laplace distribution Dirac Delta Distribution Mixtures of Distributions   Common Functions  Logistic sigmoid $\sigma(x)$ Softplus Function $\Zeta(x)$   Information Theory  Self-Information Shannon Entropy Kullback-Leibler (KL) divergence Cross-entropy Exponential Distribution   Chi-squared Distribution  Basics Goodness of Fit (GOF) Independence   T-test  One Sample Two Sample  Paired Unpaired       Linear Algebra  Types of Objects in Linear Algebra Matrix Operations Central Problem of Linear Algebra  Gaussian Elimination Reduced Row Echelon Form (RREF)  Reading of Solutions of Ax=b from RREF     LU Decomposition  LDU Decomposition   Identity and Inverses Vector Spaces  Subspaces  Orthogonal Complements     Linear Transformations  Diagonalization  Change of Basis     Linear Dependence and Span Basis and Dimension Norms Orthogonal Bases Gram-Schmidt and Orthogonal Complements Eigendecomposition Singular Value Decomposition (SVD) Moore-Penrose Pseudoinverse QR Decomposition  Gram-Schmidt   Trace Determinant Kernel, Range, Nullity,Rank  Mapping definitions Kernel Rank and Nullity   Least Squares   Calculus  Probability and Statistics Basic Concepts Axioms of Probability  Probability Measure $P: \mathbb{F} \rightarrow \mathbb{{R}}$ such that  $P(A) \geq 0$ for all $A \in \mathbb{F}$ $P(\Omega) = 1$ If $A_{1},A_{2}&hellip;$ are disjoint events, then  $P(\cup A_{i}) = \Sigma_{i} P(A_{i})$      Definitions  Sample space $\Omega$: The set of all possible outcomes Event space $\mathbb{F}:A$ is a subset of $\Omega$ a random variable quantity that has an uncertain value."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "ML Refresher Notes",
      "item": "https://mushetty.me/notes/mlnotes/ml_notes/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "ML Refresher Notes",
  "name": "ML Refresher Notes",
  "description": "Here is a compendium of my notes that I took to review for Machine Learning. This was meant to be a quick refresher on concepts and to put everything in one place. For a less cursory version, see the background section under Resources at the course website.\n Probability and Statistics  Basic Concepts  Axioms of Probability Definitions Baye\u0026rsquo;s Rule Expectation   Common Probability Distributions  Bernoulli Distribution Binomial Distribution Poisson Distribution Categorical Distribution Multinomial Distribution Guassian(Normal) Distribution Multivariate Gaussian Distribution Laplace distribution Dirac Delta Distribution Mixtures of Distributions   Common Functions  Logistic sigmoid $\\sigma(x)$ Softplus Function $\\Zeta(x)$   Information Theory  Self-Information Shannon Entropy Kullback-Leibler (KL) divergence Cross-entropy Exponential Distribution   Chi-squared Distribution  Basics Goodness of Fit (GOF) Independence   T-test  One Sample Two Sample  Paired Unpaired       Linear Algebra  Types of Objects in Linear Algebra Matrix Operations Central Problem of Linear Algebra  Gaussian Elimination Reduced Row Echelon Form (RREF)  Reading of Solutions of Ax=b from RREF     LU Decomposition  LDU Decomposition   Identity and Inverses Vector Spaces  Subspaces  Orthogonal Complements     Linear Transformations  Diagonalization  Change of Basis     Linear Dependence and Span Basis and Dimension Norms Orthogonal Bases Gram-Schmidt and Orthogonal Complements Eigendecomposition Singular Value Decomposition (SVD) Moore-Penrose Pseudoinverse QR Decomposition  Gram-Schmidt   Trace Determinant Kernel, Range, Nullity,Rank  Mapping definitions Kernel Rank and Nullity   Least Squares   Calculus  Probability and Statistics Basic Concepts Axioms of Probability  Probability Measure $P: \\mathbb{F} \\rightarrow \\mathbb{{R}}$ such that  $P(A) \\geq 0$ for all $A \\in \\mathbb{F}$ $P(\\Omega) = 1$ If $A_{1},A_{2}\u0026hellip;$ are disjoint events, then  $P(\\cup A_{i}) = \\Sigma_{i} P(A_{i})$      Definitions  Sample space $\\Omega$: The set of all possible outcomes Event space $\\mathbb{F}:A$ is a subset of $\\Omega$ a random variable quantity that has an uncertain value.",
  "keywords": [
    
  ],
  "articleBody": "Here is a compendium of my notes that I took to review for Machine Learning. This was meant to be a quick refresher on concepts and to put everything in one place. For a less cursory version, see the background section under Resources at the course website.\n Probability and Statistics  Basic Concepts  Axioms of Probability Definitions Baye’s Rule Expectation   Common Probability Distributions  Bernoulli Distribution Binomial Distribution Poisson Distribution Categorical Distribution Multinomial Distribution Guassian(Normal) Distribution Multivariate Gaussian Distribution Laplace distribution Dirac Delta Distribution Mixtures of Distributions   Common Functions  Logistic sigmoid $\\sigma(x)$ Softplus Function $\\Zeta(x)$   Information Theory  Self-Information Shannon Entropy Kullback-Leibler (KL) divergence Cross-entropy Exponential Distribution   Chi-squared Distribution  Basics Goodness of Fit (GOF) Independence   T-test  One Sample Two Sample  Paired Unpaired       Linear Algebra  Types of Objects in Linear Algebra Matrix Operations Central Problem of Linear Algebra  Gaussian Elimination Reduced Row Echelon Form (RREF)  Reading of Solutions of Ax=b from RREF     LU Decomposition  LDU Decomposition   Identity and Inverses Vector Spaces  Subspaces  Orthogonal Complements     Linear Transformations  Diagonalization  Change of Basis     Linear Dependence and Span Basis and Dimension Norms Orthogonal Bases Gram-Schmidt and Orthogonal Complements Eigendecomposition Singular Value Decomposition (SVD) Moore-Penrose Pseudoinverse QR Decomposition  Gram-Schmidt   Trace Determinant Kernel, Range, Nullity,Rank  Mapping definitions Kernel Rank and Nullity   Least Squares   Calculus  Probability and Statistics Basic Concepts Axioms of Probability  Probability Measure $P: \\mathbb{F} \\rightarrow \\mathbb{{R}}$ such that  $P(A) \\geq 0$ for all $A \\in \\mathbb{F}$ $P(\\Omega) = 1$ If $A_{1},A_{2}…$ are disjoint events, then  $P(\\cup A_{i}) = \\Sigma_{i} P(A_{i})$      Definitions  Sample space $\\Omega$: The set of all possible outcomes Event space $\\mathbb{F}:A$ is a subset of $\\Omega$ a random variable quantity that has an uncertain value.  Examples include coin flips (discrete), daily temperature (continuous)   An ensemble sampled from a random variable forms a probability distribution The probability density/mass function (pdf,pmf) denote the probability distribution of continuous and discrete functions respectively. Integral/sum must be 1. Joint probability p(x,y) denotes the probability of pair (x,y) occurring. Can be a mix of discrete and continuous. X and y can also be vectors. Marginalization refers to the “summing out” of a variable from a probability distribution ($p(x) = \\int p(x,y) dy$ for instance). This removes dependence of distribution on summed variables. This can be extended to discrete ($\\int \\rightarrow \\Sigma$) or to multiple variables ($p(x,y) = \\Sigma_w \\int p(w,x,y,z) dz$ ). Conditional Probability is the probability of x given a fixed y* value. Written as p(x|y = y*). Like taking a cross section of a CAD model  p(x|y = y*) is a relative probability, since the sum does not equal to 1. Need to normalize so that sum is 1. $p(x,y) = p(x|y)p(y) \\rightarrow p(x,y) = p(y|x)p(x)$ from symmetry   Chain Rule $p(x_{1},x_{2},…x_{k} = \\Pi_{i=1}^{k} p(x_{i}|x_{1},…,x_{i-1}) )$  Note: when $i=1$, you get $p(x_{1})= p(x|x)$, which is trivial, but important to point out In words: the joint probabilities is the product of probability of the first, time probability of the second given the first, etc. $p(x|y)=p(x)$ and $p(y|x)=p(y)$ are the conditions for x and y to be independent.  Substitute into conditional probability definition yields $p(x,y)= p(x)p(y)$     Conditional Independence: $p(x_{1}|x_{2},x_{3}) = p(x_{1}|x_{2})$ and $p(x_{1}|x_{2},x_{3}) = p(x_{1}|x_{2})$ are the conditions. Note the symmetry in $x_{1}$ and $x_{3}$  In words: $x_{1}$ is conditionally independent of $x_{3}$ given $x_{2}$. Or, if we know $x_{2}$, then $x_{1}$ gives no information about $x_{3}$. Variance of a population $\\sigma^{2} = \\frac{\\Sigma(x-\\mu)^{2}}{n} = \\frac{\\Sigma x^{2}}{n}-\\mu^{2}$ Variance of sample: $s^{2} = \\frac{\\Sigma(x-\\bar x)^{2}}{n-1} = \\frac{\\Sigma x^{2}-\\frac{(\\Sigma x)^{2}}{n}}{n-1}$ Standard Deviation = $\\sqrt(s^{2})$ Standard Error = $\\frac{s}{\\sqrt{n}}$ Null Hypothesis: There is no significant difference between two populations Type I error: Probability of rejecting a true null hypothesis  Equals significance level $\\alpha$   Type II error: Probability of failing to reject a false null hypothesis    Baye’s Rule  $p(x|y) = \\frac{p(y|x) p(x)}{p(y)}$  posterior: $p(x|y)$ or what we know about x given y prior: $p(x)$ or what we know about x before looking at y likelihood: $p(y|x)$ or the information gained about x of y Evidence: $p(y)$ knowledge of y  Typical example: a diagnostic test for Covid is 95% accurate. This means that if the test correctly identifies the true state of the patient 95% of the time (ie. if she has Covid, the test returns true, is she doesn’t, it returns false). Here X is the random variable of the true Covid state of the patient and Y is the random variable of the test result. Suppose the infection rate of Covid is 1 in 1000.  What is $p(Covid|Pos)$? Not 95%. $p(Covid|Pos) = \\frac{p(Pos|Covid)p(Covid)}{p(Pos|Covid)p(Covid)+p(Pos|NoCovid)p(NoCovid)}$ Plugging in values yields about 2%   A more conceptual way of thinking about Baye’s rule is that it tells you how to eliminate impossible outcomes given some information about the world.      Expectation  $E[f(x)] =\\int f(x) \\cdot p(x)dx$ ($\\int \\rightarrow \\Sigma$ for discrete)  p(x) is the weight assigned to each value of f(x)     Function Expectation     $x$ mean $\\mu_x$   $x^k$ k-th moment about 0   $(x-\\mu_x)^k$ k-th central moment   $(x-\\mu_x)^2$ variance (spread)   $(x-\\mu_x)^3$ skew (lean)   $(x-\\mu_x)^4$ kurtosis (peakedness)   $(x-\\mu_x)(y-\\mu_y)$ covariance of x and y     Expectation Manipulation Rules. Let a be a constant  $E[a] = a$ $E[a\\cdot x] = aE[x]$ $E[x + y] = E[x]+E[y]$ $E[x \\cdot y] = E[x]\\cdot E[y]$ if x,y are independent   Variance Manipulation Rules  $Var[a\\cdot x+b] = a^{2}\\cdot Var[x]$ $Var[x+y] = Var[x]+Var[y]$ if x,y are independent      Common Probability Distributions Bernoulli Distribution  Simple success or failure experiment let $\\lambda$ denote the probability of success. x=0 denotes failure and x=1 denotes success  $Bern_{x}(\\lambda) = \\lambda^{x}(1-\\lambda)^{1-x}$  $p(x=0) = 1-\\lambda$ $p(x=1) = \\lambda$ $E[x] = \\lambda$ $Var[x] = \\lambda(1-\\lambda)$      Binomial Distribution  Given a sequence of $N$ independent Bernoulli experiments, the Binomial distribution enumerates the probability for $m$ successes in the sequence $Bin_{m}(N,\\lambda) = {N \\choose x} \\lambda^{m}(1-\\lambda)^{N-m}$ $E[x] = N\\lambda$ $Var[x] = N\\lambda(1-\\lambda)$ For a fixed $N\\lambda$, the Binomial converges to Poisson as $N \\rightarrow \\infty$  Poisson Distribution  Consider independent events that happen at an average rate $\\lambda$ over time (ie. $\\lambda$ has “units” of number of events per time) Poisson distribution is discrete, and give the probability of a given number of events $k$ occurring in a fixed time interval $p(k,\\lambda) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!} = Pois_{k}(\\lambda)$ $E[x] = \\lambda$ $Var[x] = \\lambda$  Categorical Distribution  Suppose you run a single experiment with $K$ possible outcomes indexed from $K = {1,2,…k}$. Let $\\lambda =[\\lambda_{1},\\lambda_{2},..\\lambda_{k}]$ with $\\Sigma_{k=1}^{K} \\lambda_{k} = 1$ $p(x=k) = \\lambda_{k}$ $Cat_{x}(\\lambda) = p(x)$ $E[x=k] = \\lambda$ $Var[x=k] = \\lambda(1-\\lambda)$  Multinomial Distribution  Multinomial distribution give probability of a number of successes given a particular combination Generalization of Categorical Distribution to N trials and Binomial Distribution to K outcomes. Let $m = [m_1 m_2 … m_K]$ denote the observed counts in each category (total number of categories is $K$) over a sequence of $N$ trials Let $\\lambda = [\\lambda_1 \\lambda_2 … \\lambda_K]$ denote the probability of each category (subject to normalization of $\\lambda_1+ \\lambda_2 + … + \\lambda_K = 1$) $p(m) = {N \\choose m_1 m_2 … m_K}\\lambda_1^{m_1}\\lambda_1^{m_1}…\\lambda_1^{m_K}$ $E[m_{k}] = N\\lambda$ $Var[m_k] = N\\lambda(1-\\lambda)$  Guassian(Normal) Distribution  Useful because A Gaussian is fully specified by only two moments and central limit theorem (CLT) holds CLT states that the mean of independently draw random variables is normally distributed (given enough samples), irrespective of original distribution $E[m_{k}] = \\mu$ $Var[m_k] = \\sigma^{2}$ $p(x) = \\frac{1}{2\\pi \\sigma}e^{\\frac{-(x-\\mu)^{2}}{2\\sigma^{2}}}$ Standard normal distribution ($\\mu=0$, $\\sigma = 1$)  Multivariate Gaussian Distribution  Let $d$ be the dimension of the space Let $\\mu$ be a d-dimensonal vector (Mean vector) Let $\\Sigma$ be a DxD symmetric and positive semi-definite matrix (covariance matrix) $p(x) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\Sigma|^{\\frac{1}{2}}} exp(-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu))$ $E[m_{k}] = \\mu$ $Var[m_k] = \\Sigma$ Alternatively, $p(x) = \\frac{|\\beta|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{D}{2}}} exp(-\\frac{1}{2}(x-\\mu)^{T}\\beta(x-\\mu))$ where $\\beta = \\Sigma^{-1}$ is a precision matrix  Laplace distribution  $p(x) = \\frac{1}{2\\gamma}exp(-\\frac{-|x-\\mu|}{\\gamma})$ Places sharp peak at arbitrary point $\\mu$ with spread of $\\gamma$  Dirac Delta Distribution  $p(x) = \\delta(x-\\mu)$ Clusters all mass of the distribution function into a single point Infinity high, infinity narrow peak Exists to be integrated Can be extended to an arbitrary number of points (empirical distribution):  $p(x) = \\frac{1}{m}\\Sigma_{i-1}^{m} \\delta(x-x_{i})$    Mixtures of Distributions  Define a cluster of distributions. Define a probability distribution P(c) that selects which distribution is drawn from. $P(x) = \\Sigma_{i} P(c=i) P(x |c=i)$ c is a latent variable, or a random variable that can’t be observed directly Common types of mixture models are the empirical distribution and Gaussian mixture models, where the cluster consists of Gaussians.  Common Functions Logistic sigmoid $\\sigma(x)$  $\\sigma(x) = \\frac{1}{1+exp(-x)}$ Defined on $\\mathbb{R}$ and confined between 0 and 1 Saturates as $x \\rightarrow \\pm \\infty$ (ie insensitive to changes in x) Maximum slope at x=0 Useful to produce parameter of a Bernoulli distribution Properties:  $\\sigma(x) = \\frac{exp(x)}{exp(x)+exp(0)}$ $\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1-\\sigma(x))$ $1-\\sigma(x) = \\sigma(-x)$    Softplus Function $\\Zeta(x)$  $\\Zeta(x) = log(1+exp(x))$ Defined on $\\mathbb{R}$ and confined between 0 and $\\infty$ Produces $\\sigma$ or $\\beta$ of normal distribution A smoother version of $f(x) = max(0,x)$ Properties:  $log(\\sigma(x)) = -\\Zeta(-x)$ $\\frac{d}{dx} \\Zeta(x) = \\sigma(x)$ $\\forall x \\in (0,1), \\sigma^{-1}(x) = log(\\frac{x}{1-x})$ $\\forall x0, \\Zeta^{-1}(x) = log(exp(x)-1)$ $\\Zeta(x) = \\int_{-\\infty}^{x} \\sigma(y) dy$ $\\Zeta(x)-\\Zeta(-x) = x$    Information Theory Self-Information  General Idea: knowing something unlikely has happened provides more information that knowing something likely has happened. Information theory quantifies this statement Properties of Information  Likely events have low information (extreme: guaranteed events contain no information) Less likely events should have more information Independent events should have additive information (ie. tossing a coin twice contains twice as much information as tossing a coin once)   The above motivate the definition of self-information. Let x be an event. Then $I(x) = -log(P(x))$ is the self-information of the event. The log typically is base 2 or base e, but in theory could have any base you want; a chance in base is effectively a change in scale/units. NOTE: by definition, $lim_{x\\rightarrow 0 } x log(x) = 0$  Shannon Entropy  Extends I(x) to determine amount of uncertainty in a probability distribution:  $H(x) = H(P) = E[I(x)]$, or the expectation value of I(x)   In words, $H(x)$ gives the expect/average amount of information that you can gain from drawing from the probability distribution. Entropy is maximized for a uniform probability distribution (has a very direct connection to entropy in the thermodynamic sense) When x is continuous, Shannon entropy is known as differential entropy  Kullback-Leibler (KL) divergence  a metric of quantifying the difference in information between two distributions $P(x)$ and $Q(x)$  $D_{KL}(P||Q) = E[log(P(x))-log(Q(x))]$   In words (for the discrete case), $D_{KL}$ is the extra information needed to send a message using the code P instead of the code Q. KL divergence is non-negative Only 0 iff P and Q are the same distribution NOT symmetric in P and Q (not a true measure of distance)  Cross-entropy  $H(P,Q) = H(P)+D_{KL}(P||Q) = -E[log(Q(x))]$ Minimizing cross-entropy w.r.t. Q is the same as minimizing KL divergence  Exponential Distribution  Let $p(x,\\lambda)$ be a piecewise function such that $p(x,\\lambda)=0$ and $p(x0,\\lambda) = \\lambda exp(-\\lambda x)$ where $\\lambda$ is the decay parameter Serves to put a sharp peak at x=0  Chi-squared Distribution Basics  Chi-squared is a continuous distribution of the sum of the squares of k standard normally distributed random variables. k is called the number of “degrees of freedom” $q = \\Sigma_{i-1}^{k} x_{i}^{2}$ ($\\chi^{2}$ parameter) $p(x) = \\frac{x^{\\frac{k}{2}-1}e^{-\\frac{x}{2}}}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})}$ $E[m_{k}] = k$ $Var[m_k] = 2k$  Goodness of Fit (GOF)  $\\chi^{2} = \\Sigma \\frac{(O-E)^{2}}{E} $ degree of freedom (dof) = number of categories -1 Look up table for what p-value is. Determines if you accept or reject the null hypothesis by whether p-value is greater than (don’t reject null) or less than (reject null) significance level $\\alpha$  Independence  Chech whether categorical data is independent You are given observed data table O expected data table E is calculated cellwise  For a given row and column, the cell value is $E = \\frac{(\\text{row total})(\\text{column total})}{(\\text{grand total})}$ Calculate $\\chi^{2}$ as normal dof = (#rows-1)(#cols-1)    T-test One Sample  Tests whether the mean of a normally distributed population is different from a specific value Let $\\mu$ be the measured mean and $\\mu_{o}$ be the specific mean value to be tested $t = \\frac{\\bar{x}-\\mu_{o}}{\\frac{s}{\\sqrt{n}}}$ dof = n-1 Using a lookup table find the corresponding p-value, with the following caveats depending on what your null hypothesis is:  $\\mu\\mu_{0} \\rightarrow$ read table as given $\\muIf t-statistic has the incorrect sign, read the $1-p$ value instead   $\\mu\\mu_{0} \\rightarrow$ read table as given, but double p-value to take into account both tails   If p-value is less than $\\alpha$, reject the null hypothesis  Two Sample Test whether the means of two populations are significantly different from one another\nPaired  This means that each value in first group has a one to one mapping to the value in the second group Subtract values in a pairwise manner and run one sample t-test with $\\mu_{0}$ = 0  Unpaired  $t = \\frac{\\bar x_{1} - \\bar x_{2}}{\\sqrt{(\\frac{s_{1}^{2}}{n}+\\frac{s^{2}}{n_{2}})}}$ dof = $(n_{1}-1)+(n_{2}-1)$ Read off p-value and compare to significance level  Linear Algebra Types of Objects in Linear Algebra  Scalars: A number or a 1x1 matrix.  Can be confined to various domains eg. ($s \\in \\mathbb{R}$) is a real-valued scalar and ($n \\in \\mathbb{N}$) defines a natural number scalar   Vectors: An array of numbers. Each number is identified by its index.  typically though of as nx1 matrices (ie. column vectors).  $x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{m} \\end{bmatrix}$ Can also think of vectors as points embedded in n-dimensional space, with each element representing a coordinate on a different axis   Suppose you want to index only certain elements of a vector. Create a set S = {1,3,6} and write $x_{S}$ to denote elements $x_1, x_3, x_6$. $x_{-S}$ refers to complement of $x_{s}$, or all the elements in x excluding $x_1, x_3, x_6$   Matrices  2D array of numbers that uses 2 indices.  if A has m rows and n columns, then $A \\in \\mathbb{R}^{m \\times n}$ $A_{i,j}$ denotes the element in the ith row and jth column.  You can take slices with a “:\", namely $A_{i,:}$ denotes the horizontal slice at row i and $A_{:,j}$ denotes the vertical slice at col j   Can be explicitly written out by elements:  $\\begin{bmatrix} A_{1,1} \u0026 A_{1,2} \\\\ A_{2,1} \u0026 A_{2,2} \\\\ \\end{bmatrix}$       Tensors  n-dimensional arrays that can have an arbitrary number of indices (like the Levi-Civita tensor $\\epsilon_{ijk}$)    Matrix Operations  Transpose: taking the mirror image across the diagonal  Can also think about it as swapping the rows and columns, or more algebraically $A_{i,j} = A_{j,i}$ Can transpose a column vector into a row vector. So another way of writing a column vector is $\\begin{bmatrix} x_{1} \u0026 x_{2} \u0026 x_{3} \\end{bmatrix}^{T}$ When transposing a product of matrices, you reverse the order, and then transpose individually ie.  $(AB)^{T} = B^{T}A^{T}$     Multiplication Can multiply matrices by each other. If A is a m × n and B is a n × p matrix, then C is a m × p matrix.  $C = AB$ $C_{i,j} = \\Sigma_{k} A_{i,k}B_{k,j}$ Matrix multiplication is associative, distributive, but NOT commutative (ie. AB=BA is not always true)   Can multiply scalars by matrices: scale each element in matrix by the scalar c Hadamard Product ($A \\odot B$)  Element-wise multiplication between two same sized matrices   Dot Product: Between two vectors of the same size (say x and y), the dot product is defined as $x^{T}y$  The dot product is commutative: $x^{T}y = y^{T}x$   Addition  Can add matrices together as long as their dimensions align Can add scalars to matrices: Let A be an mxn matrix. Create a mxn matrix whose entries are entirely ones and scale it by the scalar, then add to A Broadcasting: Adding a vector to a matrix  let C and A be mxn matrices and b be a mx1 vector  C = A+b where b is implicitly added to each column of A        Central Problem of Linear Algebra  We want to solve $Ax = b$  Various techniques exists to state whether this is possible, how to do it if it is possible, or how close we can get    Gaussian Elimination  Use matrix multiplication on an augmented matrix $A|b$ to try and simplify the problem as much as possible Through taking linear combinations of the rows, you can simplify the problem while maintaining the row space (and thus the solution) of the problem  Reduced Row Echelon Form (RREF)  Defined as a matrix with the following properties  The pivot of any row is 1  The pivot is the first non-zero entry of a matrix   The pivot of any given row is always to the right of the pivot of the row above it  It does not need to be in adjacent column, it just needs to be to the right   The pivot is the only non-zero entry in its column   The end goal of Gaussian elimination is to get the augmented matrix into RREF  Reading of Solutions of Ax=b from RREF  For the ith column in RREF that does not have a pivot (ie. the free variables), define a variable $\\lambda_{i}$.  For the set of $\\lambda$, first set all lambda’s to 0 and solve x Then cycle through the set of $\\lambda$, setting only 1 of them to 1 (the rest of the free variables) and solve for x   The solution is then a linear combination of the vectors found, with the vector with $\\lambda$ all equal to zero having a prefactor of 1, and the rest of the vectors having a prefactor of their associated $\\lambda$  LU Decomposition  We can write any square matrix as the product of a lower triangular matrix and an upper triangular matrix  $M = LU$   Steps to get LU Decomposition  Perform Gaussian elimination until you get an upper triangular matrix. This is U As you do row operations, keep track of the elimination matrices used along the way Explicitly, you start from $A=LU$  $A=LU \\rightarrow E_{0}E_{1}…E_{n-1}A = U$ So $L^{-1} = E_{0}E_{1}…E_{n-1} \\rightarrow L = E_{n-1}^{-1}…E_{1}^{-1}E_{0}^{-1}$   The product of inverse elimination matrices are simple: You just add a row instead of subtract a row    LDU Decomposition  The goal of LDU is to have ones on the main diagonals of L and U.  D is a diagonal matrix This is accomplished by scaling each row of U such that the diagonal element in each row is 1 You place the scaling factor in the corresponding slot in D    Identity and Inverses  The identity matrix is defined as $A_{i,j} = \\delta_{i,j}$, where $\\delta$ is the Kronecker delta. More explicitly, A has ones along the main diagonal (top left to bottom right) and zeros everywhere else  I*A = A   The Inverse matrix of a square matrix A is defined as $AA^{-1} = A^{-1}A = I$  Assuming $A_{-1}$ exists, its solves the fundamental problem of linear algebra: $Ax=b \\rightarrow x = A^{-1}b$    Vector Spaces  A vector space over some field $\\mathbb{F}$ (think $\\mathbb{R}$,$\\mathbb{C}$,$\\mathbb{Z}$) is a set $V$ with two operations + and * that satisfies the following properties (given $\\forall u,v \\in \\mathbb{R}$ and $c,d \\in \\mathbb{R}$)  Closure under addition: $u+v \\in V$ Commutativity of addition: $u+v=v+u$ Associativity of addition: $(u+v)+2=u+(v+w)$ Zero: $\\exist 0_{v} \\in V$ such that $u+0_{v} = u \\ \\forall u \\in V$ Additive inverse: $\\exist u \\in V$ such that $u+w = 0 \\ \\forall w \\in V$ Multiplicative Closure: For every $u \\in V$ there exists $w \\in V$ such that $u + w = 0_{v}$ Distributivity of scalars over scalar addition: (c+d)·v = c·v+d·v where $c,d,v \\in \\mathbb{F}$ Distributivity of scalars over vector addition: c·(u+v) = c·u+c·v where $c \\in \\mathbb{F}$ and $u,v \\in \\mathbb{F}$ Associativity of scalar multiplication: (cd)·v = c· (d·v) where $c,d \\in \\mathbb{F}$ and $v \\in V$ Existence of Unity: 1 · v = v for all v ∈ V where $1 \\in \\mathbb{F}$    Subspaces  A subspace U is a subset of V that satisfies:  $\\alpha_{1}u_{1}+\\alpha_{2}u_{2} \\in U$ where $\\forall \\alpha_{1},\\alpha_{2} \\in \\mathbb{R}$ and $\\ u_{1},u{2} \\in U$   For any subset $S \\subset V$, span(S) is a subspace of V  Orthogonal Complements  Let U and V be subspaces of vector space W  The sum of U and V is: $U+V = span(U\\cup V) = \\{u+v|u\\in U,v\\in V\\}$ The direct sum of U and V is: $U\\otimes V = span(U\\cup V) = \\{u+v|u\\in U,v\\in V\\}$, assuming that $U\\cap V = \\{0_{w}\\}$  Let $w = u+v \\in U\\otimes V$. There is only one way to write w as a the sum of a vector in U and a vector in V     Given a subspace U of W, we define:  $U^{\\perp} = \\{w\\in W|w\\cdot u = 0,\\ \\forall u \\in U\\}$ This is the orthogonal complement Let U be subspace of a finite dimensional vector space W. Then the set $U{\\perp}$ is a subspace of W and $W = U\\otimes U^{\\perp}$    Linear Transformations  Define a map $L: V \\rightarrow W$. This map is linear if the following conditions hold:  $L(u+v) = L(u)+L(v)$ $L(cv) = cL(v)$   Alternatively, you can combine the two conditions:  $L(ru+sv) = rL(u)+sL(v)$    Diagonalization  Given a linear transformation, how can we write it as a matrix?  Simplest example: $L: V\\rightarrow V$ and we have a basis consisting of the linearly independent eigenvectors of L and the corresponding eigenvalues. Then L can be written as a diagonal matrix whose entries are the eigenvalues  the matrix for L in the basis S is diagonal iff S is a basis of eigenvectors for L      Change of Basis  A change of basis refers to writing on basis S in terms of another basis T  Look at each vector in S, and figure out a linear combination of T that creates S. This linear combination goes into the associated column of the matrix P   Two matrices are similar if there exists and invertible matrix P such that  $N = P^{-1}MP$ A matrix M is diagonalizeable if there exists an invertible matrix P and Diagonal matrix D such that  $D = P^{-1}MP$   What this means is that, to rewrite a matrix M in terms of a different basis, perform the transformation $M \\rightarrow P^{-1}MP$ with the appropriate change of basis matrix P If you know the eigendecomposition of a matrix M, then you can diagonalize M by performing a change of basis to the eigenvector basis    Linear Dependence and Span  One can think about the product $Ax$ as the following:  each component of x scales the corresponding column of a. The scaled columns are then summed together  $Ax = \\Sigma_{i} x_{i}A_{:,i}$   This is one way to think about a linear combination. Given a set of vectors, you scale each vector by some factor and sum the scaled vectors span denotes the set of all possible linear combinations   One can reformulate the fundamental problem $Ax=b$ as determining whether some combination of the columns equals b, or equivalently, does b lie in the span of the column space of A? For a given set of vectors, we say the set is linearly independent if no combination yields the zero vector  Basis and Dimension  Dimension refers to the number of components necessary to describe an vector Let V be a vector space. The a set S is a basis for V is S is linearly independent and V = span(S)  In $\\mathbb{R^{n}}$, the standard basis is the columns of the identity matrix A basis is not unique  Although each vector has a unique representation in a given basis     V = span($v_{1}v_{2}…v_{n}$) iff all vectors are linearly independent  Norms  The norm is a function that gives a metric of the size of a vector. It must satisfy the following properties:  $f(x) = 0 \\rightarrow x = 0$ $f(x+y) \\leq f(x) + f(y)$ (triangle equality) $ \\forall \\alpha \\in \\mathbb{R}, f(\\alpha x) = |\\alpha|f(x)$   The $L^{p}$ norm is defined as (for $p \\in \\mathbb{R}, p \\geq 1$)  $||x||_{p} = (\\Sigma_{i}|x_{i}|^{p})^{\\frac{1}{2}}$   The squared $L^{2}$ norm is useful since mathematically and computationally, it is easier to work with (derivative is simple, don’t need a square root) The $L{1}$ norm is useful when the difference between zero and nonzero elements is important  $L{1}$ norm increases by $\\epsilon$ as x goes from 0 to $\\epsilon$   The $L_{\\infty}$ norm denotes the absolute value of the largest element in the array  $||x||_{\\infty} = max|x_{i}|$   There also exists a notion of norm for a matrix. The most common is the Forbenius norm:  $||A||_{F}\\sqrt{\\Sigma_{i,j}A^{2}_{i,j}}$    Orthogonal Bases  a basis is orthogonal if all the vectors are prependicular to each other  $v_{i}\\cdot v_{j} = \\delta_{ij}$  Orthonormal bases impose the additional condition that the norm of each basis vector is 1   In an orthonormal basis, any vector can be written as  $v = \\Sigma_{i}(v\\cdot u_{i})u_{i}$   Change of basis matrices between orthonormal bases are orthogonal matrices ie.  $P^{-1} = P^{T}$      Gram-Schmidt and Orthogonal Complements Eigendecomposition  Solve the matrix equation $Av = \\lambda v$, where $\\lambda$ is a constant.  $\\lambda$ is called the eigenvalue and $v$ is called the eigenvector Eigenvectors are NOT unique. This is because we can think of an eigenvector as any vector whose direction is unchanged after being acted on by a matrix. Hence, eigenvectors are typically normalized for convenience. Multiple eigenvectors can have the same eigenvalue Real matrices can have complex eigenvalues   The eigendecomposition is not possible for all matrices Given the eigendecomposition of a matrix (ie $\\lambda = [\\lambda_{1},\\lambda_{1},…,\\lambda_{n}]$ and V is a matrix whose columns are the eigenvectors), you can write the matrix as:  $A = \\bold{V} diag(\\lambda)V^{-1}$ Typically, the eigenvalues are sorted in descending order Make sure that the eigenvectors are in the column that correspond to the correct eigenvalue   For a real symmetric matrix, the eigendecomposition is:  $Q\\Lambda Q^{T}$, where $Q$ is an orthogonal matrix The eigendecomposition is guaranteed to exist for a real, symmetric matrix   A matrix is singular iff any of the eigenvalues are zero Let $f(x) = x^{T}Ax$ subject to $||x_{2}||=1$. Whenever x is equal to an eigenvector of A, f equals the corresponding eigenvalue  The minimum and maximum of f within the constraint region (ie. the surface of a n-dimensional unit ball) is the minimum and maximum eigenvalue respectively    Singular Value Decomposition (SVD)  Every real matrix has a singular value decomposition. Suppose A is an mxn matrix. Let U be a mxm orthogonal matrix, D be a mxn diagonal matrix (NOTE: not necessarily square), and V be a orthogonal nxn matrix. Then  $A = UDV^{T}$ The columns of U are the left-singular vectors, the columns of V are the right-singular vectors, and the diagonal elements are the singular values U can be found by taking the eigenvectors of $AA^{T}$, V can be found by taking the eigenvectors of $A^{T}A$, and the singular values are the square root of the eigenvalues of $A^{T}A$ of $AA^{T}$, whichever has the smaller dimension.    Moore-Penrose Pseudoinverse  For non-square matrices, there may exist a matrix $B$ that solves $Ax = y$ by left-multiplication:  $Ax=y \\rightarrow x = By$   If you have a tall matrix, there may exist no solution, and if you have a wide matrix, there could be multiple solutions Define the matrix:  $A^{+} = \\lim_{\\alpha \\rightarrow 0}(A^{T}A+\\alpha I)^{-1}A^{T}$   The practical formula for computing this matrix is  $A^{+} = VD_{+}U^{T}$  $U,V,D$ are from the SVD of A. $D^{+}$ can be computed by taking the reciprocal of the nonzero elements and then transposing     If A has more columns than row, then $x=A^{+}y$ is one of the possible solutions  This solution has the minimum Euclidean norm $||x||_{2}$ among all possible solutions   If A has more rows than columns, the x from the pseudoinverse minimizes $||Ax-y||^{2}$ (ie. the vector that has the smallest Euclidean norm that almost solve the problem)  QR Decomposition Gram-Schmidt  For each column vector in matrix M, let the first column be your first basis vector  The second column can be made orthogonal by subtracting of the projection of column 2 from column 1  $v^{\\perp} = v-\\frac{u \\cdot v}{u \\cdot u}u$   Rinse and repeat for the rest of the columns, subtracting off the projections of the established basis vectors Normalize every vector at the end   Place all the normalized basis vectors in a orthogonal matrix Q The R matrix can be figured out with the following fact:  The (i,j) entry of the upper triangular matrix R equals the dot product of the i-th column of Q with the j-th column of M   Gram-Schmidt is quite numerically unstable, so it is good for visualization but impractical  Trace  The trace gives the sum of all diagonal entries of a matrix  $Tr(A) = \\Sigma_{i}A_{i,j}$   Useful to replace summations  Frobenius norm can be rewritten as  $||A||_{F} = \\sqrt{Tr(AA^{T})}$     $Tr(A) = Tr(A^{T})$ The trace of a square matrix is invariant under cyclic permuation of matrices  $Tr(ABC)=Tr(CAB)=Tr(BCA)$ This holds even if A and B are not square matrices, as long as the final product is a square matrix    Determinant  Denotes as Det(A) is only defined for square matrices. Det(A) equals the product of all eigenvalues of a matrix $|Det(A)|$ can be thought of as how the volume of the space changes upon action by the matrix A  If Det(A) = 0, then the space collapsed to a lower dimensional one  Det(A) = 0 also means that the matrix is not-invertible   If Det(A) = 1, then the transformation preserves volume   Determinant changes sign upon swapping either two rows or two columns Scaling a row by $\\lambda$ changes the determinant by a factor of $\\lambda$ Row operations do not change the value of the determinant det(AB) = det(A) det(B) det($A^{T}$) = det(A) det($A^{-1}$) = $\\frac{1}{det(A)}$  Kernel, Range, Nullity,Rank Mapping definitions  Let $f: S \\rightarrow T$ be a map from S to T.  S is the domain of the function T is the codomain of the fraction the set $ran(f) = im(f) = f(S) = \\{f(s)|s\\in S\\} \\subset T$ is called the range or image of f  You can think of the range as the part of the codomain T that S actually maps to the image of $L(V)$ is a subspace of W   for some subset U of T, then $f^{-1}(U) = \\{s\\in S|f(s) \\in U\\}\\subset S$  The preimage of a set $U$ is the set of all elements of $S$ which maps to $U$   The function f is one-to-one if different elements of S always map to different elements of T (for $x\\neq y, f(x)\\neq f(y)$)  also called injective   The function f is onto if every element of T is mapped to some element of T (ie. $\\forall t \\in T, \\ \\exists s \\in S \\rightarrow f(s)= t$)  also called surjective   functions that are both injective and surjective are called bijective    Kernel  Let $L: V\\rightarrow W$ be a linear transformation. The kernel of L is defined as  $ker\\ L = \\{v\\in V|Lv=0_{w}\\}$ In words, the kernel is all $v \\in V$ such that the linear transformation maps onto the zero vector in $W$ A linear transformation L is injective iff $ker\\ L = \\{0_{w}\\}$  ie. only the zero vector in $V$ maps to the zero vector in $W$ $ker\\ L$ is a subspace of V      Rank and Nullity  The rank of a linear transformation $L$ is the dimension of its image, written as $rank(L) = dim\\ L(V) = dim\\ Im(L)$ The nullity is the dimension of the kernel, written as $null\\ L = dim\\ ker\\ L$ $dim\\ V = dim\\ ker\\ V + dim\\ L(V)$  Least Squares  Solving the equation $M^{T}MX = M^{T}V$, where M is a rectangular matrix  Used when there exists no perfect solution to $MX=V$. Finds the best possible solution   For polynomial fits, you construct a Vandermonde matrix from your data and solve for the coefficients of the polynomial  Calculus ",
  "wordCount" : "5183",
  "inLanguage": "en",
  "datePublished": "2023-01-07T17:29:54-05:00",
  "dateModified": "2023-01-23T11:26:25-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mushetty.me/notes/mlnotes/ml_notes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "mushetty.me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mushetty.me/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mushetty.me" accesskey="h" title="mushetty.me (Alt + H)">mushetty.me</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mushetty.me/simulations/" title="Simulations">
                    <span>Simulations</span>
                </a>
            </li>
            <li>
                <a href="https://mushetty.me/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://mushetty.me/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      ML Refresher Notes
    </h1>
    <div class="post-meta"><span title='2023-01-07 17:29:54 -0500 EST'>Date Created: January 7, 2023</span>&nbsp;·&nbsp;<span title='2023-01-23 11:26:25 -0500 EST'>Last Modified: January 23, 2023</span>

</div>
  </header> 
  <div class="post-content"><p>Here is a compendium of my notes that I took to review for Machine Learning. This was meant to be a quick refresher on concepts and to put everything in one place. For a less cursory version, see the background section under Resources at the course <a href="https://www.cs.columbia.edu/~verma/classes/ml/index.html">website</a>.</p>
<ul>
<li><a href="#probability-and-statistics">Probability and Statistics</a>
<ul>
<li><a href="#basic-concepts">Basic Concepts</a>
<ul>
<li><a href="#axioms-of-probability">Axioms of Probability</a></li>
<li><a href="#definitions">Definitions</a></li>
<li><a href="#bayes-rule">Baye&rsquo;s Rule</a></li>
<li><a href="#expectation">Expectation</a></li>
</ul>
</li>
<li><a href="#common-probability-distributions">Common Probability Distributions</a>
<ul>
<li><a href="#bernoulli-distribution">Bernoulli Distribution</a></li>
<li><a href="#binomial-distribution">Binomial Distribution</a></li>
<li><a href="#poisson-distribution">Poisson Distribution</a></li>
<li><a href="#categorical-distribution">Categorical Distribution</a></li>
<li><a href="#multinomial-distribution">Multinomial Distribution</a></li>
<li><a href="#guassiannormal-distribution">Guassian(Normal) Distribution</a></li>
<li><a href="#multivariate-gaussian-distribution">Multivariate Gaussian Distribution</a></li>
<li><a href="#laplace-distribution">Laplace distribution</a></li>
<li><a href="#dirac-delta-distribution">Dirac Delta Distribution</a></li>
<li><a href="#mixtures-of-distributions">Mixtures of Distributions</a></li>
</ul>
</li>
<li><a href="#common-functions">Common Functions</a>
<ul>
<li><a href="#logistic-sigmoid-sigmax">Logistic sigmoid $\sigma(x)$</a></li>
<li><a href="#softplus-function-zetax">Softplus Function $\Zeta(x)$</a></li>
</ul>
</li>
<li><a href="#information-theory">Information Theory</a>
<ul>
<li><a href="#self-information">Self-Information</a></li>
<li><a href="#shannon-entropy">Shannon Entropy</a></li>
<li><a href="#kullback-leibler-kl-divergence">Kullback-Leibler (KL) divergence</a></li>
<li><a href="#cross-entropy">Cross-entropy</a></li>
<li><a href="#exponential-distribution">Exponential Distribution</a></li>
</ul>
</li>
<li><a href="#chi-squared-distribution">Chi-squared Distribution</a>
<ul>
<li><a href="#basics">Basics</a></li>
<li><a href="#goodness-of-fit-gof">Goodness of Fit (GOF)</a></li>
<li><a href="#independence">Independence</a></li>
</ul>
</li>
<li><a href="#t-test">T-test</a>
<ul>
<li><a href="#one-sample">One Sample</a></li>
<li><a href="#two-sample">Two Sample</a>
<ul>
<li><a href="#paired">Paired</a></li>
<li><a href="#unpaired">Unpaired</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#linear-algebra">Linear Algebra</a>
<ul>
<li><a href="#types-of-objects-in-linear-algebra">Types of Objects in Linear Algebra</a></li>
<li><a href="#matrix-operations">Matrix Operations</a></li>
<li><a href="#central-problem-of-linear-algebra">Central Problem of Linear Algebra</a>
<ul>
<li><a href="#gaussian-elimination">Gaussian Elimination</a></li>
<li><a href="#reduced-row-echelon-form-rref">Reduced Row Echelon Form (RREF)</a>
<ul>
<li><a href="#reading-of-solutions-of-axb-from-rref">Reading of Solutions of Ax=b from RREF</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#lu-decomposition">LU Decomposition</a>
<ul>
<li><a href="#ldu-decomposition">LDU Decomposition</a></li>
</ul>
</li>
<li><a href="#identity-and-inverses">Identity and Inverses</a></li>
<li><a href="#vector-spaces">Vector Spaces</a>
<ul>
<li><a href="#subspaces">Subspaces</a>
<ul>
<li><a href="#orthogonal-complements">Orthogonal Complements</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#linear-transformations">Linear Transformations</a>
<ul>
<li><a href="#diagonalization">Diagonalization</a>
<ul>
<li><a href="#change-of-basis">Change of Basis</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#linear-dependence-and-span">Linear Dependence and Span</a></li>
<li><a href="#basis-and-dimension">Basis and Dimension</a></li>
<li><a href="#norms">Norms</a></li>
<li><a href="#orthogonal-bases">Orthogonal Bases</a></li>
<li><a href="#gram-schmidt-and-orthogonal-complements">Gram-Schmidt and Orthogonal Complements</a></li>
<li><a href="#eigendecomposition">Eigendecomposition</a></li>
<li><a href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></li>
<li><a href="#moore-penrose-pseudoinverse">Moore-Penrose Pseudoinverse</a></li>
<li><a href="#qr-decomposition">QR Decomposition</a>
<ul>
<li><a href="#gram-schmidt">Gram-Schmidt</a></li>
</ul>
</li>
<li><a href="#trace">Trace</a></li>
<li><a href="#determinant">Determinant</a></li>
<li><a href="#kernel-range-nullityrank">Kernel, Range, Nullity,Rank</a>
<ul>
<li><a href="#mapping-definitions">Mapping definitions</a></li>
<li><a href="#kernel">Kernel</a></li>
<li><a href="#rank-and-nullity">Rank and Nullity</a></li>
</ul>
</li>
<li><a href="#least-squares">Least Squares</a></li>
</ul>
</li>
<li><a href="#calculus">Calculus</a></li>
</ul>
<h2 id="probability-and-statistics">Probability and Statistics<a hidden class="anchor" aria-hidden="true" href="#probability-and-statistics">#</a></h2>
<h3 id="basic-concepts">Basic Concepts<a hidden class="anchor" aria-hidden="true" href="#basic-concepts">#</a></h3>
<h4 id="axioms-of-probability">Axioms of Probability<a hidden class="anchor" aria-hidden="true" href="#axioms-of-probability">#</a></h4>
<ul>
<li>Probability Measure $P: \mathbb{F} \rightarrow \mathbb{{R}}$ such that
<ul>
<li>$P(A) \geq 0$ for all $A \in \mathbb{F}$</li>
<li>$P(\Omega) = 1$</li>
<li>If $A_{1},A_{2}&hellip;$ are disjoint events, then
<ul>
<li>$P(\cup A_{i}) = \Sigma_{i} P(A_{i})$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="definitions">Definitions<a hidden class="anchor" aria-hidden="true" href="#definitions">#</a></h4>
<ul>
<li>Sample space $\Omega$: The set of all possible outcomes</li>
<li>Event space $\mathbb{F}:A$ is a subset of $\Omega$</li>
<li>a <strong>random variable</strong> quantity that has an uncertain value.
<ul>
<li>Examples include coin flips (discrete), daily temperature (continuous)</li>
</ul>
</li>
<li>An ensemble sampled from a random variable forms a <strong>probability distribution</strong></li>
<li>The <strong>probability density/mass function (pdf,pmf)</strong> denote the probability distribution of continuous and discrete functions respectively. Integral/sum must be 1.</li>
<li><strong>Joint probability p(x,y)</strong> denotes the probability of pair (x,y) occurring. Can be a mix of discrete and continuous. X and y can also be vectors.</li>
<li><strong>Marginalization</strong> refers to the &ldquo;summing out&rdquo; of a variable from a probability distribution ($p(x) = \int p(x,y) dy$ for instance). This removes dependence of distribution on summed variables. This can be extended to discrete ($\int \rightarrow \Sigma$) or to multiple variables ($p(x,y) = \Sigma_w \int p(w,x,y,z) dz$ ).</li>
<li><strong>Conditional Probability</strong> is the probability of x given a fixed y* value. Written as p(x|y = y*). Like taking a cross section of a CAD model
<ul>
<li>p(x|y = y*) is a relative probability, since the sum does not equal to 1. Need to normalize so that sum is 1.</li>
<li>$p(x,y) = p(x|y)p(y) \rightarrow p(x,y) = p(y|x)p(x)$ from symmetry</li>
</ul>
</li>
<li><strong>Chain Rule</strong> $p(x_{1},x_{2},&hellip;x_{k} = \Pi_{i=1}^{k} p(x_{i}|x_{1},&hellip;,x_{i-1}) )$
<ul>
<li>Note: when $i=1$, you get $p(x_{1})= p(x|x)$, which is trivial, but important to point out</li>
<li>In words: the joint probabilities is the product of probability of the first, time probability of the second given the first, etc.</li>
<li>$p(x|y)=p(x)$ and $p(y|x)=p(y)$ are the conditions for x and y to be independent.
<ul>
<li>Substitute into conditional probability definition yields $p(x,y)= p(x)p(y)$</li>
</ul>
</li>
</ul>
</li>
<li><strong>Conditional Independence</strong>: $p(x_{1}|x_{2},x_{3}) = p(x_{1}|x_{2})$ and $p(x_{1}|x_{2},x_{3}) = p(x_{1}|x_{2})$ are the conditions. Note the symmetry in $x_{1}$ and $x_{3}$
<ul>
<li>In words: $x_{1}$ is conditionally independent of $x_{3}$ given $x_{2}$. Or, if we know $x_{2}$, then $x_{1}$ gives no information about $x_{3}$.</li>
<li>Variance of a population $\sigma^{2} = \frac{\Sigma(x-\mu)^{2}}{n} = \frac{\Sigma x^{2}}{n}-\mu^{2}$</li>
<li>Variance of sample: $s^{2} = \frac{\Sigma(x-\bar x)^{2}}{n-1} = \frac{\Sigma x^{2}-\frac{(\Sigma x)^{2}}{n}}{n-1}$</li>
<li>Standard Deviation = $\sqrt(s^{2})$</li>
<li>Standard Error = $\frac{s}{\sqrt{n}}$</li>
<li>Null Hypothesis: There is no significant difference between two populations</li>
<li>Type I error: Probability of rejecting a true null hypothesis
<ul>
<li>Equals significance level $\alpha$</li>
</ul>
</li>
<li>Type II error: Probability of failing to reject a false null hypothesis</li>
</ul>
</li>
</ul>
<h4 id="bayes-rule">Baye&rsquo;s Rule<a hidden class="anchor" aria-hidden="true" href="#bayes-rule">#</a></h4>
<ul>
<li>$p(x|y) = \frac{p(y|x) p(x)}{p(y)}$
<ul>
<li>posterior: $p(x|y)$ or what we know about x given y</li>
<li>prior: $p(x)$ or what we know about x before looking at y</li>
<li>likelihood: $p(y|x)$ or the information gained about x of y</li>
<li>Evidence: $p(y)$ knowledge of y
<ul>
<li>Typical example: a diagnostic test for Covid is 95% accurate. This means that if the test correctly identifies the true state of the patient 95% of the time (ie. if she has Covid, the test returns true, is she doesn&rsquo;t, it returns false). Here X is the random variable of the true Covid state of the patient and Y is the random variable of the test result. Suppose the infection rate of Covid is 1 in 1000.
<ul>
<li>What is $p(Covid|Pos)$? Not 95%.</li>
<li>$p(Covid|Pos) = \frac{p(Pos|Covid)p(Covid)}{p(Pos|Covid)p(Covid)+p(Pos|NoCovid)p(NoCovid)}$</li>
<li>Plugging in values yields about 2%</li>
</ul>
</li>
<li>A more conceptual way of thinking about Baye&rsquo;s rule is that it tells you how to eliminate impossible outcomes given some information about the world.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="expectation">Expectation<a hidden class="anchor" aria-hidden="true" href="#expectation">#</a></h4>
<ul>
<li>$E[f(x)] =\int f(x) \cdot p(x)dx$ ($\int \rightarrow \Sigma$ for discrete)
<ul>
<li>p(x) is the weight assigned to each value of f(x)</li>
</ul>
<table>
<thead>
<tr>
<th>Function</th>
<th>Expectation</th>
</tr>
</thead>
<tbody>
<tr>
<td>$x$</td>
<td>mean  $\mu_x$</td>
</tr>
<tr>
<td>$x^k$</td>
<td>k-th moment about 0</td>
</tr>
<tr>
<td>$(x-\mu_x)^k$</td>
<td>k-th central moment</td>
</tr>
<tr>
<td>$(x-\mu_x)^2$</td>
<td>variance  (spread)</td>
</tr>
<tr>
<td>$(x-\mu_x)^3$</td>
<td>skew (lean)</td>
</tr>
<tr>
<td>$(x-\mu_x)^4$</td>
<td>kurtosis (peakedness)</td>
</tr>
<tr>
<td>$(x-\mu_x)(y-\mu_y)$</td>
<td>covariance of x and y</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Expectation Manipulation Rules</strong>. Let a be a constant
<ul>
<li>$E[a] = a$</li>
<li>$E[a\cdot x] = aE[x]$</li>
<li>$E[x + y] = E[x]+E[y]$</li>
<li>$E[x \cdot y] = E[x]\cdot E[y]$ if x,y are independent</li>
</ul>
</li>
<li><strong>Variance Manipulation Rules</strong>
<ul>
<li>$Var[a\cdot x+b] = a^{2}\cdot Var[x]$</li>
<li>$Var[x+y] = Var[x]+Var[y]$ if x,y are independent</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="common-probability-distributions">Common Probability Distributions<a hidden class="anchor" aria-hidden="true" href="#common-probability-distributions">#</a></h3>
<h4 id="bernoulli-distribution">Bernoulli Distribution<a hidden class="anchor" aria-hidden="true" href="#bernoulli-distribution">#</a></h4>
<ul>
<li>Simple success or failure experiment</li>
<li>let $\lambda$ denote the probability of success. x=0 denotes failure and x=1 denotes success
<ul>
<li>$Bern_{x}(\lambda) = \lambda^{x}(1-\lambda)^{1-x}$
<ul>
<li>$p(x=0) = 1-\lambda$</li>
<li>$p(x=1) = \lambda$</li>
<li>$E[x] = \lambda$</li>
<li>$Var[x] = \lambda(1-\lambda)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="binomial-distribution">Binomial Distribution<a hidden class="anchor" aria-hidden="true" href="#binomial-distribution">#</a></h4>
<ul>
<li>Given a sequence of $N$ <strong>independent</strong> Bernoulli experiments, the Binomial distribution enumerates the probability for $m$ successes in the sequence</li>
<li>$Bin_{m}(N,\lambda) = {N \choose x} \lambda^{m}(1-\lambda)^{N-m}$</li>
<li>$E[x] = N\lambda$</li>
<li>$Var[x] = N\lambda(1-\lambda)$</li>
<li>For a fixed $N\lambda$, the Binomial converges to <a href="#poisson-distribution">Poisson</a> as $N \rightarrow \infty$</li>
</ul>
<h4 id="poisson-distribution">Poisson Distribution<a hidden class="anchor" aria-hidden="true" href="#poisson-distribution">#</a></h4>
<ul>
<li>Consider independent events that happen at an average rate $\lambda$ over time (ie. $\lambda$ has &ldquo;units&rdquo; of number of events per time)</li>
<li>Poisson distribution is discrete, and give the probability of a given number of events $k$ occurring in a fixed time interval</li>
<li>$p(k,\lambda) = \frac{\lambda^{k}e^{-\lambda}}{k!} = Pois_{k}(\lambda)$</li>
<li>$E[x] = \lambda$</li>
<li>$Var[x] = \lambda$</li>
</ul>
<h4 id="categorical-distribution">Categorical Distribution<a hidden class="anchor" aria-hidden="true" href="#categorical-distribution">#</a></h4>
<ul>
<li>Suppose you run a single experiment with $K$ possible outcomes indexed from $K = {1,2,&hellip;k}$. Let $\lambda =[\lambda_{1},\lambda_{2},..\lambda_{k}]$ with $\Sigma_{k=1}^{K} \lambda_{k} = 1$</li>
<li>$p(x=k) = \lambda_{k}$</li>
<li>$Cat_{x}(\lambda) = p(x)$</li>
<li>$E[x=k] = \lambda$</li>
<li>$Var[x=k] = \lambda(1-\lambda)$</li>
</ul>
<h4 id="multinomial-distribution">Multinomial Distribution<a hidden class="anchor" aria-hidden="true" href="#multinomial-distribution">#</a></h4>
<ul>
<li>Multinomial distribution give probability of a number of successes given a particular combination</li>
<li>Generalization of <a href="#categorical-distribution">Categorical Distribution</a> to N trials and <a href="#binomial-distribution">Binomial Distribution</a> to K outcomes.</li>
<li>Let $m = [m_1 m_2 &hellip; m_K]$ denote the observed counts in each category (total number of categories is $K$) over a sequence of $N$ trials</li>
<li>Let $\lambda = [\lambda_1 \lambda_2 &hellip; \lambda_K]$ denote the probability of each category (subject to normalization of $\lambda_1+ \lambda_2 + &hellip; + \lambda_K = 1$)</li>
<li>$p(m) = {N \choose m_1 m_2 &hellip; m_K}\lambda_1^{m_1}\lambda_1^{m_1}&hellip;\lambda_1^{m_K}$</li>
<li>$E[m_{k}] = N\lambda$</li>
<li>$Var[m_k] = N\lambda(1-\lambda)$</li>
</ul>
<h4 id="guassiannormal-distribution">Guassian(Normal) Distribution<a hidden class="anchor" aria-hidden="true" href="#guassiannormal-distribution">#</a></h4>
<ul>
<li>Useful because A Gaussian is fully specified by only two moments and <strong>central limit theorem (CLT)</strong> holds</li>
<li>CLT states that the mean of independently draw random variables is normally distributed (given enough samples), irrespective of original distribution</li>
<li>$E[m_{k}] = \mu$</li>
<li>$Var[m_k] = \sigma^{2}$</li>
<li>$p(x) = \frac{1}{2\pi \sigma}e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}$</li>
<li>Standard normal distribution ($\mu=0$, $\sigma = 1$)</li>
</ul>
<h4 id="multivariate-gaussian-distribution">Multivariate Gaussian Distribution<a hidden class="anchor" aria-hidden="true" href="#multivariate-gaussian-distribution">#</a></h4>
<ul>
<li>Let $d$ be the dimension of the space</li>
<li>Let $\mu$ be a d-dimensonal vector (Mean vector)</li>
<li>Let $\Sigma$ be a DxD symmetric and positive semi-definite matrix (covariance matrix)</li>
<li>$p(x) = \frac{1}{(2\pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu))$</li>
<li>$E[m_{k}] = \mu$</li>
<li>$Var[m_k] = \Sigma$</li>
<li>Alternatively, $p(x) = \frac{|\beta|^{\frac{1}{2}}}{(2\pi)^{\frac{D}{2}}} exp(-\frac{1}{2}(x-\mu)^{T}\beta(x-\mu))$ where $\beta = \Sigma^{-1}$ is a <strong>precision matrix</strong></li>
</ul>
<h4 id="laplace-distribution">Laplace distribution<a hidden class="anchor" aria-hidden="true" href="#laplace-distribution">#</a></h4>
<ul>
<li>$p(x) = \frac{1}{2\gamma}exp(-\frac{-|x-\mu|}{\gamma})$</li>
<li>Places sharp peak at arbitrary point $\mu$ with spread of $\gamma$</li>
</ul>
<h4 id="dirac-delta-distribution">Dirac Delta Distribution<a hidden class="anchor" aria-hidden="true" href="#dirac-delta-distribution">#</a></h4>
<ul>
<li>$p(x) = \delta(x-\mu)$</li>
<li>Clusters all mass of the distribution function into a single point</li>
<li>Infinity high, infinity narrow peak</li>
<li>Exists to be integrated</li>
<li>Can be extended to an arbitrary number of points (empirical distribution):
<ul>
<li>$p(x) = \frac{1}{m}\Sigma_{i-1}^{m} \delta(x-x_{i})$</li>
</ul>
</li>
</ul>
<h4 id="mixtures-of-distributions">Mixtures of Distributions<a hidden class="anchor" aria-hidden="true" href="#mixtures-of-distributions">#</a></h4>
<ul>
<li>Define a cluster of distributions. Define a probability distribution P(c) that selects which distribution is drawn from.</li>
<li>$P(x) = \Sigma_{i} P(c=i) P(x |c=i)$</li>
<li>c is a <strong>latent variable</strong>, or a random variable that can&rsquo;t be observed directly</li>
<li>Common types of mixture models are the empirical distribution and Gaussian mixture models, where the cluster consists of Gaussians.</li>
</ul>
<h3 id="common-functions">Common Functions<a hidden class="anchor" aria-hidden="true" href="#common-functions">#</a></h3>
<h4 id="logistic-sigmoid-sigmax">Logistic sigmoid $\sigma(x)$<a hidden class="anchor" aria-hidden="true" href="#logistic-sigmoid-sigmax">#</a></h4>
<ul>
<li>$\sigma(x) = \frac{1}{1+exp(-x)}$</li>
<li>Defined on $\mathbb{R}$ and confined between 0 and 1</li>
<li>Saturates as $x \rightarrow \pm \infty$ (ie insensitive to changes in x)</li>
<li>Maximum slope at x=0</li>
<li>Useful to produce parameter of a Bernoulli distribution</li>
<li>Properties:
<ul>
<li>$\sigma(x) = \frac{exp(x)}{exp(x)+exp(0)}$</li>
<li>$\frac{d}{dx}\sigma(x) = \sigma(x)(1-\sigma(x))$</li>
<li>$1-\sigma(x) = \sigma(-x)$</li>
</ul>
</li>
</ul>
<h4 id="softplus-function-zetax">Softplus Function $\Zeta(x)$<a hidden class="anchor" aria-hidden="true" href="#softplus-function-zetax">#</a></h4>
<ul>
<li>$\Zeta(x) = log(1+exp(x))$</li>
<li>Defined on $\mathbb{R}$ and confined between 0 and $\infty$</li>
<li>Produces $\sigma$ or $\beta$ of normal distribution</li>
<li>A smoother version of $f(x) = max(0,x)$</li>
<li>Properties:
<ul>
<li>$log(\sigma(x)) = -\Zeta(-x)$</li>
<li>$\frac{d}{dx} \Zeta(x) = \sigma(x)$</li>
<li>$\forall x \in (0,1), \sigma^{-1}(x) = log(\frac{x}{1-x})$</li>
<li>$\forall x&gt;0, \Zeta^{-1}(x) = log(exp(x)-1)$</li>
<li>$\Zeta(x) = \int_{-\infty}^{x} \sigma(y) dy$</li>
<li>$\Zeta(x)-\Zeta(-x) = x$</li>
</ul>
</li>
</ul>
<h3 id="information-theory">Information Theory<a hidden class="anchor" aria-hidden="true" href="#information-theory">#</a></h3>
<h4 id="self-information">Self-Information<a hidden class="anchor" aria-hidden="true" href="#self-information">#</a></h4>
<ul>
<li>General Idea: knowing something unlikely has happened provides more information that knowing something likely has happened. Information theory quantifies this statement</li>
<li>Properties of Information
<ul>
<li>Likely events have low information (extreme: guaranteed events contain no information)</li>
<li>Less likely events should have more information</li>
<li>Independent events should have additive information (ie. tossing a coin twice contains twice as much information as tossing a coin once)</li>
</ul>
</li>
<li>The above motivate the definition of self-information. Let x be an event. Then $I(x) = -log(P(x))$ is the self-information of the event. The log typically is base 2 or base e, but in theory could have any base you want; a chance in base is effectively a change in scale/units.</li>
<li><strong>NOTE</strong>: by definition, $lim_{x\rightarrow 0 } x log(x) = 0$</li>
</ul>
<h4 id="shannon-entropy">Shannon Entropy<a hidden class="anchor" aria-hidden="true" href="#shannon-entropy">#</a></h4>
<ul>
<li>Extends I(x) to determine amount of uncertainty in a probability distribution:
<ul>
<li>$H(x) = H(P) =  E[I(x)]$, or the expectation value of I(x)</li>
</ul>
</li>
<li>In words, $H(x)$ gives the expect/average amount of information that you can gain from drawing from the probability distribution. Entropy is maximized for a uniform probability distribution (has a very direct connection to entropy in the thermodynamic sense)</li>
<li>When x is continuous, Shannon entropy is known as <strong>differential entropy</strong></li>
</ul>
<h4 id="kullback-leibler-kl-divergence">Kullback-Leibler (KL) divergence<a hidden class="anchor" aria-hidden="true" href="#kullback-leibler-kl-divergence">#</a></h4>
<ul>
<li>a metric of quantifying the difference in information between two distributions $P(x)$ and $Q(x)$
<ul>
<li>$D_{KL}(P||Q) = E[log(P(x))-log(Q(x))]$</li>
</ul>
</li>
<li>In words (for the discrete case), $D_{KL}$ is the extra information needed to send a message using the code P instead of the code Q.</li>
<li>KL divergence is non-negative</li>
<li>Only 0 iff P and Q are the same distribution</li>
<li>NOT symmetric in P and Q (not a true measure of distance)</li>
</ul>
<h4 id="cross-entropy">Cross-entropy<a hidden class="anchor" aria-hidden="true" href="#cross-entropy">#</a></h4>
<ul>
<li>$H(P,Q) = H(P)+D_{KL}(P||Q) = -E[log(Q(x))]$</li>
<li>Minimizing cross-entropy w.r.t. Q is the same as minimizing KL divergence</li>
</ul>
<h4 id="exponential-distribution">Exponential Distribution<a hidden class="anchor" aria-hidden="true" href="#exponential-distribution">#</a></h4>
<ul>
<li>Let $p(x,\lambda)$ be a piecewise function such that $p(x&lt;0&gt;,\lambda)=0$ and $p(x&gt;0,\lambda) = \lambda exp(-\lambda x)$ where $\lambda$ is the decay parameter</li>
<li>Serves to put a sharp peak at x=0</li>
</ul>
<h3 id="chi-squared-distribution">Chi-squared Distribution<a hidden class="anchor" aria-hidden="true" href="#chi-squared-distribution">#</a></h3>
<h4 id="basics">Basics<a hidden class="anchor" aria-hidden="true" href="#basics">#</a></h4>
<ul>
<li>Chi-squared is a continuous distribution of the sum of the squares of k standard normally distributed random variables.</li>
<li>k is called the number of &ldquo;degrees of freedom&rdquo;</li>
<li>$q = \Sigma_{i-1}^{k} x_{i}^{2}$ ($\chi^{2}$ parameter)</li>
<li>$p(x) = \frac{x^{\frac{k}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})}$</li>
<li>$E[m_{k}] = k$</li>
<li>$Var[m_k] = 2k$</li>
</ul>
<h4 id="goodness-of-fit-gof">Goodness of Fit (GOF)<a hidden class="anchor" aria-hidden="true" href="#goodness-of-fit-gof">#</a></h4>
<ul>
<li>$\chi^{2} = \Sigma \frac{(O-E)^{2}}{E} $</li>
<li>degree of freedom (dof) = number of categories -1</li>
<li>Look up table for what p-value is. Determines if you accept or reject the null hypothesis by whether p-value is greater than (don&rsquo;t reject null) or less than (reject null) significance level $\alpha$</li>
</ul>
<h4 id="independence">Independence<a hidden class="anchor" aria-hidden="true" href="#independence">#</a></h4>
<ul>
<li>Chech whether categorical data is independent</li>
<li>You are given observed data table O</li>
<li>expected data table E is calculated cellwise
<ul>
<li>For a given row and column, the cell value is $E = \frac{(\text{row total})(\text{column total})}{(\text{grand total})}$</li>
<li>Calculate $\chi^{2}$ as normal</li>
<li>dof = (#rows-1)(#cols-1)</li>
</ul>
</li>
</ul>
<h3 id="t-test">T-test<a hidden class="anchor" aria-hidden="true" href="#t-test">#</a></h3>
<h4 id="one-sample">One Sample<a hidden class="anchor" aria-hidden="true" href="#one-sample">#</a></h4>
<ul>
<li>Tests whether the mean of a normally distributed population is different from a specific value</li>
<li>Let $\mu$ be the measured mean and $\mu_{o}$ be the specific mean value to be tested</li>
<li>$t = \frac{\bar{x}-\mu_{o}}{\frac{s}{\sqrt{n}}}$</li>
<li>dof = n-1</li>
<li>Using a lookup table find the corresponding p-value, with the following caveats depending on what your null hypothesis is:
<ul>
<li>$\mu&gt;\mu_{0} \rightarrow$ read table as given</li>
<li>$\mu&lt;\mu_{0} \rightarrow$ if t-statistic is negative, read table as though t-value was positive
<ul>
<li>If t-statistic has the incorrect sign, read the $1-p$ value instead</li>
</ul>
</li>
<li>$\mu&gt;\mu_{0} \rightarrow$ read table as given, but double p-value to take into account both tails</li>
</ul>
</li>
<li>If p-value is less than $\alpha$, reject the null hypothesis</li>
</ul>
<h4 id="two-sample">Two Sample<a hidden class="anchor" aria-hidden="true" href="#two-sample">#</a></h4>
<p>Test whether the means of two populations are significantly different from one another</p>
<h5 id="paired">Paired<a hidden class="anchor" aria-hidden="true" href="#paired">#</a></h5>
<ul>
<li>This means that each value in first group has a one to one mapping to the value in the second group</li>
<li>Subtract values in a pairwise manner and run one sample t-test with $\mu_{0}$ = 0</li>
</ul>
<h5 id="unpaired">Unpaired<a hidden class="anchor" aria-hidden="true" href="#unpaired">#</a></h5>
<ul>
<li>$t = \frac{\bar x_{1} - \bar x_{2}}{\sqrt{(\frac{s_{1}^{2}}{n}+\frac{s^{2}}{n_{2}})}}$</li>
<li>dof = $(n_{1}-1)+(n_{2}-1)$</li>
<li>Read off p-value and compare to significance level</li>
</ul>
<h2 id="linear-algebra">Linear Algebra<a hidden class="anchor" aria-hidden="true" href="#linear-algebra">#</a></h2>
<h3 id="types-of-objects-in-linear-algebra">Types of Objects in Linear Algebra<a hidden class="anchor" aria-hidden="true" href="#types-of-objects-in-linear-algebra">#</a></h3>
<ul>
<li>Scalars: A number or a 1x1 matrix.
<ul>
<li>Can be confined to various domains eg. ($s \in \mathbb{R}$) is a real-valued scalar and ($n \in \mathbb{N}$) defines a natural number scalar</li>
</ul>
</li>
<li>Vectors: An array of numbers. Each number is identified by its index.
<ul>
<li>typically though of as nx1 matrices (ie. column vectors).
<ul>
<li>$x = \begin{bmatrix} x_{1} \\ x_{2} \\  \vdots \\ x_{m} \end{bmatrix}$</li>
<li>Can also think of vectors as points embedded in n-dimensional space, with each element representing a coordinate on a different axis</li>
</ul>
</li>
<li>Suppose you want to index only certain elements of a vector. Create a set S = {1,3,6} and write $x_{S}$ to denote elements $x_1, x_3, x_6$. $x_{-S}$ refers to complement of $x_{s}$, or all the elements in x excluding $x_1, x_3, x_6$</li>
</ul>
</li>
<li>Matrices
<ul>
<li>2D array of numbers that uses 2 indices.
<ul>
<li>if A has m rows and n columns, then $A \in \mathbb{R}^{m \times n}$</li>
<li>$A_{i,j}$ denotes the element in the ith row and jth column.
<ul>
<li>You can take slices with a &ldquo;:&quot;, namely $A_{i,:}$ denotes the horizontal slice at row i and $A_{:,j}$ denotes the vertical slice at col j</li>
</ul>
</li>
<li>Can be explicitly written out by elements:
<ul>
<li>$\begin{bmatrix} A_{1,1} &amp; A_{1,2} \\ A_{2,1} &amp; A_{2,2} \\   \end{bmatrix}$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Tensors
<ul>
<li>n-dimensional arrays that can have an arbitrary number of indices (like the Levi-Civita tensor $\epsilon_{ijk}$)</li>
</ul>
</li>
</ul>
<h3 id="matrix-operations">Matrix Operations<a hidden class="anchor" aria-hidden="true" href="#matrix-operations">#</a></h3>
<ul>
<li>Transpose: taking the mirror image across the diagonal
<ul>
<li>Can also think about it as swapping the rows and columns, or more algebraically $A_{i,j} = A_{j,i}$</li>
<li>Can transpose a column vector into a row vector. So another way of writing a column vector is $\begin{bmatrix} x_{1} &amp; x_{2} &amp; x_{3} \end{bmatrix}^{T}$</li>
<li>When transposing a product of matrices, you reverse the order, and then transpose individually ie.
<ul>
<li>$(AB)^{T} = B^{T}A^{T}$</li>
</ul>
</li>
</ul>
</li>
<li>Multiplication</li>
<li>Can multiply matrices by each other. If A is a m × n and B is a n × p matrix, then C is a m × p matrix.
<ul>
<li>$C = AB$</li>
<li>$C_{i,j} = \Sigma_{k} A_{i,k}B_{k,j}$</li>
<li>Matrix multiplication is associative, distributive,  but <strong>NOT</strong> commutative (ie. AB=BA is not always true)</li>
</ul>
</li>
<li>Can multiply scalars by matrices: scale each element in matrix by the scalar c</li>
<li>Hadamard Product ($A \odot B$)
<ul>
<li>Element-wise multiplication between two same sized matrices</li>
</ul>
</li>
<li>Dot Product: Between two vectors of the same size (say x and y), the dot product is defined as $x^{T}y$
<ul>
<li>The dot product is commutative: $x^{T}y = y^{T}x$</li>
</ul>
</li>
<li>Addition
<ul>
<li>Can add matrices together as long as their dimensions align</li>
<li>Can add scalars to matrices: Let A be an mxn matrix. Create a mxn matrix whose entries are entirely ones and scale it by the scalar, then add to A</li>
<li>Broadcasting: Adding a vector to a matrix
<ul>
<li>let C and A be mxn matrices and b be a mx1 vector
<ul>
<li>C = A+b where b is implicitly added to each column of A</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="central-problem-of-linear-algebra">Central Problem of Linear Algebra<a hidden class="anchor" aria-hidden="true" href="#central-problem-of-linear-algebra">#</a></h3>
<ul>
<li>We want to solve $Ax = b$
<ul>
<li>Various techniques exists to state whether this is possible, how to do it if it is possible, or how close we can get</li>
</ul>
</li>
</ul>
<h4 id="gaussian-elimination">Gaussian Elimination<a hidden class="anchor" aria-hidden="true" href="#gaussian-elimination">#</a></h4>
<ul>
<li>Use matrix multiplication on an augmented matrix $A|b$ to try and simplify the problem as much as possible</li>
<li>Through taking linear combinations of the rows, you can simplify the problem while maintaining the row space (and thus the solution) of the problem</li>
</ul>
<h4 id="reduced-row-echelon-form-rref">Reduced Row Echelon Form (RREF)<a hidden class="anchor" aria-hidden="true" href="#reduced-row-echelon-form-rref">#</a></h4>
<ul>
<li>Defined as a matrix with the following properties
<ul>
<li>The pivot of any row is 1
<ul>
<li>The pivot is the first non-zero entry of a matrix</li>
</ul>
</li>
<li>The pivot of any given row is always to the right of the pivot of the row above it
<ul>
<li>It does not need to be in adjacent column, it just needs to be to the right</li>
</ul>
</li>
<li>The pivot is the only non-zero entry in its column</li>
</ul>
</li>
<li>The end goal of Gaussian elimination is to get the augmented matrix into RREF</li>
</ul>
<h5 id="reading-of-solutions-of-axb-from-rref">Reading of Solutions of Ax=b from RREF<a hidden class="anchor" aria-hidden="true" href="#reading-of-solutions-of-axb-from-rref">#</a></h5>
<ul>
<li>For the ith column in RREF that does not have a pivot (ie. the free variables), define a variable $\lambda_{i}$.
<ul>
<li>For the set of $\lambda$, first set all lambda&rsquo;s to 0 and solve x</li>
<li>Then cycle through the set of $\lambda$, setting only 1 of them to 1 (the rest of the free variables) and solve for x</li>
</ul>
</li>
<li>The solution is then a linear combination of the vectors found, with the vector with $\lambda$ all equal to zero having a prefactor of 1, and the rest of the vectors having a prefactor of their associated $\lambda$</li>
</ul>
<h3 id="lu-decomposition">LU Decomposition<a hidden class="anchor" aria-hidden="true" href="#lu-decomposition">#</a></h3>
<ul>
<li>We can write any square matrix as the product of a lower triangular matrix and an upper triangular matrix
<ul>
<li>$M = LU$</li>
</ul>
</li>
<li>Steps to get LU Decomposition
<ul>
<li>Perform Gaussian elimination until you get an upper triangular matrix. This is U</li>
<li>As you do row operations, keep track of the elimination matrices used along the way</li>
<li>Explicitly, you start from $A=LU$
<ul>
<li>$A=LU \rightarrow E_{0}E_{1}&hellip;E_{n-1}A = U$</li>
<li>So $L^{-1} = E_{0}E_{1}&hellip;E_{n-1} \rightarrow L = E_{n-1}^{-1}&hellip;E_{1}^{-1}E_{0}^{-1}$</li>
</ul>
</li>
<li>The product of inverse elimination matrices are simple: You just add a row instead of subtract a row</li>
</ul>
</li>
</ul>
<h4 id="ldu-decomposition">LDU Decomposition<a hidden class="anchor" aria-hidden="true" href="#ldu-decomposition">#</a></h4>
<ul>
<li>The goal of LDU is to have ones on the main diagonals of L and U.
<ul>
<li>D is a diagonal matrix</li>
<li>This is accomplished by scaling each row of U such that the diagonal element in each row is 1</li>
<li>You place the scaling factor in  the corresponding slot in D</li>
</ul>
</li>
</ul>
<h3 id="identity-and-inverses">Identity and Inverses<a hidden class="anchor" aria-hidden="true" href="#identity-and-inverses">#</a></h3>
<ul>
<li>The identity matrix is defined as $A_{i,j} = \delta_{i,j}$, where $\delta$ is the Kronecker delta. More explicitly, A has ones along the main diagonal (top left to bottom right) and zeros everywhere else
<ul>
<li>I*A = A</li>
</ul>
</li>
<li>The Inverse matrix of a square matrix A is defined as $AA^{-1} = A^{-1}A = I$
<ul>
<li>Assuming $A_{-1}$ exists, its solves the fundamental problem of linear algebra: $Ax=b \rightarrow x = A^{-1}b$</li>
</ul>
</li>
</ul>
<h3 id="vector-spaces">Vector Spaces<a hidden class="anchor" aria-hidden="true" href="#vector-spaces">#</a></h3>
<ul>
<li>A vector space over some field $\mathbb{F}$ (think $\mathbb{R}$,$\mathbb{C}$,$\mathbb{Z}$) is a set $V$ with two operations + and * that satisfies the following properties (given $\forall u,v \in \mathbb{R}$ and $c,d \in \mathbb{R}$)
<ul>
<li>Closure under addition: $u+v \in V$</li>
<li>Commutativity of addition: $u+v=v+u$</li>
<li>Associativity of addition: $(u+v)+2=u+(v+w)$</li>
<li>Zero: $\exist 0_{v} \in V$ such that $u+0_{v} = u \ \forall u \in V$</li>
<li>Additive inverse: $\exist u \in V$ such that $u+w = 0 \ \forall w \in V$</li>
<li>Multiplicative Closure: For every $u \in V$ there exists $w \in V$ such that $u + w = 0_{v}$</li>
<li>Distributivity of scalars over scalar addition: (c+d)·v = c·v+d·v where $c,d,v \in \mathbb{F}$</li>
<li>Distributivity of scalars over vector addition: c·(u+v) = c·u+c·v where $c \in \mathbb{F}$ and $u,v \in \mathbb{F}$</li>
<li>Associativity of scalar multiplication: (cd)·v = c· (d·v) where $c,d \in \mathbb{F}$ and $v \in V$</li>
<li>Existence of Unity: 1 · v = v for all v ∈ V where $1 \in \mathbb{F}$</li>
</ul>
</li>
</ul>
<h4 id="subspaces">Subspaces<a hidden class="anchor" aria-hidden="true" href="#subspaces">#</a></h4>
<ul>
<li>A subspace U is a subset of V that satisfies:
<ul>
<li>$\alpha_{1}u_{1}+\alpha_{2}u_{2} \in U$ where $\forall \alpha_{1},\alpha_{2} \in \mathbb{R}$ and $\ u_{1},u{2} \in U$</li>
</ul>
</li>
<li>For <strong>any</strong> subset $S \subset V$, span(S) is a subspace of V</li>
</ul>
<h5 id="orthogonal-complements">Orthogonal Complements<a hidden class="anchor" aria-hidden="true" href="#orthogonal-complements">#</a></h5>
<ul>
<li>Let U and V be subspaces of vector space W
<ul>
<li>The sum of U and V is: $U+V = span(U\cup V) = \{u+v|u\in U,v\in V\}$</li>
<li>The direct sum of U and V is: $U\otimes V = span(U\cup V) = \{u+v|u\in U,v\in V\}$, assuming that $U\cap V = \{0_{w}\}$
<ul>
<li>Let $w = u+v \in U\otimes V$. There is only one way to write w as a the sum of a vector in U and a vector in V</li>
</ul>
</li>
</ul>
</li>
<li>Given a subspace U of W, we define:
<ul>
<li>$U^{\perp} = \{w\in W|w\cdot u = 0,\ \forall u \in U\}$</li>
<li>This is the <strong>orthogonal complement</strong></li>
<li>Let U be subspace of a finite dimensional vector space W. Then the set $U{\perp}$ is a subspace of W and $W = U\otimes U^{\perp}$</li>
</ul>
</li>
</ul>
<h3 id="linear-transformations">Linear Transformations<a hidden class="anchor" aria-hidden="true" href="#linear-transformations">#</a></h3>
<ul>
<li>Define a map $L: V \rightarrow W$. This map is <strong>linear</strong> if the following conditions hold:
<ul>
<li>$L(u+v) = L(u)+L(v)$</li>
<li>$L(cv) = cL(v)$</li>
</ul>
</li>
<li>Alternatively, you can combine the two conditions:
<ul>
<li>$L(ru+sv) = rL(u)+sL(v)$</li>
</ul>
</li>
</ul>
<h4 id="diagonalization">Diagonalization<a hidden class="anchor" aria-hidden="true" href="#diagonalization">#</a></h4>
<ul>
<li>Given a linear transformation, how can we write it as a matrix?
<ul>
<li>Simplest example: $L: V\rightarrow V$ and we have a basis consisting of the linearly independent eigenvectors of L and the corresponding eigenvalues. Then L can be written as a diagonal matrix whose entries are the eigenvalues
<ul>
<li>the matrix for L in the basis S is diagonal iff S is a basis of eigenvectors for L</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="change-of-basis">Change of Basis<a hidden class="anchor" aria-hidden="true" href="#change-of-basis">#</a></h5>
<ul>
<li>A change of basis refers to writing on basis S in terms of another basis T
<ul>
<li>Look at each vector in S, and figure out a linear combination of T that creates S. This linear combination goes into the associated column of the matrix P</li>
</ul>
</li>
<li>Two matrices are <strong>similar</strong> if there exists and invertible matrix P such that
<ul>
<li>$N = P^{-1}MP$</li>
<li>A matrix M is <strong>diagonalizeable</strong> if there exists an invertible matrix P and Diagonal matrix D such that
<ul>
<li>$D = P^{-1}MP$</li>
</ul>
</li>
<li>What this means is that, to rewrite a matrix M in terms of a different basis, perform the transformation $M \rightarrow P^{-1}MP$ with the appropriate change of basis matrix P</li>
<li>If you know the eigendecomposition of a matrix M, then you can diagonalize M by performing a change of basis to the eigenvector basis</li>
</ul>
</li>
</ul>
<h3 id="linear-dependence-and-span">Linear Dependence and Span<a hidden class="anchor" aria-hidden="true" href="#linear-dependence-and-span">#</a></h3>
<ul>
<li>One can think about the product $Ax$ as the following:
<ul>
<li>each component of x scales the corresponding column of a. The scaled columns are then summed together
<ul>
<li>$Ax = \Sigma_{i} x_{i}A_{:,i}$</li>
</ul>
</li>
<li>This is one way to think about a <strong>linear combination</strong>. Given a set of vectors, you scale each vector by some factor and sum the scaled vectors</li>
<li><strong>span</strong> denotes the set of all possible linear combinations</li>
</ul>
</li>
<li>One can reformulate the fundamental problem $Ax=b$ as determining whether some combination of the columns equals b, or equivalently, does b lie in the span of the column space of A?</li>
<li>For a given set of vectors, we say the set is linearly independent if no combination yields the zero vector</li>
</ul>
<h3 id="basis-and-dimension">Basis and Dimension<a hidden class="anchor" aria-hidden="true" href="#basis-and-dimension">#</a></h3>
<ul>
<li>Dimension refers to the number of components necessary to describe an vector</li>
<li>Let V be a vector space. The a set S is a <strong>basis</strong> for V is S is linearly independent and V = span(S)
<ul>
<li>In $\mathbb{R^{n}}$, the standard basis is the columns of the identity matrix</li>
<li>A basis is not unique
<ul>
<li>Although each vector has a unique representation in a given basis</li>
</ul>
</li>
</ul>
</li>
<li>V = span($v_{1}v_{2}&hellip;v_{n}$) iff all vectors are linearly independent</li>
</ul>
<h3 id="norms">Norms<a hidden class="anchor" aria-hidden="true" href="#norms">#</a></h3>
<ul>
<li>The <strong>norm</strong> is a function that gives a metric of the size of a vector. It must satisfy the following properties:
<ul>
<li>$f(x) = 0 \rightarrow x = 0$</li>
<li>$f(x+y) \leq f(x) + f(y)$ (triangle equality)</li>
<li>$ \forall \alpha \in \mathbb{R}, f(\alpha x) = |\alpha|f(x)$</li>
</ul>
</li>
<li>The $L^{p}$ norm is defined as (for $p \in \mathbb{R}, p \geq 1$)
<ul>
<li>$||x||_{p} = (\Sigma_{i}|x_{i}|^{p})^{\frac{1}{2}}$</li>
</ul>
</li>
<li>The squared $L^{2}$ norm is useful since mathematically and computationally, it is easier to work with (derivative is simple, don&rsquo;t need a square root)</li>
<li>The $L{1}$ norm is useful when the difference between zero and nonzero elements is important
<ul>
<li>$L{1}$ norm increases by $\epsilon$ as x goes from 0 to $\epsilon$</li>
</ul>
</li>
<li>The $L_{\infty}$ norm denotes the absolute value of the largest element in the array
<ul>
<li>$||x||_{\infty} = max|x_{i}|$</li>
</ul>
</li>
<li>There also exists a notion of norm for a matrix. The most common is the <strong>Forbenius norm</strong>:
<ul>
<li>$||A||_{F}\sqrt{\Sigma_{i,j}A^{2}_{i,j}}$</li>
</ul>
</li>
</ul>
<h3 id="orthogonal-bases">Orthogonal Bases<a hidden class="anchor" aria-hidden="true" href="#orthogonal-bases">#</a></h3>
<ul>
<li>a basis is orthogonal if all the vectors are prependicular to each other
<ul>
<li>$v_{i}\cdot v_{j} = \delta_{ij}$
<ul>
<li>Orthonormal bases impose the additional condition that the norm of each basis vector is 1</li>
</ul>
</li>
<li>In an orthonormal basis, any vector can be written as
<ul>
<li>$v = \Sigma_{i}(v\cdot u_{i})u_{i}$</li>
</ul>
</li>
<li>Change of basis matrices between orthonormal bases are orthogonal matrices ie.
<ul>
<li>$P^{-1} = P^{T}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="gram-schmidt-and-orthogonal-complements">Gram-Schmidt and Orthogonal Complements<a hidden class="anchor" aria-hidden="true" href="#gram-schmidt-and-orthogonal-complements">#</a></h3>
<h3 id="eigendecomposition">Eigendecomposition<a hidden class="anchor" aria-hidden="true" href="#eigendecomposition">#</a></h3>
<ul>
<li>Solve the matrix equation $Av = \lambda v$, where $\lambda$ is a constant.
<ul>
<li>$\lambda$ is called the <strong>eigenvalue</strong> and $v$ is called the <strong>eigenvector</strong></li>
<li>Eigenvectors are <strong>NOT</strong> unique. This is because we can think of an eigenvector as any vector whose direction is unchanged after being acted on by a matrix. Hence, eigenvectors are typically normalized for convenience.</li>
<li>Multiple eigenvectors can have the same eigenvalue</li>
<li>Real matrices can have complex eigenvalues</li>
</ul>
</li>
<li>The eigendecomposition is not possible for all matrices</li>
<li>Given the eigendecomposition of a matrix (ie $\lambda =  [\lambda_{1},\lambda_{1},&hellip;,\lambda_{n}]$ and V is a matrix whose columns are the eigenvectors), you can write the matrix as:
<ul>
<li>$A = \bold{V} diag(\lambda)V^{-1}$</li>
<li>Typically, the eigenvalues are sorted in descending order</li>
<li>Make sure that the eigenvectors are in the column that correspond to the correct eigenvalue</li>
</ul>
</li>
<li>For a real symmetric matrix, the eigendecomposition is:
<ul>
<li>$Q\Lambda Q^{T}$, where $Q$ is an orthogonal matrix</li>
<li>The eigendecomposition is guaranteed to exist for a real, symmetric matrix</li>
</ul>
</li>
<li>A matrix is singular iff any of the eigenvalues are zero</li>
<li>Let $f(x) = x^{T}Ax$ subject to $||x_{2}||=1$. Whenever x is equal to an eigenvector of A, f equals the corresponding eigenvalue
<ul>
<li>The minimum and maximum of f within the constraint region (ie. the surface of a n-dimensional unit ball) is the minimum and maximum eigenvalue respectively</li>
</ul>
</li>
</ul>
<h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)<a hidden class="anchor" aria-hidden="true" href="#singular-value-decomposition-svd">#</a></h3>
<ul>
<li>Every real matrix has a singular value decomposition. Suppose A is an mxn matrix. Let U be a mxm  orthogonal matrix, D be a mxn diagonal matrix (NOTE: not necessarily square), and V be a orthogonal nxn matrix. Then
<ul>
<li>$A = UDV^{T}$</li>
<li>The columns of U are the <strong>left-singular vectors</strong>, the columns of V are the <strong>right-singular vectors</strong>, and the diagonal elements are the <strong>singular values</strong></li>
<li>U can be found by taking the eigenvectors of $AA^{T}$, V can be found by taking the eigenvectors of $A^{T}A$, and the singular values are the square root of the eigenvalues of $A^{T}A$ of $AA^{T}$, whichever has the smaller dimension.</li>
</ul>
</li>
</ul>
<h3 id="moore-penrose-pseudoinverse">Moore-Penrose Pseudoinverse<a hidden class="anchor" aria-hidden="true" href="#moore-penrose-pseudoinverse">#</a></h3>
<ul>
<li>For non-square matrices, there may exist a matrix $B$ that solves $Ax = y$ by left-multiplication:
<ul>
<li>$Ax=y \rightarrow x = By$</li>
</ul>
</li>
<li>If you have a tall matrix, there may exist no solution, and if you have a wide matrix, there could be multiple solutions</li>
<li>Define the matrix:
<ul>
<li>$A^{+} = \lim_{\alpha \rightarrow 0}(A^{T}A+\alpha I)^{-1}A^{T}$</li>
</ul>
</li>
<li>The practical formula for computing this matrix is
<ul>
<li>$A^{+} = VD_{+}U^{T}$
<ul>
<li>$U,V,D$ are from the SVD of A. $D^{+}$ can be computed by taking the reciprocal of the nonzero elements and then transposing</li>
</ul>
</li>
</ul>
</li>
<li>If A has more columns than row, then $x=A^{+}y$ is one of the possible solutions
<ul>
<li>This solution has the minimum Euclidean norm $||x||_{2}$ among all possible solutions</li>
</ul>
</li>
<li>If A has more rows than columns, the x from the pseudoinverse minimizes $||Ax-y||^{2}$ (ie. the vector that has the smallest Euclidean norm that almost solve the problem)</li>
</ul>
<h3 id="qr-decomposition">QR Decomposition<a hidden class="anchor" aria-hidden="true" href="#qr-decomposition">#</a></h3>
<h4 id="gram-schmidt">Gram-Schmidt<a hidden class="anchor" aria-hidden="true" href="#gram-schmidt">#</a></h4>
<ul>
<li>For each column vector in matrix M, let the first column be your first basis vector
<ul>
<li>The second column can be made orthogonal by subtracting of the projection of column 2 from column 1
<ul>
<li>$v^{\perp} = v-\frac{u \cdot v}{u \cdot u}u$</li>
</ul>
</li>
<li>Rinse and repeat for the rest of the columns, subtracting off the projections of the established basis vectors</li>
<li>Normalize every vector at the end</li>
</ul>
</li>
<li>Place all the normalized basis vectors in a orthogonal matrix Q</li>
<li>The R matrix can be figured out with the following fact:
<ul>
<li>The (i,j) entry of the upper triangular matrix R equals the dot product of the i-th column of Q with the j-th column of M</li>
</ul>
</li>
<li>Gram-Schmidt is quite numerically unstable, so it is good for visualization but impractical</li>
</ul>
<h3 id="trace">Trace<a hidden class="anchor" aria-hidden="true" href="#trace">#</a></h3>
<ul>
<li>The <strong>trace</strong> gives the sum of all diagonal entries of a matrix
<ul>
<li>$Tr(A) = \Sigma_{i}A_{i,j}$</li>
</ul>
</li>
<li>Useful to replace summations
<ul>
<li>Frobenius norm can be rewritten as
<ul>
<li>$||A||_{F} = \sqrt{Tr(AA^{T})}$</li>
</ul>
</li>
</ul>
</li>
<li>$Tr(A) = Tr(A^{T})$</li>
<li>The trace of a square matrix is invariant under cyclic permuation of matrices
<ul>
<li>$Tr(ABC)=Tr(CAB)=Tr(BCA)$</li>
<li>This holds even if A and B are not square matrices, as long as the final product is a square matrix</li>
</ul>
</li>
</ul>
<h3 id="determinant">Determinant<a hidden class="anchor" aria-hidden="true" href="#determinant">#</a></h3>
<ul>
<li>Denotes as Det(A) is only defined for square matrices. Det(A) equals the product of all eigenvalues of a matrix</li>
<li>$|Det(A)|$ can be thought of as how the volume of the space changes upon action by the matrix A
<ul>
<li>If Det(A) = 0, then the space collapsed to a lower dimensional one
<ul>
<li>Det(A) = 0 also means that the matrix is not-invertible</li>
</ul>
</li>
<li>If Det(A) = 1, then the transformation preserves volume</li>
</ul>
</li>
<li>Determinant changes sign upon swapping either two rows or two columns</li>
<li>Scaling a row by $\lambda$ changes the determinant by a factor of $\lambda$</li>
<li>Row operations do not change the value of the determinant</li>
<li>det(AB) = det(A) det(B)</li>
<li>det($A^{T}$) = det(A)</li>
<li>det($A^{-1}$) = $\frac{1}{det(A)}$</li>
</ul>
<h3 id="kernel-range-nullityrank">Kernel, Range, Nullity,Rank<a hidden class="anchor" aria-hidden="true" href="#kernel-range-nullityrank">#</a></h3>
<h4 id="mapping-definitions">Mapping definitions<a hidden class="anchor" aria-hidden="true" href="#mapping-definitions">#</a></h4>
<ul>
<li>Let $f: S \rightarrow T$ be a map from S to T.
<ul>
<li>S is the <strong>domain</strong> of the function</li>
<li>T is the <strong>codomain</strong> of the fraction</li>
<li>the set $ran(f) = im(f) = f(S) = \{f(s)|s\in S\} \subset T$ is called the <strong>range</strong> or <strong>image</strong> of f
<ul>
<li>You can think of the range as the part of the codomain T that S actually maps to</li>
<li>the image of $L(V)$ is a subspace of W</li>
</ul>
</li>
<li>for some subset U of T, then $f^{-1}(U) = \{s\in S|f(s) \in U\}\subset S$
<ul>
<li>The <strong>preimage</strong> of a set $U$ is the set of all elements of $S$ which maps to $U$</li>
</ul>
</li>
<li>The function f is <strong>one-to-one</strong> if different elements of S always map to different elements of T (for $x\neq y, f(x)\neq f(y)$)
<ul>
<li>also called <strong>injective</strong></li>
</ul>
</li>
<li>The function f is <strong>onto</strong> if every element of T is mapped to some element of T (ie. $\forall t \in T, \ \exists s \in S \rightarrow f(s)= t$)
<ul>
<li>also called <strong>surjective</strong></li>
</ul>
</li>
<li>functions that are both injective and surjective are called <strong>bijective</strong></li>
</ul>
</li>
</ul>
<h4 id="kernel">Kernel<a hidden class="anchor" aria-hidden="true" href="#kernel">#</a></h4>
<ul>
<li>Let $L: V\rightarrow W$ be a linear transformation. The <strong>kernel</strong> of L is defined as
<ul>
<li>$ker\ L = \{v\in V|Lv=0_{w}\}$</li>
<li>In words, the kernel is all $v \in V$ such that the linear transformation maps onto the zero vector in $W$</li>
<li>A linear transformation L is injective iff $ker\ L = \{0_{w}\}$
<ul>
<li>ie. only the zero vector in $V$ maps to the zero vector in $W$</li>
<li>$ker\ L$ is a subspace of V</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="rank-and-nullity">Rank and Nullity<a hidden class="anchor" aria-hidden="true" href="#rank-and-nullity">#</a></h4>
<ul>
<li>The <strong>rank</strong> of a linear transformation $L$ is the dimension of its image, written as $rank(L) = dim\ L(V) = dim\ Im(L)$</li>
<li>The nullity is the dimension of the kernel, written as $null\ L = dim\ ker\ L$</li>
<li>$dim\ V = dim\ ker\ V + dim\ L(V)$</li>
</ul>
<h3 id="least-squares">Least Squares<a hidden class="anchor" aria-hidden="true" href="#least-squares">#</a></h3>
<ul>
<li>Solving the equation $M^{T}MX = M^{T}V$, where M is a rectangular matrix
<ul>
<li>Used when there exists no perfect solution to $MX=V$. Finds the best possible solution</li>
</ul>
</li>
<li>For polynomial fits, you construct a Vandermonde matrix from your data and solve for the coefficients of the polynomial</li>
</ul>
<h2 id="calculus">Calculus<a hidden class="anchor" aria-hidden="true" href="#calculus">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://mushetty.me">mushetty.me</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
